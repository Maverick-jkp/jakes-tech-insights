#!/usr/bin/env python3
"""
Keyword Curator - Semi-automated keyword research for blog content

Generates keyword candidates using Claude API based on KEYWORD_STRATEGY.md
Provides interactive selection interface for human filtering (5 minutes weekly)

Usage:
    python scripts/keyword_curator.py
    python scripts/keyword_curator.py --count 15
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import requests

try:
    from anthropic import Anthropic
except ImportError:
    print("Error: anthropic package not installed")
    print("Install with: pip install anthropic")
    sys.exit(1)


CURATION_PROMPT_WITH_TRENDS = """ì—­í• :
ë„ˆëŠ” ê´‘ê³  ìˆ˜ìµ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ íë ˆì´í„°ë‹¤.
ì•„ë˜ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ê³ CPC, ê°ì • ë°˜ì‘í˜•** í‚¤ì›Œë“œë¥¼ ì œì•ˆí•˜ë¼.

ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„°:
{trends_data}

ëª©í‘œ:
í•œêµ­ì–´ / ì˜ì–´ / ì¼ë³¸ì–´ ê°ê°ì—ì„œ
**ë¶ˆì•ˆ, ë¶„ë…¸, ê¶ê¸ˆì¦**ì„ ìœ ë°œí•˜ëŠ” í‚¤ì›Œë“œë§Œ ì œì•ˆí•˜ë¼.

ê¸ˆì§€:
- ì¶”ìƒì ì¸ íŠ¸ë Œë“œ ìš”ì•½ ("AI íŠ¸ë Œë“œ", "ìƒˆë¡œìš´ ê¸°ìˆ ")
- êµìœ¡/ì •ë³´ì„± í‚¤ì›Œë“œ ("~í•˜ëŠ” ë°©ë²•", "~ë€ ë¬´ì—‡ì¸ê°€")
- ê¸ì •ì ì´ê³  í‰í™”ë¡œìš´ í‚¤ì›Œë“œ

ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ìœ í˜•:
1. ì‚¬ê±´/ì‚¬ê³ /ë…¼ë€ ì¤‘ì‹¬ (ê³„ì • ì •ì§€, ë¨¹í†µ, ê³¼ì§•ê¸ˆ, í™˜ë¶ˆ ê±°ë¶€)
2. ì •ë¶€ì •ì±… í˜œíƒ/ì¡°ê±´ (ì‹ í˜¼ë¶€ë¶€ ì§€ì›, ì²­ë…„ ëŒ€ì¶œ, ì„¸ê¸ˆ ê°ë©´)
3. ì—°ì˜ˆì¸/ìœ ëª…ì¸ ìŠ¤ìº”ë“¤ (ë…¼ë€, ì‚¬ê³¼, í‡´ì¶œ, ë³µê·€)
4. ì„œë¹„ìŠ¤ í”¼í•´ ì‚¬ë¡€ (í™˜ë¶ˆ ì•ˆë¨, ê³„ì • ì°¨ë‹¨, ë²„ê·¸ë¡œ ì†í•´)
5. ê·œì œ/ì œì¬/ê¸ˆì§€ (ì‚¬ìš© ê¸ˆì§€, ì œì¬ ëŒ€ìƒ, ë¶ˆë²• íŒì •)

ì¶œë ¥ í˜•ì‹:
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼.

[
  {{
    "keyword": "í‚¤ì›Œë“œ ë¬¸êµ¬",
    "raw_search_title": "ì‚¬ìš©ìê°€ êµ¬ê¸€ì— ê²€ìƒ‰í•  ë•Œ ì •í™•íˆ ì…ë ¥í•˜ëŠ” ê²€ìƒ‰ì–´ (ì†Œë¬¸ì, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´)",
    "editorial_title": "ê¸°ì‚¬ ì œëª© í˜•ì‹ì˜ ë…ì ì¹œí™”ì  ì œëª©",
    "core_fear_question": "ì‚¬ìš©ìì˜ í•µì‹¬ ë‘ë ¤ì›€ì„ ë‹´ì€ ì§ˆë¬¸ í•œ ë¬¸ì¥",
    "language": "ko",
    "category": "tech",
    "search_intent": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ ë‹¹ì¥ ê²€ìƒ‰í•˜ëŠ” ì´ìœ  (í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì—‡ì„ ìƒëŠ”ì§€)",
    "angle": "ì´ í‚¤ì›Œë“œë¥¼ ë‹¤ë£° ë•Œì˜ ê´€ì ",
    "competition_level": "low",
    "why_it_works": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ì˜êµ¬ì ìœ¼ë¡œ ë¬´ì—‡ì„ ìƒëŠ”ì§€ (ë§ˆê°/ê¸°íšŒ ì†ì‹¤ ì¤‘ì‹¬)",
    "purpose": "high competitionì¸ ê²½ìš°ì—ë§Œ: Traffic acquisition / Brand positioning / Viral content ì¤‘ í•˜ë‚˜",
    "keyword_type": "trend",
    "priority": 7,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "intent_signal": "STATE_CHANGE"
  }}
]

ì¤‘ìš”:
- keyword_typeì€ "trend" ë˜ëŠ” "evergreen" ì¤‘ í•˜ë‚˜
- categoryëŠ” "tech", "business", "lifestyle", "society", "entertainment" ì¤‘ í•˜ë‚˜ (5ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ)
- languageëŠ” "en", "ko", "ja" ì¤‘ í•˜ë‚˜ (3ê°œ ì–¸ì–´ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ)
- competition_levelì€ "low", "medium", "high" ì¤‘ í•˜ë‚˜
- priorityëŠ” 1-10 ì‚¬ì´ì˜ ìˆ«ì (ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
- risk_levelì€ "safe", "caution", "high_risk" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "safe")
- name_policyëŠ” "no_real_names", "generic_only" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "no_real_names")
- intent_signalì€ "STATE_CHANGE", "PROMISE_BROKEN", "SILENCE", "DEADLINE_LOST", "COMPARISON" ì¤‘ í•˜ë‚˜
- ì§€ê¸ˆ ì‹œì (2026ë…„ 1ì›”)ì—ì„œ í˜„ì‹¤ì ì¸ í‚¤ì›Œë“œë§Œ ì œì•ˆ
- ì˜ˆì‹œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì‹¤ì œ ê²€ìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ í‚¤ì›Œë“œë§Œ ì œì•ˆ
- ìœ„ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ í‚¤ì›Œë“œ ì œì•ˆ
- **ì¤‘ìš”**: 5ê°œ ì¹´í…Œê³ ë¦¬(tech, business, lifestyle, society, entertainment)ë¥¼ ë°˜ë“œì‹œ ê³ ë¥´ê²Œ ë¶„ë°°í•  ê²ƒ

ì–¸ì–´ë³„ í†¤ ì°¨ì´:
- ğŸ‡ºğŸ‡¸ English: rights, compensation, legal leverage, lawsuits ì¤‘ì‹¬
- ğŸ‡°ğŸ‡· Korean: ë¶ˆê³µì •, ì¢Œì ˆ, ì†Œë¹„ì ë³´í˜¸, ì±…ì„ ì¶”ê¶ ì¤‘ì‹¬
- ğŸ‡¯ğŸ‡µ Japanese: ë¶ˆíˆ¬ëª…ì„±, ê³µì‹ ì ˆì°¨, ì ì ˆí•œ ëŒ€ì‘ ë°©ë²• ì¤‘ì‹¬

**ğŸ”´ ì•ˆì „ ê°€ì´ë“œë¼ì¸ (CRITICAL - AdSense/ë²•ì  ë¦¬ìŠ¤í¬ ë°©ì§€):**

ì ˆëŒ€ ê¸ˆì§€:
- ì‹¤ëª… ì‚¬ìš© (ì—°ì˜ˆì¸, ê¸°ì—…ì¸, ì •ì¹˜ì¸, íŠ¹ì • ê¸°ì—…ëª…)
- í™•ì •ë˜ì§€ ì•Šì€ ì˜í˜¹Â·ë…¼ë€ í”„ë ˆì´ë°
- ëª…ì˜ˆí›¼ì† ë¦¬ìŠ¤í¬ í‚¤ì›Œë“œ

ì•ˆì „í•œ ëŒ€ì²´ í‘œí˜„:
- "K-pop idol" (ì‹¤ëª… âŒ)
- "major agency" (êµ¬ì²´ì  íšŒì‚¬ëª… âŒ)
- "top celebrity" (ì‹¤ëª… âŒ)
- "government policy" (Xë¶€ì²˜ âŒ)
- "tech platform" (êµ¬ì²´ì  ì„œë¹„ìŠ¤ëª… âŒ)

ì¡°ê±´ë¶€ í—ˆìš© (3ì¡°ê±´ ëª¨ë‘ ì¶©ì¡± ì‹œë§Œ):
1. ì‚¬ë²•/í–‰ì •ì ìœ¼ë¡œ ê²°ë¡  ë‚œ ì‚¬ê±´
2. ëª¨ë“  ì„œìˆ ì´ íŒ©íŠ¸ ë‚˜ì—´ë§Œ
3. ê°ì • í”„ë ˆì´ë° ì œê±°

ê° í‚¤ì›Œë“œì— ë¦¬ìŠ¤í¬ ë ˆë²¨ í‘œì‹œ:
- "risk_level": "safe" (AdSense/í”Œë«í¼ ì•ˆì „)
- "risk_level": "caution" (ì‚¬ì‹¤ í™•ì¸ í•„ìˆ˜)
- "risk_level": "high_risk" (ë²•ì  ê²€í†  í•„ìš”)

ê° í‚¤ì›Œë“œì— ì‹¤ëª… ì •ì±… í‘œì‹œ:
- "name_policy": "no_real_names" (ê¸°ë³¸ê°’, ì‹¤ëª… ì‚¬ìš© ë¶ˆê°€)
- "name_policy": "generic_only" (ë²”ì£¼Â·ì—­í• ë§Œ í—ˆìš©)

**ì¤‘ë³µ ë°©ì§€ ê·œì¹™:**
- Intent signals: STATE_CHANGE, PROMISE_BROKEN, SILENCE, DEADLINE_LOST, COMPARISON
- ê°™ì€ signalì„ ê°€ì§„ í‚¤ì›Œë“œëŠ” ì–¸ì–´ë‹¹ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ
- 5ê°œ signalì„ ì–¸ì–´ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°

**ë°˜ë“œì‹œ ì •í™•íˆ {count}ê°œì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ë¼:**
- ì˜ì–´(en): {per_lang}ê°œ
- í•œêµ­ì–´(ko): {per_lang}ê°œ
- ì¼ë³¸ì–´(ja): {per_lang}ê°œ
- ì´í•©: ì •í™•íˆ {count}ê°œ

ê° ì–¸ì–´ ë‚´ì—ì„œ 5ê°œ ì¹´í…Œê³ ë¦¬(tech, business, lifestyle, society, entertainment)ë¥¼ ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ë˜,
ë°˜ë“œì‹œ ì´ {count}ê°œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ìµœìš°ì„ ì´ë‹¤."""


class KeywordCurator:
    def __init__(self, api_key: str = None, google_api_key: str = None, google_cx: str = None):
        """Initialize keyword curator with Claude API and Google Custom Search"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found")

        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")

        if not self.google_api_key or not self.google_cx:
            print("âš ï¸  Google Custom Search credentials not found")
            print("   Set GOOGLE_API_KEY and GOOGLE_CX environment variables")
            print("   Falling back to Claude-only mode")

        self.client = Anthropic(api_key=self.api_key)
        self.model = "claude-sonnet-4-20250514"

        # Load existing queue
        self.queue_path = Path("data/topics_queue.json")
        self.queue_data = self._load_queue()

    def _load_queue(self) -> Dict:
        """Load existing topic queue"""
        if not self.queue_path.exists():
            return {"topics": []}

        with open(self.queue_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_queue(self):
        """Save updated topic queue"""
        with open(self.queue_path, 'w', encoding='utf-8') as f:
            json.dump(self.queue_data, f, indent=2, ensure_ascii=False)

    def detect_intent_signals(self, query: str) -> list:
        """Detect intent signals from query for deduplication"""
        signals = []

        # State transition patterns
        if any(word in query.lower() for word in ["after", "ê°‘ìê¸°", "suddenly", "çªç„¶", "overnight"]):
            signals.append("STATE_CHANGE")

        # Promise broken patterns
        if any(word in query.lower() for word in ["promised", "supposed to", "ì•½ì†", "ç™ºè¡¨", "denied", "ê±°ë¶€", "æ‹’å¦"]):
            signals.append("PROMISE_BROKEN")

        # Silence patterns
        if any(word in query.lower() for word in ["no response", "ignored", "èª¬æ˜ãªã—", "ë¬´ì‘ë‹µ", "ì¹¨ë¬µ"]):
            signals.append("SILENCE")

        # Deadline/time loss patterns
        if any(word in query.lower() for word in ["deadline", "too late", "ë§ˆê°", "æœŸé™", "ë†“ì¹¨", "é€ƒã—"]):
            signals.append("DEADLINE_LOST")

        # Comparison/injustice patterns
        if any(word in query.lower() for word in ["others got", "only me", "ë‚˜ë§Œ", "è‡ªåˆ†ã ã‘"]):
            signals.append("COMPARISON")

        return signals if signals else ["GENERAL"]

    def fetch_trending_topics(self) -> str:
        """Fetch trending topics using Google Custom Search API"""
        if not self.google_api_key or not self.google_cx:
            return "No trending data available (Google API not configured)"

        print(f"\n{'='*60}")
        print(f"  ğŸ” Fetching trending topics from Google...")
        print(f"{'='*60}\n")

        # Search queries for high-CPC, emotion-driven keywords
        # Strategy: STATE TRANSITIONS (ìƒíƒœ ì „í™˜) + EXPECTATION COLLAPSE (ê¸°ëŒ€ ë¶•ê´´)
        # Focus: "after X", "but Y", "suddenly Z", "no response", "others got"
        search_queries = [
            # Tech - State Transition + Silence (ìƒíƒœ ì „í™˜ + ì¹¨ë¬µ)
            "account banned after update no response",
            "service outage promised compensation denied",
            "ì•± ì—…ë°ì´íŠ¸ í›„ ê°‘ìê¸° ë¨¹í†µ",
            "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåœæ­¢ ç†ç”±èª¬æ˜ãªã—",

            # Business - Deadline Loss + Others Got (ì‹œê°„ ì†ì‹¤ + ë¹„êµ ë¶„ë…¸)
            "class action deadline passed too late",
            "refund promised but denied suddenly",
            "ì§‘ë‹¨ì†Œì†¡ ì‹ ì²­ ë§ˆê° ë†“ì¹¨",
            "è¿”é‡‘ç´„æŸã—ãŸãŒ æ‹’å¦ã•ã‚ŒãŸ",

            # Society - Expectation Collapse (ê¸°ëŒ€ ë¶•ê´´)
            "government support supposed to but denied",
            "new policy suddenly stricter than announced",
            "ì •ë¶€ì§€ì› ì¡°ê±´ ë°œí‘œì™€ ë‹¤ë¦„",
            "æ”¿åºœæ”¯æ´ çªç„¶ æ¡ä»¶å³ã—ã",

            # Entertainment - Action â†’ Rejection (í–‰ë™ â†’ ê±°ë¶€)
            "celebrity apology issued but backlash continues",
            "idol agency promised explanation ignored fans",
            "ì‚¬ê³¼ë¬¸ ëƒˆì§€ë§Œ ë…¼ë€ ê³„ì†",
            "è¬ç½ªæ–‡å‡ºã—ãŸãŒ ç‚ä¸Šç¶šã",

            # Lifestyle - Safety Promise Broken (ì•ˆì „ ì•½ì† ë¶•ê´´)
            "product recall announced but no refund",
            "food contamination others got compensated only me",
            "ë¦¬ì½œ ë°œí‘œí–ˆëŠ”ë° í™˜ë¶ˆ ê±°ë¶€",
            "ãƒªã‚³ãƒ¼ãƒ«ç™ºè¡¨ è¿”é‡‘å¯¾å¿œãªã—"
        ]

        all_results = []
        for query in search_queries:
            try:
                url = "https://www.googleapis.com/customsearch/v1"
                params = {
                    "key": self.google_api_key,
                    "cx": self.google_cx,
                    "q": query,
                    "num": 5  # Get top 5 results per query
                }

                response = requests.get(url, params=params)
                response.raise_for_status()

                data = response.json()

                if "items" in data:
                    # Detect intent signals for this query
                    signals = self.detect_intent_signals(query)

                    for item in data["items"]:
                        all_results.append({
                            "query": query,
                            "signals": signals,  # Add intent signals
                            "title": item.get("title", ""),
                            "snippet": item.get("snippet", ""),
                            "link": item.get("link", "")
                        })

                print(f"  âœ“ Fetched {len(data.get('items', []))} results for: {query}")

            except requests.exceptions.RequestException as e:
                print(f"  âš ï¸  Error fetching results for '{query}': {e}")
                continue

        print(f"\nâœ… Total {len(all_results)} trending topics fetched\n")

        # Format results for Claude
        trends_summary = "\n\n".join([
            f"Query: {r['query']}\nTitle: {r['title']}\nSnippet: {r['snippet']}\n"
            for r in all_results
        ])

        return trends_summary

    def filter_by_risk(self, candidates: List[Dict]) -> List[Dict]:
        """Filter out high-risk keywords automatically"""
        safe_candidates = []
        filtered_count = 0

        for kw in candidates:
            # Auto-reject high-risk
            if kw.get("risk_level") == "high_risk":
                filtered_count += 1
                print(f"  ğŸ”´ Filtered high-risk: {kw.get('keyword', 'unknown')}")
                continue

            # Flag caution items for manual review
            if kw.get("risk_level") == "caution":
                kw["needs_review"] = True
                print(f"  ğŸŸ¡ Caution flagged: {kw.get('keyword', 'unknown')}")

            safe_candidates.append(kw)

        if filtered_count > 0:
            print(f"\nâš ï¸  {filtered_count} high-risk keywords filtered out\n")

        return safe_candidates

    def generate_candidates(self, count: int = 15) -> List[Dict]:
        """Generate keyword candidates using Claude API with trending data"""
        print(f"\n{'='*60}")
        print(f"  ğŸ” Generating {count} keyword candidates...")
        print(f"{'='*60}\n")

        # Fetch trending topics from Google
        trends_data = self.fetch_trending_topics()

        # Calculate per-language count
        per_lang = count // 3  # Distribute evenly across 3 languages

        # Generate prompt with trending data
        prompt = CURATION_PROMPT_WITH_TRENDS.format(
            trends_data=trends_data,
            count=count,
            per_lang=per_lang
        )

        response = self.client.messages.create(
            model=self.model,
            max_tokens=8000,  # Increased for larger outputs
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        # Parse JSON response
        content = response.content[0].text.strip()

        # Extract JSON from markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        try:
            candidates = json.loads(content)
        except json.JSONDecodeError as e:
            print(f"âŒ Failed to parse JSON response: {e}")
            print(f"Raw response:\n{content[:500]}")
            sys.exit(1)

        print(f"âœ… Generated {len(candidates)} candidates\n")

        # Apply risk filtering
        filtered_candidates = self.filter_by_risk(candidates)

        return filtered_candidates

    def display_candidates(self, candidates: List[Dict]):
        """Display candidates with numbered list"""
        print(f"{'='*60}")
        print(f"  ğŸ“‹ Keyword Candidates")
        print(f"{'='*60}\n")

        # Group by language
        by_lang = {"en": [], "ko": [], "ja": []}
        for c in candidates:
            lang = c.get("language", "en")
            by_lang[lang].append(c)

        idx = 1
        lang_names = {"en": "English", "ko": "Korean", "ja": "Japanese"}

        for lang in ["en", "ko", "ja"]:
            if by_lang[lang]:
                print(f"\n[{lang_names[lang]}]")
                print("-" * 60)

                for candidate in by_lang[lang]:
                    type_emoji = "ğŸ”¥" if candidate.get("keyword_type") == "trend" else "ğŸŒ²"
                    comp_emoji = {
                        "low": "ğŸŸ¢",
                        "medium": "ğŸŸ¡",
                        "high": "ğŸ”´"
                    }.get(candidate.get("competition_level", "medium"), "âšª")

                    print(f"\n{idx}. {type_emoji} {candidate['keyword']}")
                    print(f"   Category: {candidate['category']} | Competition: {comp_emoji} {candidate.get('competition_level', 'N/A')}")
                    print(f"   Intent: {candidate['search_intent']}")
                    print(f"   Angle: {candidate['angle']}")
                    print(f"   Why: {candidate.get('why_it_works', 'N/A')[:80]}...")

                    idx += 1

        print(f"\n{'='*60}\n")

    def interactive_selection(self, candidates: List[Dict]) -> List[Dict]:
        """Interactive selection of keywords"""
        print("ì–´ë–¤ í‚¤ì›Œë“œë¥¼ íì— ì¶”ê°€í• ê¹Œìš”?")
        print("ìˆ«ìë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3,5,7,10)")
        print("ë˜ëŠ” 'all'ì„ ì…ë ¥í•˜ë©´ ì „ë¶€ ì¶”ê°€ë©ë‹ˆë‹¤.")
        print("'q'ë¥¼ ì…ë ¥í•˜ë©´ ì·¨ì†Œí•©ë‹ˆë‹¤.\n")

        while True:
            user_input = input("ì„ íƒ: ").strip()

            if user_input.lower() == 'q':
                print("âŒ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return []

            if user_input.lower() == 'all':
                return candidates

            try:
                # Parse selected indices
                selected_indices = [int(x.strip()) for x in user_input.split(',')]

                # Validate indices
                if any(idx < 1 or idx > len(candidates) for idx in selected_indices):
                    print(f"âš ï¸  ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(candidates)} ë²”ìœ„ë¡œ ì…ë ¥í•˜ì„¸ìš”.\n")
                    continue

                # Convert to 0-based index and return selected candidates
                selected = [candidates[idx - 1] for idx in selected_indices]
                return selected

            except ValueError:
                print("âš ï¸  ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. ì˜ˆ: 1,3,5\n")

    def add_to_queue(self, selected: List[Dict]):
        """Add selected keywords to topic queue"""
        if not selected:
            print("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\n{'='*60}")
        print(f"  ğŸ’¾ íì— {len(selected)}ê°œ í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
        print(f"{'='*60}\n")

        # Get next ID
        existing_ids = [int(t['id'].split('-')[0]) for t in self.queue_data['topics'] if t['id'].split('-')[0].isdigit()]
        next_id = max(existing_ids) + 1 if existing_ids else 1

        added_count = 0
        for candidate in selected:
            # Generate topic ID
            topic_id = f"{next_id:03d}-{candidate['language']}-{candidate['category']}-{candidate['keyword'][:20].replace(' ', '-')}"

            # Create topic entry
            topic = {
                "id": topic_id,
                "keyword": candidate['keyword'],
                "category": candidate['category'],
                "lang": candidate['language'],
                "priority": candidate.get('priority', 7),
                "status": "pending",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "keyword_type": candidate.get('keyword_type', 'evergreen'),
                "search_intent": candidate.get('search_intent', ''),
                "angle": candidate.get('angle', ''),
                "competition_level": candidate.get('competition_level', 'medium')
            }

            # Add expiry_days for trend keywords
            if topic['keyword_type'] == 'trend':
                topic['expiry_days'] = 21  # 3 weeks expiry

            self.queue_data['topics'].append(topic)

            type_label = "ğŸ”¥ Trend" if topic['keyword_type'] == 'trend' else "ğŸŒ² Evergreen"
            print(f"  âœ“ Added: {type_label} | {candidate['keyword']}")

            added_count += 1
            next_id += 1

        # Save queue
        self._save_queue()

        print(f"\nâœ… {added_count}ê°œ í‚¤ì›Œë“œê°€ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“Š Total topics in queue: {len(self.queue_data['topics'])}")

        # Show statistics
        self._show_queue_stats()

    def _show_queue_stats(self):
        """Show queue statistics"""
        topics = self.queue_data['topics']

        # Count by status
        by_status = {"pending": 0, "in_progress": 0, "completed": 0}
        for t in topics:
            by_status[t.get('status', 'pending')] += 1

        # Count by type
        by_type = {"trend": 0, "evergreen": 0, "unknown": 0}
        for t in topics:
            ktype = t.get('keyword_type', 'unknown')
            by_type[ktype] = by_type.get(ktype, 0) + 1

        # Count by language
        by_lang = {"en": 0, "ko": 0, "ja": 0}
        for t in topics:
            lang = t.get('lang', 'en')
            by_lang[lang] = by_lang.get(lang, 0) + 1

        print(f"\n{'='*60}")
        print(f"  ğŸ“Š Queue Statistics")
        print(f"{'='*60}")
        print(f"  Status: Pending={by_status['pending']}, In Progress={by_status['in_progress']}, Completed={by_status['completed']}")
        print(f"  Type: ğŸ”¥ Trend={by_type['trend']}, ğŸŒ² Evergreen={by_type['evergreen']}, Unknown={by_type['unknown']}")
        print(f"  Language: EN={by_lang['en']}, KO={by_lang['ko']}, JA={by_lang['ja']}")
        print(f"{'='*60}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Keyword Curator for blog content")
    parser.add_argument('--count', type=int, default=15, help="Number of candidates to generate (default: 15)")
    args = parser.parse_args()

    # Check API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        print("Error: ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)

    # Initialize curator
    curator = KeywordCurator()

    # Generate candidates
    candidates = curator.generate_candidates(count=args.count)

    # Display candidates
    curator.display_candidates(candidates)

    # Interactive selection
    selected = curator.interactive_selection(candidates)

    # Add to queue
    if selected:
        curator.add_to_queue(selected)

    print("\nâœ¨ Done!\n")


if __name__ == "__main__":
    main()
