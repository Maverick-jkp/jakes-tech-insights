#!/usr/bin/env python3
"""
Keyword Curator - Semi-automated keyword research for blog content

Generates keyword candidates using Claude API based on KEYWORD_STRATEGY.md
Provides interactive selection interface for human filtering (5 minutes weekly)

Usage:
    python scripts/keyword_curator.py
    python scripts/keyword_curator.py --count 15
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import requests

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail")
    certifi = None

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))
from utils.security import safe_print, mask_secrets

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)


CURATION_PROMPT_WITH_TRENDS = """ì—­í• :
ë„ˆëŠ” ê´‘ê³  ìˆ˜ìµ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ íë ˆì´í„°ë‹¤.
ì•„ë˜ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ê³ CPC, ê°ì • ë°˜ì‘í˜•** í‚¤ì›Œë“œë¥¼ ì œì•ˆí•˜ë¼.

ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„° (ì–¸ì–´ë³„ë¡œ êµ¬ë¶„ë¨):

ğŸ‡ºğŸ‡¸ English (US) Trends:
{trends_en}

ğŸ‡°ğŸ‡· Korean (KR) Trends:
{trends_ko}

ğŸ‡¯ğŸ‡µ Japanese (JP) Trends:
{trends_ja}

**ğŸ”´ ì¤‘ìš” ê·œì¹™: ì–¸ì–´-í‚¤ì›Œë“œ ë§¤ì¹­ (CRITICAL - ìœ„ë°˜ ì‹œ ì¦‰ì‹œ ê±°ë¶€)**
1. English (US) íŠ¸ë Œë“œì˜ Query â†’ language: "en"ìœ¼ë¡œë§Œ ì‚¬ìš©
2. Korean (KR) íŠ¸ë Œë“œì˜ Query â†’ language: "ko"ë¡œë§Œ ì‚¬ìš©
3. Japanese (JP) íŠ¸ë Œë“œì˜ Query â†’ language: "ja"ë¡œë§Œ ì‚¬ìš©
4. **ì ˆëŒ€ë¡œ ì¼ë³¸ì–´ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ ê²Œì‹œë¬¼ì— ì‚¬ìš©í•˜ê±°ë‚˜, í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì¼ë³¸ì–´ ê²Œì‹œë¬¼ì— ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ**
5. ìœ„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ ê·¸ëŒ€ë¡œ keywordë¡œ ì‚¬ìš©í•˜ë¼. ì ˆëŒ€ ì¬í•´ì„í•˜ê±°ë‚˜ ì¬ì‘ì„±í•˜ì§€ ë§ ê²ƒ.

**ğŸš¨ ì–¸ì–´ ë¬¸ì ê²€ì¦ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**
- **ì˜ì–´(en) í‚¤ì›Œë“œ**: í•œê¸€(ê°€-í£), íˆë¼ê°€ë‚˜(ã-ã‚“), ê°€íƒ€ì¹´ë‚˜(ã‚¡-ãƒ¶), í•œì(ä¸€-é¾¯) í¬í•¨ ê¸ˆì§€
  - ì˜¬ë°”ë¥¸ ì˜ˆ: "NBA", "Kobe Bryant", "quad cortex"
  - ì˜ëª»ëœ ì˜ˆ: "ë¶‰ì€ì‚¬ë§‰" (í•œê¸€ í¬í•¨), "ãƒ•ã‚©ãƒ¼ãƒˆãƒŠã‚¤ãƒˆ" (ê°€íƒ€ì¹´ë‚˜ í¬í•¨)
- **í•œêµ­ì–´(ko) í‚¤ì›Œë“œ**: ë°˜ë“œì‹œ í•œê¸€(ê°€-í£) í¬í•¨ í•„ìš”
  - ì˜¬ë°”ë¥¸ ì˜ˆ: "ë¶‰ì€ì‚¬ë§‰", "ê¹€ì—°ì•„", "u23" (ì˜ë¬¸ ì•½ì–´ëŠ” í—ˆìš©)
  - ì˜ëª»ëœ ì˜ˆ: "red desert" (í•œê¸€ ì—†ìŒ), "ãƒ•ã‚©ãƒ¼ãƒˆãƒŠã‚¤ãƒˆ" (ì¼ë³¸ì–´)
- **ì¼ë³¸ì–´(ja) í‚¤ì›Œë“œ**: ë°˜ë“œì‹œ íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜/í•œì í¬í•¨ í•„ìš”
  - ì˜¬ë°”ë¥¸ ì˜ˆ: "ãƒ•ã‚©ãƒ¼ãƒˆãƒŠã‚¤ãƒˆ", "ä¸‰ç¬˜è–«", "åœ°éœ‡é€Ÿå ±"
  - ì˜ëª»ëœ ì˜ˆ: "fortnite" (ì¼ë³¸ì–´ ë¬¸ì ì—†ìŒ), "ë¶‰ì€ì‚¬ë§‰" (í•œê¸€)

ëª©í‘œ:
í•œêµ­ì–´ / ì˜ì–´ / ì¼ë³¸ì–´ ê°ê°ì—ì„œ
**ë¶ˆì•ˆ, ë¶„ë…¸, ê¶ê¸ˆì¦**ì„ ìœ ë°œí•˜ëŠ” í‚¤ì›Œë“œë§Œ ì œì•ˆí•˜ë¼.

ê¸ˆì§€:
- ì¶”ìƒì ì¸ íŠ¸ë Œë“œ ìš”ì•½ ("AI íŠ¸ë Œë“œ", "ìƒˆë¡œìš´ ê¸°ìˆ ")
- êµìœ¡/ì •ë³´ì„± í‚¤ì›Œë“œ ("~í•˜ëŠ” ë°©ë²•", "~ë€ ë¬´ì—‡ì¸ê°€")
- ê¸ì •ì ì´ê³  í‰í™”ë¡œìš´ í‚¤ì›Œë“œ
- **Queryë¥¼ ì¬í•´ì„í•˜ê±°ë‚˜ ë‹¤ì‹œ ì“°ëŠ” ê²ƒ**
- **ê°™ì€ í‚¤ì›Œë“œë¥¼ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¡œ ì¤‘ë³µ ì œì•ˆí•˜ëŠ” ê²ƒ** (ì˜ˆ: "ç›¸è‘‰é›…ç´€"ë¥¼ techì™€ society ëª¨ë‘ì— ì œì•ˆí•˜ì§€ ë§ ê²ƒ. í•˜ë‚˜ì˜ í‚¤ì›Œë“œëŠ” í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ë§Œ ê°€ì ¸ì•¼ í•¨)

ì¶œë ¥ í˜•ì‹:
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼.

[
  {{
    "keyword": "ìœ„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì¬í•´ì„ ê¸ˆì§€)",
    "raw_search_title": "ì‚¬ìš©ìê°€ êµ¬ê¸€ì— ê²€ìƒ‰í•  ë•Œ ì •í™•íˆ ì…ë ¥í•˜ëŠ” ê²€ìƒ‰ì–´ (keywordì™€ ë™ì¼í•˜ê²Œ)",
    "editorial_title": "ê¸°ì‚¬ ì œëª© í˜•ì‹ì˜ ë…ì ì¹œí™”ì  ì œëª©",
    "core_fear_question": "ì‚¬ìš©ìì˜ í•µì‹¬ ë‘ë ¤ì›€ì„ ë‹´ì€ ì§ˆë¬¸ í•œ ë¬¸ì¥",
    "language": "ko",
    "category": "tech",  # or: business, lifestyle, society, entertainment, sports, finance, education
    "search_intent": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ ë‹¹ì¥ ê²€ìƒ‰í•˜ëŠ” ì´ìœ  (í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì—‡ì„ ìƒëŠ”ì§€)",
    "angle": "ì´ í‚¤ì›Œë“œë¥¼ ë‹¤ë£° ë•Œì˜ ê´€ì ",
    "competition_level": "low",
    "why_it_works": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ì˜êµ¬ì ìœ¼ë¡œ ë¬´ì—‡ì„ ìƒëŠ”ì§€ (ë§ˆê°/ê¸°íšŒ ì†ì‹¤ ì¤‘ì‹¬)",
    "purpose": "high competitionì¸ ê²½ìš°ì—ë§Œ: Traffic acquisition / Brand positioning / Viral content ì¤‘ í•˜ë‚˜",
    "keyword_type": "trend",
    "priority": 7,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "intent_signal": "STATE_CHANGE"
  }}
]

ì¤‘ìš”:
- keyword_typeì€ ë¬´ì¡°ê±´ "trend"ë§Œ ì‚¬ìš© (evergreen ê¸ˆì§€)
- categoryëŠ” "tech", "business", "lifestyle", "society", "entertainment", "sports", "finance", "education" ì¤‘ í•˜ë‚˜ (8ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ)
- languageëŠ” "en", "ko", "ja" ì¤‘ í•˜ë‚˜ (3ê°œ ì–¸ì–´ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ)
- competition_levelì€ "low", "medium", "high" ì¤‘ í•˜ë‚˜
- priorityëŠ” 1-10 ì‚¬ì´ì˜ ìˆ«ì (ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
- risk_levelì€ "safe", "caution", "high_risk" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "safe")
- name_policyëŠ” "no_real_names", "generic_only" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "no_real_names")
- intent_signalì€ "STATE_CHANGE", "PROMISE_BROKEN", "SILENCE", "DEADLINE_LOST", "COMPARISON" ì¤‘ í•˜ë‚˜
- ì§€ê¸ˆ ì‹œì (2026ë…„ 1ì›”)ì—ì„œ í˜„ì‹¤ì ì¸ í‚¤ì›Œë“œë§Œ ì œì•ˆ
- ì˜ˆì‹œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì‹¤ì œ ê²€ìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ í‚¤ì›Œë“œë§Œ ì œì•ˆ
- **ì¤‘ìš”**: ìœ„ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ keyword í•„ë“œì— ê·¸ëŒ€ë¡œ ë³µì‚¬í•  ê²ƒ
- **keyword í•„ë“œëŠ” ì ˆëŒ€ ì¬ì‘ì„±í•˜ì§€ ë§ê³  Queryë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- **ì¤‘ìš”**: 8ê°œ ì¹´í…Œê³ ë¦¬(tech, business, lifestyle, society, entertainment, sports, finance, education)ë¥¼ ë°˜ë“œì‹œ ê³ ë¥´ê²Œ ë¶„ë°°í•  ê²ƒ

**ğŸ”´ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê°€ì´ë“œ (CRITICAL - ë°˜ë“œì‹œ ì¤€ìˆ˜):**
- **sports**: ëª¨ë“  ìš´ë™ ê²½ê¸°, ì„ ìˆ˜, íŒ€ (ì¶•êµ¬, ì•¼êµ¬, ë†êµ¬, í…Œë‹ˆìŠ¤, ê³¨í”„, UFC/ê²©íˆ¬ê¸°, eìŠ¤í¬ì¸ , U23/ì²­ì†Œë…„ ìŠ¤í¬ì¸ , ì˜¬ë¦¼í”½, ì›”ë“œì»µ ë“±)
  - ì˜ˆì‹œ: "UFC", "u23", "ì†í¥ë¯¼", "KBO", "NBA", "wimbledon", "world cup"
  - **ì¤‘ìš”**: ê²©íˆ¬ê¸°(UFC, ë³µì‹±), ì²­ì†Œë…„ ìŠ¤í¬ì¸ (U23, U21)ë„ ë°˜ë“œì‹œ sports ì¹´í…Œê³ ë¦¬
- **entertainment**: ì˜í™”, ë“œë¼ë§ˆ, ìŒì•…, ì˜ˆëŠ¥, ì—°ì˜ˆì¸ (ë‹¨, ìŠ¤í¬ì¸  ì„ ìˆ˜ëŠ” ì œì™¸)
  - ì˜ˆì‹œ: "ë„·í”Œë¦­ìŠ¤", "BTS", "ì˜¤ì§•ì–´ê²Œì„", "ê¹€ì—°ì•„ ì˜ˆëŠ¥ ì¶œì—°" (ìŠ¤í¬ì¸  ì„ ìˆ˜ê°€ ì˜ˆëŠ¥ì— ë‚˜ì˜¨ ê²½ìš°)
- **society**: ì‚¬íšŒ ì´ìŠˆ, ì •ì¹˜, ì •ì±…, ë²”ì£„, ì¬ë‚œ (ë‹¨, ìŠ¤í¬ì¸  ê´€ë ¨ ì‚¬íšŒ ì´ìŠˆë„ sportsë¡œ ë¶„ë¥˜)
  - ì˜ˆì‹œ: "ì§€ì§„ì†ë³´", "ì •ë¶€ ì •ì±…", "ì‚¬íšŒ ë¬¸ì œ"
  - **ì£¼ì˜**: "U23 ëŒ€í‘œíŒ€"ì€ societyê°€ ì•„ë‹ˆë¼ sportsì…ë‹ˆë‹¤
- **tech**: ê¸°ìˆ , IT, AI, ê²Œì„, ì•±, ì†Œí”„íŠ¸ì›¨ì–´
- **business**: ê²½ì œ, ê¸°ì—…, ì£¼ì‹, ë¶€ë™ì‚°, ì°½ì—…
- **lifestyle**: ì¼ìƒ, ê±´ê°•, ì—¬í–‰, ìŒì‹, íŒ¨ì…˜
- **finance**: ê¸ˆìœµ, íˆ¬ì, ì„¸ê¸ˆ, ë³´í—˜, ì—°ê¸ˆ
- **education**: êµìœ¡, ëŒ€í•™, ì…ì‹œ, ìê²©ì¦, í•™ìŠµ

ì–¸ì–´ë³„ í†¤ ì°¨ì´:
- ğŸ‡ºğŸ‡¸ English: rights, compensation, legal leverage, lawsuits ì¤‘ì‹¬
- ğŸ‡°ğŸ‡· Korean: ë¶ˆê³µì •, ì¢Œì ˆ, ì†Œë¹„ì ë³´í˜¸, ì±…ì„ ì¶”ê¶ ì¤‘ì‹¬
- ğŸ‡¯ğŸ‡µ Japanese: ë¶ˆíˆ¬ëª…ì„±, ê³µì‹ ì ˆì°¨, ì ì ˆí•œ ëŒ€ì‘ ë°©ë²• ì¤‘ì‹¬

**ğŸ”´ ì•ˆì „ ê°€ì´ë“œë¼ì¸:**

ì£¼ì˜ì‚¬í•­:
- ëª…ì˜ˆí›¼ì†/ë¹„ë‚œ/ë¹„ë°© í‘œí˜„ ê¸ˆì§€
- ì‚¬ì‹¤ ê¸°ë°˜ì˜ trending í‚¤ì›Œë“œëŠ” ì‹¤ëª… ì‚¬ìš© ê°€ëŠ¥

ê° í‚¤ì›Œë“œì— ë¦¬ìŠ¤í¬ ë ˆë²¨ í‘œì‹œ:
- "risk_level": "safe" (ê¸°ë³¸ê°’)
- "risk_level": "caution" (ë…¼ë€ ê°€ëŠ¥ì„± ìˆìŒ)

ê° í‚¤ì›Œë“œì— ì‹¤ëª… ì •ì±… í‘œì‹œ:
- "name_policy": "no_real_names" (ì‹¤ëª… ë¶ˆí•„ìš”)
- "name_policy": "real_names_ok" (trending ë‰´ìŠ¤ ë“± ì‹¤ëª… í¬í•¨ ê°€ëŠ¥)

**ì¤‘ë³µ ë°©ì§€ ê·œì¹™:**
- Intent signals: STATE_CHANGE, PROMISE_BROKEN, SILENCE, DEADLINE_LOST, COMPARISON
- ê°™ì€ signalì„ ê°€ì§„ í‚¤ì›Œë“œëŠ” ì–¸ì–´ë‹¹ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ
- 5ê°œ signalì„ ì–¸ì–´ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°

**ğŸš¨ ì–¸ì–´ë³„ í‚¤ì›Œë“œ ìƒì„± ê·œì¹™ (ì ˆëŒ€ ì¤€ìˆ˜):**
ë°˜ë“œì‹œ ì •í™•íˆ {count}ê°œì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ë¼:
- ì˜ì–´(en): ì •í™•íˆ {per_lang}ê°œ (1ê°œë¼ë„ ë¶€ì¡±í•˜ê±°ë‚˜ ì´ˆê³¼í•˜ë©´ ì•ˆ ë¨)
- í•œêµ­ì–´(ko): ì •í™•íˆ {per_lang}ê°œ (1ê°œë¼ë„ ë¶€ì¡±í•˜ê±°ë‚˜ ì´ˆê³¼í•˜ë©´ ì•ˆ ë¨)
- ì¼ë³¸ì–´(ja): ì •í™•íˆ {per_lang}ê°œ (1ê°œë¼ë„ ë¶€ì¡±í•˜ê±°ë‚˜ ì´ˆê³¼í•˜ë©´ ì•ˆ ë¨)
- ì´í•©: ì •í™•íˆ {count}ê°œ

**ì–¸ì–´ë³„ íŠ¸ë Œë“œ ë°ì´í„° ì‚¬ìš© ê·œì¹™:**
- ğŸ‡ºğŸ‡¸ English (US) Trendsì—ì„œ {per_lang}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ language: "en"
- ğŸ‡°ğŸ‡· Korean (KR) Trendsì—ì„œ {per_lang}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ language: "ko"
- ğŸ‡¯ğŸ‡µ Japanese (JP) Trendsì—ì„œ {per_lang}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ language: "ja"
- ë§Œì•½ í•œ ì–¸ì–´ì˜ íŠ¸ë Œë“œê°€ ë¶€ì¡±í•˜ë©´, ë‹¤ë¥¸ ì–¸ì–´ íŠ¸ë Œë“œë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³  í•´ë‹¹ ì–¸ì–´ë¡œ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ë¼

ê° ì–¸ì–´ ë‚´ì—ì„œ 8ê°œ ì¹´í…Œê³ ë¦¬(tech, business, lifestyle, society, entertainment, sports, finance, education)ë¥¼ ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ë˜,
ë°˜ë“œì‹œ ê° ì–¸ì–´ë³„ë¡œ ì •í™•íˆ {per_lang}ê°œì”© ìƒì„±í•˜ëŠ” ê²ƒì´ ìµœìš°ì„ ì´ë‹¤."""


class KeywordCurator:
    def __init__(self, api_key: str = None, google_api_key: str = None, google_cx: str = None):
        """Initialize keyword curator with Claude API and Google Custom Search"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("âŒ ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError("ANTHROPIC_API_KEY not found")

        # Brave Search API (replacing Google Custom Search)
        self.brave_api_key = os.environ.get("BRAVE_API_KEY")

        # Keep Google API keys for backward compatibility (deprecated)
        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")

        if not self.brave_api_key:
            safe_print("âš ï¸  Brave Search API key not found")
            safe_print("   Set BRAVE_API_KEY environment variable")
            safe_print("   Falling back to Claude-only mode")
            if self.google_api_key and self.google_cx:
                safe_print("   Note: Google Custom Search API is deprecated for new users")

        try:
            self.client = Anthropic(api_key=self.api_key)
            self.model = "claude-sonnet-4-20250514"
            safe_print("  âœ“ Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"âŒ ERROR: Failed to initialize Anthropic client")
            safe_print(f"   Error: {mask_secrets(str(e))}")
            raise

        # Load existing queue
        self.queue_path = Path("data/topics_queue.json")
        try:
            self.queue_data = self._load_queue()
            safe_print(f"  âœ“ Loaded topic queue: {len(self.queue_data.get('topics', []))} topics")
        except Exception as e:
            safe_print(f"âš ï¸  WARNING: Failed to load existing queue, starting fresh")
            safe_print(f"   Error: {str(e)}")
            self.queue_data = {"topics": []}

    def _load_queue(self) -> Dict:
        """Load existing topic queue"""
        if not self.queue_path.exists():
            return {"topics": []}

        with open(self.queue_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_queue(self):
        """Save updated topic queue"""
        try:
            # Ensure parent directory exists
            self.queue_path.parent.mkdir(parents=True, exist_ok=True)

            with open(self.queue_path, 'w', encoding='utf-8') as f:
                json.dump(self.queue_data, f, indent=2, ensure_ascii=False)
        except IOError as e:
            safe_print(f"âŒ ERROR: Failed to save queue to filesystem")
            safe_print(f"   Path: {self.queue_path}")
            safe_print(f"   Error: {str(e)}")
            raise
        except Exception as e:
            safe_print(f"âŒ ERROR: Unexpected error saving queue")
            safe_print(f"   Error: {str(e)}")
            raise

    def detect_intent_signals(self, query: str) -> list:
        """Detect intent signals from query for deduplication"""
        signals = []

        # State transition patterns
        if any(word in query.lower() for word in ["after", "ê°‘ìê¸°", "suddenly", "çªç„¶", "overnight"]):
            signals.append("STATE_CHANGE")

        # Promise broken patterns
        if any(word in query.lower() for word in ["promised", "supposed to", "ì•½ì†", "ç™ºè¡¨", "denied", "ê±°ë¶€", "æ‹’å¦"]):
            signals.append("PROMISE_BROKEN")

        # Silence patterns
        if any(word in query.lower() for word in ["no response", "ignored", "èª¬æ˜ãªã—", "ë¬´ì‘ë‹µ", "ì¹¨ë¬µ"]):
            signals.append("SILENCE")

        # Deadline/time loss patterns
        if any(word in query.lower() for word in ["deadline", "too late", "ë§ˆê°", "æœŸé™", "ë†“ì¹¨", "é€ƒã—"]):
            signals.append("DEADLINE_LOST")

        # Comparison/injustice patterns
        if any(word in query.lower() for word in ["others got", "only me", "ë‚˜ë§Œ", "è‡ªåˆ†ã ã‘"]):
            signals.append("COMPARISON")

        return signals if signals else ["GENERAL"]

    def fetch_trending_from_rss(self) -> Dict[str, List[str]]:
        """Fetch trending topics from Google Trends RSS feeds grouped by language"""
        import xml.etree.ElementTree as ET

        rss_urls = {
            "KR": "https://trends.google.co.kr/trending/rss?geo=KR",
            "US": "https://trends.google.co.kr/trending/rss?geo=US",
            "JP": "https://trends.google.co.kr/trending/rss?geo=JP"
        }

        # Map region to language
        region_to_lang = {
            "KR": "ko",
            "US": "en",
            "JP": "ja"
        }

        # Group trends by language
        trends_by_lang = {"ko": [], "en": [], "ja": []}

        for geo, url in rss_urls.items():
            try:
                verify_ssl = certifi.where() if certifi else True
                response = requests.get(url, timeout=10, verify=verify_ssl)
                response.raise_for_status()

                # Parse XML
                root = ET.fromstring(response.content)

                # Find all items (trending topics)
                items = root.findall('.//item')

                lang = region_to_lang[geo]
                for item in items[:5]:  # Top 5 per region (15 total)
                    title_elem = item.find('title')
                    if title_elem is not None and title_elem.text:
                        trends_by_lang[lang].append(title_elem.text.strip())

                safe_print(f"  âœ“ Found {min(len(items), 5)} trends from {geo} â†’ {lang}")

            except requests.exceptions.Timeout:
                safe_print(f"  âš ï¸  RSS fetch timeout for {geo}: Request took too long")
                continue
            except requests.exceptions.HTTPError as e:
                safe_print(f"  âš ï¸  RSS HTTP error for {geo}: {e.response.status_code if e.response else 'unknown'}")
                continue
            except ET.ParseError as e:
                safe_print(f"  âš ï¸  RSS parse error for {geo}: Invalid XML format")
                safe_print(f"     Error: {str(e)}")
                continue
            except Exception as e:
                safe_print(f"  âš ï¸  RSS fetch error for {geo}: {mask_secrets(str(e))}")
                continue

        return trends_by_lang

    def fetch_trending_topics(self) -> Dict[str, str]:
        """Fetch trending topics using Google Trends RSS feeds, grouped by language"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ”¥ Fetching REAL-TIME trending topics from Google Trends RSS...")
        safe_print(f"{'='*60}\n")

        # Try RSS feeds first (most reliable method)
        trends_by_lang = self.fetch_trending_from_rss()

        # Check if we got any trends
        total_trends = sum(len(trends) for trends in trends_by_lang.values())

        if total_trends > 0:
            safe_print(f"\n  ğŸ‰ Total {total_trends} real-time trending topics from RSS!")
            safe_print(f"     EN: {len(trends_by_lang['en'])}, KO: {len(trends_by_lang['ko'])}, JA: {len(trends_by_lang['ja'])}\n")
        else:
            safe_print("  âš ï¸  RSS feeds failed. Falling back to pattern-based queries...\n")
            # Fallback to pattern queries (grouped by language)
            trends_by_lang = {
                "en": [
                    "account banned after update no response",
                    "service outage promised compensation denied",
                    "class action deadline passed too late",
                    "refund promised but denied suddenly",
                    "government support supposed to but denied",
                    "new policy suddenly stricter than announced",
                    "celebrity apology issued but backlash continues"
                ],
                "ko": [
                    "ì•± ì—…ë°ì´íŠ¸ í›„ ê°‘ìê¸° ë¨¹í†µ",
                    "ì§‘ë‹¨ì†Œì†¡ ì‹ ì²­ ë§ˆê° ë†“ì¹¨",
                    "ì •ë¶€ì§€ì› ì¡°ê±´ ë°œí‘œì™€ ë‹¤ë¦„",
                    "ì‚¬ê³¼ë¬¸ ëƒˆì§€ë§Œ ë…¼ë€ ê³„ì†",
                    "ë¦¬ì½œ ë°œí‘œí–ˆëŠ”ë° í™˜ë¶ˆ ê±°ë¶€"
                ],
                "ja": [
                    "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåœæ­¢ ç†ç”±èª¬æ˜ãªã—",
                    "è¿”é‡‘ç´„æŸã—ãŸãŒ æ‹’å¦ã•ã‚ŒãŸ",
                    "æ”¿åºœæ”¯æ´ çªç„¶ æ¡ä»¶å³ã—ã",
                    "è¬ç½ªæ–‡å‡ºã—ãŸãŒ ç‚ä¸Šç¶šã",
                    "ãƒªã‚³ãƒ¼ãƒ«ç™ºè¡¨ è¿”é‡‘å¯¾å¿œãªã—"
                ]
            }

        # Flatten for search queries (but keep language tracking)
        all_queries = []
        for lang, queries in trends_by_lang.items():
            for query in queries:
                all_queries.append((query, lang))

        # If no Brave Search API, skip search results
        if not self.brave_api_key:
            safe_print("  ğŸš¨ CRITICAL WARNING: Brave Search API not configured")
            safe_print("  ğŸ“Œ References will NOT be generated for keywords!")
            safe_print("  ğŸ“Œ Set BRAVE_API_KEY environment variable")
            safe_print("  ğŸ“Œ OR: Add it as GitHub Secret for automated workflows\n")
            self.search_results = []

            # Format trends by language for prompt
            trends_formatted = {}
            for lang, queries in trends_by_lang.items():
                trends_formatted[lang] = "\n".join([f"Query: {q}" for q in queries[:10]])

            return trends_formatted

        all_results = []
        for query, query_lang in all_queries:
            try:
                # Brave Search API endpoint
                url = "https://api.search.brave.com/res/v1/web/search"
                headers = {
                    "Accept": "application/json",
                    "X-Subscription-Token": self.brave_api_key
                }
                params = {
                    "q": query,
                    "count": 3,  # Get top 3 results per query for better quality
                    "freshness": "pw"  # Past week (ìµœì‹  ë‰´ìŠ¤)
                }

                # Add delay to avoid rate limiting
                time.sleep(0.5)

                verify_ssl = certifi.where() if certifi else True
                response = requests.get(url, headers=headers, params=params, verify=verify_ssl)
                response.raise_for_status()

                data = response.json()

                # Brave API returns results in "web" -> "results" structure
                web_results = data.get("web", {}).get("results", [])

                if web_results:
                    # Detect intent signals for this query
                    signals = self.detect_intent_signals(query)

                    for item in web_results:
                        all_results.append({
                            "query": query,
                            "query_lang": query_lang,  # Track which language this query belongs to
                            "signals": signals,  # Add intent signals
                            "title": item.get("title", ""),
                            "snippet": item.get("description", ""),  # Brave uses "description" not "snippet"
                            "link": item.get("url", ""),  # Brave uses "url" not "link"
                            "source": item.get("url", "").split("/")[2] if item.get("url") else ""  # Extract domain
                        })

                safe_print(f"  âœ“ Fetched {len(web_results)} results for: {query}")

            except requests.exceptions.Timeout:
                safe_print(f"  âš ï¸  Timeout fetching results for '{query[:50]}...'")
                continue
            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code if e.response else 'unknown'
                safe_print(f"  âš ï¸  HTTP error ({status_code}) for '{query[:50]}...'")
                if status_code == 403:
                    safe_print(f"     âš ï¸  Brave API Access Forbidden - check API key")
                elif status_code == 429:
                    safe_print(f"     Rate limit exceeded (2000/month limit)")
                continue
            except json.JSONDecodeError:
                safe_print(f"  âš ï¸  Invalid JSON response for '{query[:50]}...'")
                continue
            except requests.exceptions.RequestException as e:
                safe_print(f"  âš ï¸  Network error for '{query[:50]}...': {mask_secrets(str(e))}")
                continue
            except Exception as e:
                safe_print(f"  âš ï¸  Unexpected error for '{query[:50]}...': {mask_secrets(str(e))}")
                continue

        safe_print(f"\nâœ… Total {len(all_results)} trending topics fetched\n")

        # Store results for reference extraction
        self.search_results = all_results

        # Format results for Claude, grouped by language
        trends_by_lang_formatted = {"en": [], "ko": [], "ja": []}
        for r in all_results:
            lang = r.get('query_lang', 'en')
            trends_by_lang_formatted[lang].append(
                f"Query: {r['query']}\nTitle: {r['title']}\nSnippet: {r['snippet']}\n"
            )

        # Convert to string format per language
        trends_formatted = {}
        for lang in ["en", "ko", "ja"]:
            trends_formatted[lang] = "\n\n".join(trends_by_lang_formatted[lang][:10])  # Top 10 per language

        return trends_formatted

    def filter_by_risk(self, candidates: List[Dict]) -> List[Dict]:
        """Filter out high-risk keywords automatically"""
        safe_candidates = []
        filtered_count = 0

        for kw in candidates:
            # Auto-reject high-risk
            if kw.get("risk_level") == "high_risk":
                filtered_count += 1
                safe_print(f"  ğŸ”´ Filtered high-risk: {kw.get('keyword', 'unknown')}")
                continue

            # Flag caution items for manual review
            if kw.get("risk_level") == "caution":
                kw["needs_review"] = True
                safe_print(f"  ğŸŸ¡ Caution flagged: {kw.get('keyword', 'unknown')}")

            safe_candidates.append(kw)

        if filtered_count > 0:
            safe_print(f"\nâš ï¸  {filtered_count} high-risk keywords filtered out\n")

        return safe_candidates

    def extract_references(self, all_results: List[Dict], keyword: str, lang: str) -> List[Dict]:
        """Extract top 3 references for a keyword based on search results"""
        # Find relevant results for this keyword
        # Match by language and keyword similarity
        relevant = []

        for result in all_results:
            query = result.get("query", "").lower()
            # Simple matching: if keyword words appear in query
            keyword_words = set(keyword.lower().split())
            query_words = set(query.split())

            # Check language match (simple heuristic)
            is_relevant = len(keyword_words & query_words) > 0

            if is_relevant:
                relevant.append(result)

        # Take top 3 unique sources
        references = []
        seen_domains = set()

        for result in relevant[:10]:  # Check first 10 relevant results
            link = result.get("link", "")
            source = result.get("source", "")
            title = result.get("title", "")

            if link and source and source not in seen_domains:
                references.append({
                    "title": title[:100],  # Truncate long titles
                    "url": link,
                    "source": source
                })
                seen_domains.add(source)

            if len(references) >= 2:  # Get only 2 references per keyword
                break

        return references

    def generate_candidates(self, count: int = 15) -> List[Dict]:
        """Generate keyword candidates using Claude API with trending data"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ” Generating {count} keyword candidates...")
        safe_print(f"{'='*60}\n")

        # Fetch trending topics from Google (store for reference extraction)
        self.search_results = []  # Store search results
        trends_by_lang = self.fetch_trending_topics()

        # Calculate per-language count
        per_lang = count // 3  # Distribute evenly across 3 languages

        # Generate prompt with trending data (grouped by language)
        prompt = CURATION_PROMPT_WITH_TRENDS.format(
            trends_en=trends_by_lang.get('en', 'No English trends available'),
            trends_ko=trends_by_lang.get('ko', 'No Korean trends available'),
            trends_ja=trends_by_lang.get('ja', 'No Japanese trends available'),
            count=count,
            per_lang=per_lang
        )

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=16000,  # Increased for 30+ keywords
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
        except Exception as e:
            safe_print(f"âŒ ERROR: Claude API call failed")
            safe_print(f"   Error: {mask_secrets(str(e))}")
            safe_print(f"   This is a critical error - cannot continue without keyword candidates")
            sys.exit(1)

        if not response or not response.content:
            safe_print(f"âŒ ERROR: Empty response from Claude API")
            safe_print(f"   This is a critical error - cannot continue without keyword candidates")
            sys.exit(1)

        # Parse JSON response
        content = response.content[0].text.strip()

        # Extract JSON from markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        try:
            candidates = json.loads(content)
        except json.JSONDecodeError as e:
            safe_print(f"âŒ ERROR: Failed to parse JSON response from Claude")
            safe_print(f"   Parse error: {str(e)}")
            safe_print(f"   Raw response (first 500 chars):\n{content[:500]}")
            safe_print(f"   This is a critical error - cannot continue with invalid JSON")
            sys.exit(1)

        safe_print(f"âœ… Generated {len(candidates)} candidates\n")

        # STEP 1: Remove duplicates (keep first occurrence, regardless of category)
        seen_keywords = {}
        dedup_candidates = []
        duplicates_removed = 0

        for candidate in candidates:
            keyword_lower = candidate.get('keyword', '').lower()
            if keyword_lower in seen_keywords:
                duplicates_removed += 1
                first_category = seen_keywords[keyword_lower]
                duplicate_category = candidate.get('category')
                safe_print(f"  ğŸ”´ DUPLICATE REMOVED: '{candidate.get('keyword')}' (duplicate category: {duplicate_category}, already exists as: {first_category})")
            else:
                # Store the category of the first occurrence
                seen_keywords[keyword_lower] = candidate.get('category')
                dedup_candidates.append(candidate)

        if duplicates_removed > 0:
            safe_print(f"\nâš ï¸  Removed {duplicates_removed} duplicate keywords from Claude's response")
            safe_print(f"    Policy: One keyword = one category (first occurrence wins)\n")

        # STEP 2: Auto-correct sports keywords category
        sports_keywords = ['vs', 'vs.', 'game', 'match', 'league', 'cup', 'tournament', 'championship',
                          'basketball', 'football', 'soccer', 'baseball', 'hockey', 'tennis', 'golf',
                          'nba', 'nfl', 'mlb', 'nhl', 'premier league', 'uefa', 'champions league',
                          'world cup', 'olympics', 'ufc', 'boxing', 'wrestling', 'mma',
                          'u23', 'u-23', 'u21', 'u-21', 'player', 'team', 'squad']

        corrected_count = 0
        for candidate in dedup_candidates:
            keyword_lower = candidate.get('keyword', '').lower()
            category = candidate.get('category', '')

            # Auto-detect sports keywords
            if category != 'sports':
                is_sports = any(sport_term in keyword_lower for sport_term in sports_keywords)
                if is_sports:
                    old_category = category
                    candidate['category'] = 'sports'
                    corrected_count += 1
                    safe_print(f"  âœ… AUTO-CORRECTED: {candidate.get('keyword')} ({old_category} â†’ sports)")

        if corrected_count > 0:
            safe_print(f"\nâœ… Auto-corrected {corrected_count} sports keywords\n")

        # Apply risk filtering
        filtered_candidates = self.filter_by_risk(dedup_candidates)

        # Extract references for each candidate
        safe_print(f"ğŸ“š Extracting references for {len(filtered_candidates)} candidates...\n")
        keywords_with_refs = 0
        keywords_without_refs = 0

        for candidate in filtered_candidates:
            keyword = candidate.get("keyword", "")
            lang = candidate.get("language", "en")
            references = self.extract_references(self.search_results, keyword, lang)
            candidate["references"] = references
            if references:
                safe_print(f"  âœ“ {len(references)} refs for: {keyword[:50]}...")
                keywords_with_refs += 1
            else:
                keywords_without_refs += 1

        safe_print("")

        # Validation warning
        if keywords_without_refs > 0:
            safe_print(f"âš ï¸  WARNING: {keywords_without_refs}/{len(filtered_candidates)} keywords have NO references")
            safe_print(f"   This means generated posts will lack credible sources!")
            if not self.google_api_key or not self.google_cx:
                safe_print(f"   ROOT CAUSE: Google Custom Search API credentials not configured")
                safe_print(f"   FIX: Set GOOGLE_API_KEY and GOOGLE_CX environment variables\n")
        else:
            safe_print(f"âœ… All {keywords_with_refs} keywords have references!\n")

        return filtered_candidates

    def display_candidates(self, candidates: List[Dict]):
        """Display candidates with numbered list"""
        safe_print(f"{'='*60}")
        safe_print(f"  ğŸ“‹ Keyword Candidates")
        safe_print(f"{'='*60}\n")

        # Group by language
        by_lang = {"en": [], "ko": [], "ja": []}
        for c in candidates:
            lang = c.get("language", "en")
            by_lang[lang].append(c)

        idx = 1
        lang_names = {"en": "English", "ko": "Korean", "ja": "Japanese"}

        for lang in ["en", "ko", "ja"]:
            if by_lang[lang]:
                safe_print(f"\n[{lang_names[lang]}]")
                safe_print("-" * 60)

                for candidate in by_lang[lang]:
                    type_emoji = "ğŸ”¥" if candidate.get("keyword_type") == "trend" else "ğŸŒ²"
                    comp_emoji = {
                        "low": "ğŸŸ¢",
                        "medium": "ğŸŸ¡",
                        "high": "ğŸ”´"
                    }.get(candidate.get("competition_level", "medium"), "âšª")

                    safe_print(f"\n{idx}. {type_emoji} {candidate['keyword']}")
                    safe_print(f"   Category: {candidate['category']} | Competition: {comp_emoji} {candidate.get('competition_level', 'N/A')}")
                    safe_print(f"   Intent: {candidate['search_intent']}")
                    safe_print(f"   Angle: {candidate['angle']}")
                    safe_print(f"   Why: {candidate.get('why_it_works', 'N/A')[:80]}...")

                    idx += 1

        safe_print(f"\n{'='*60}\n")

    def interactive_selection(self, candidates: List[Dict]) -> List[Dict]:
        """Interactive selection of keywords"""
        safe_print("ì–´ë–¤ í‚¤ì›Œë“œë¥¼ íì— ì¶”ê°€í• ê¹Œìš”?")
        safe_print("ìˆ«ìë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3,5,7,10)")
        safe_print("ë˜ëŠ” 'all'ì„ ì…ë ¥í•˜ë©´ ì „ë¶€ ì¶”ê°€ë©ë‹ˆë‹¤.")
        safe_print("'q'ë¥¼ ì…ë ¥í•˜ë©´ ì·¨ì†Œí•©ë‹ˆë‹¤.\n")

        while True:
            user_input = input("ì„ íƒ: ").strip()

            if user_input.lower() == 'q':
                safe_print("âŒ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return []

            if user_input.lower() == 'all':
                return candidates

            try:
                # Parse selected indices
                selected_indices = [int(x.strip()) for x in user_input.split(',')]

                # Validate indices
                if any(idx < 1 or idx > len(candidates) for idx in selected_indices):
                    safe_print(f"âš ï¸  ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(candidates)} ë²”ìœ„ë¡œ ì…ë ¥í•˜ì„¸ìš”.\n")
                    continue

                # Convert to 0-based index and return selected candidates
                selected = [candidates[idx - 1] for idx in selected_indices]
                return selected

            except ValueError:
                safe_print("âš ï¸  ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. ì˜ˆ: 1,3,5\n")

    def _validate_keyword_language(self, keyword: str, language: str) -> bool:
        """Validate that keyword matches the specified language"""
        import unicodedata

        def has_hangul(text):
            """Check if text contains Korean characters"""
            return any('\uac00' <= char <= '\ud7a3' for char in text)

        def has_hiragana_katakana(text):
            """Check if text contains Japanese characters"""
            return any(
                ('\u3040' <= char <= '\u309f') or  # Hiragana
                ('\u30a0' <= char <= '\u30ff')     # Katakana
                for char in text
            )

        def has_kanji_only(text):
            """Check if text contains only Kanji/Chinese characters (could be Japanese)"""
            return any('\u4e00' <= char <= '\u9fff' for char in text)

        def has_vietnamese_chars(text):
            """Check if text contains Vietnamese diacritics"""
            vietnamese_chars = ['Ä‘', 'Äƒ', 'Ã¢', 'Ãª', 'Ã´', 'Æ¡', 'Æ°', 'Ã¡', 'Ã ', 'áº£', 'Ã£', 'áº¡',
                               'áº¯', 'áº±', 'áº³', 'áºµ', 'áº·', 'áº¥', 'áº§', 'áº©', 'áº«', 'áº­',
                               'Ã©', 'Ã¨', 'áº»', 'áº½', 'áº¹', 'áº¿', 'á»', 'á»ƒ', 'á»…', 'á»‡',
                               'Ã­', 'Ã¬', 'á»‰', 'Ä©', 'á»‹', 'Ã³', 'Ã²', 'á»', 'Ãµ', 'á»',
                               'á»‘', 'á»“', 'á»•', 'á»—', 'á»™', 'á»›', 'á»', 'á»Ÿ', 'á»¡', 'á»£',
                               'Ãº', 'Ã¹', 'á»§', 'Å©', 'á»¥', 'á»©', 'á»«', 'á»­', 'á»¯', 'á»±',
                               'Ã½', 'á»³', 'á»·', 'á»¹', 'á»µ']
            return any(char in text.lower() for char in vietnamese_chars)

        def has_spanish_only_chars(text):
            """Check if text contains Spanish-only characters (Ã±, Ã¡, Ã©, Ã­, Ã³, Ãº, Ã¼, Â¿, Â¡)"""
            # Check for Spanish question/exclamation marks
            if 'Â¿' in text or 'Â¡' in text:
                return True
            # Check for Ã±
            if 'Ã±' in text.lower():
                return True
            return False

        # Validation rules
        if language == 'ko':
            # Korean must have Hangul
            if not has_hangul(keyword):
                return False
            # Korean cannot have Japanese characters
            if has_hiragana_katakana(keyword) or (has_kanji_only(keyword) and not has_hangul(keyword)):
                return False
            # Korean cannot have Vietnamese/Spanish
            if has_vietnamese_chars(keyword) or has_spanish_only_chars(keyword):
                return False
        elif language == 'ja':
            # Japanese must have Hiragana/Katakana or Kanji
            if not (has_hiragana_katakana(keyword) or has_kanji_only(keyword)):
                return False
            # Japanese cannot have Korean
            if has_hangul(keyword):
                return False
            # Japanese cannot have Vietnamese/Spanish
            if has_vietnamese_chars(keyword) or has_spanish_only_chars(keyword):
                return False
        elif language == 'en':
            # English cannot have Korean/Japanese
            if has_hangul(keyword) or has_hiragana_katakana(keyword):
                return False
            # English cannot have Vietnamese (common in trends)
            if has_vietnamese_chars(keyword):
                return False
            # English cannot have Spanish-only markers
            if has_spanish_only_chars(keyword):
                return False

        return True

    def add_to_queue(self, selected: List[Dict]):
        """Add selected keywords to topic queue with language and duplicate validation"""
        if not selected:
            safe_print("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ’¾ íì— {len(selected)}ê°œ í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
        safe_print(f"{'='*60}\n")

        # Get existing keywords for duplicate check (case-insensitive)
        existing_keywords = {t['keyword'].lower() for t in self.queue_data['topics']}

        # Get next ID
        existing_ids = [int(t['id'].split('-')[0]) for t in self.queue_data['topics'] if t['id'].split('-')[0].isdigit()]
        next_id = max(existing_ids) + 1 if existing_ids else 1

        added_count = 0
        rejected_count = 0
        for candidate in selected:
            # Validate keyword-language match
            keyword = candidate.get('keyword', '')
            language = candidate.get('language', 'en')

            # Check for duplicate keyword
            if keyword.lower() in existing_keywords:
                safe_print(f"  ğŸ”´ REJECTED: Duplicate keyword")
                safe_print(f"     Keyword: {keyword}")
                safe_print(f"     Reason: Keyword already exists in queue")
                rejected_count += 1
                continue

            if not self._validate_keyword_language(keyword, language):
                safe_print(f"  ğŸ”´ REJECTED: Keyword-language mismatch")
                safe_print(f"     Keyword: {keyword}")
                safe_print(f"     Language: {language}")
                safe_print(f"     Reason: Keyword contains characters from different language")
                rejected_count += 1
                continue
            # Generate topic ID
            topic_id = f"{next_id:03d}-{candidate['language']}-{candidate['category']}-{candidate['keyword'][:20].replace(' ', '-')}"

            # Create topic entry
            topic = {
                "id": topic_id,
                "keyword": candidate['keyword'],
                "category": candidate['category'],
                "lang": candidate['language'],
                "priority": candidate.get('priority', 7),
                "status": "pending",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "keyword_type": candidate.get('keyword_type', 'evergreen'),
                "search_intent": candidate.get('search_intent', ''),
                "angle": candidate.get('angle', ''),
                "competition_level": candidate.get('competition_level', 'medium'),
                "references": candidate.get('references', [])
            }

            # Add expiry_days for trend keywords
            if topic['keyword_type'] == 'trend':
                topic['expiry_days'] = 3  # 3 days expiry for trending keywords

            self.queue_data['topics'].append(topic)

            type_label = "ğŸ”¥ Trend" if topic['keyword_type'] == 'trend' else "ğŸŒ² Evergreen"
            safe_print(f"  âœ“ Added: {type_label} | {candidate['keyword']}")

            added_count += 1
            next_id += 1

        # Save queue
        self._save_queue()

        safe_print(f"\nâœ… {added_count}ê°œ í‚¤ì›Œë“œê°€ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if rejected_count > 0:
            safe_print(f"ğŸ”´ {rejected_count}ê°œ í‚¤ì›Œë“œê°€ ì–¸ì–´ ë¶ˆì¼ì¹˜ë¡œ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        safe_print(f"ğŸ“Š Total topics in queue: {len(self.queue_data['topics'])}")

        # Show statistics
        self._show_queue_stats()

    def _show_queue_stats(self):
        """Show queue statistics"""
        topics = self.queue_data['topics']

        # Count by status
        by_status = {"pending": 0, "in_progress": 0, "completed": 0}
        for t in topics:
            by_status[t.get('status', 'pending')] += 1

        # Count by type
        by_type = {"trend": 0, "evergreen": 0, "unknown": 0}
        for t in topics:
            ktype = t.get('keyword_type', 'unknown')
            by_type[ktype] = by_type.get(ktype, 0) + 1

        # Count by language
        by_lang = {"en": 0, "ko": 0, "ja": 0}
        for t in topics:
            lang = t.get('lang', 'en')
            by_lang[lang] = by_lang.get(lang, 0) + 1

        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ“Š Queue Statistics")
        safe_print(f"{'='*60}")
        safe_print(f"  Status: Pending={by_status['pending']}, In Progress={by_status['in_progress']}, Completed={by_status['completed']}")
        safe_print(f"  Type: ğŸ”¥ Trend={by_type['trend']}, ğŸŒ² Evergreen={by_type['evergreen']}, Unknown={by_type['unknown']}")
        safe_print(f"  Language: EN={by_lang['en']}, KO={by_lang['ko']}, JA={by_lang['ja']}")
        safe_print(f"{'='*60}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Keyword Curator for blog content")
    parser.add_argument('--count', type=int, default=15, help="Number of candidates to generate (default: 15)")
    parser.add_argument('--auto', action='store_true', help="Automatically add all candidates without interactive selection")
    args = parser.parse_args()

    # Check API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        safe_print("Error: ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)

    # Initialize curator
    curator = KeywordCurator()

    # Generate candidates
    candidates = curator.generate_candidates(count=args.count)

    # Display candidates
    curator.display_candidates(candidates)

    # Selection
    if args.auto:
        # Auto mode: add all candidates
        safe_print("\nğŸ¤– Auto mode: Adding all candidates to queue...\n")
        selected = candidates
    else:
        # Interactive mode: ask user
        selected = curator.interactive_selection(candidates)

    # Add to queue
    if selected:
        curator.add_to_queue(selected)

    safe_print("\nâœ¨ Done!\n")


if __name__ == "__main__":
    main()
