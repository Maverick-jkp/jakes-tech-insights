#!/usr/bin/env python3
"""
Keyword Curator - Semi-automated keyword research for blog content

Generates keyword candidates using Claude API based on KEYWORD_STRATEGY.md
Provides interactive selection interface for human filtering (5 minutes weekly)

Usage:
    python scripts/keyword_curator.py
    python scripts/keyword_curator.py --count 15
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import requests

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))
from utils.security import safe_print, mask_secrets

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)


CURATION_PROMPT_WITH_TRENDS = """ì—­í• :
ë„ˆëŠ” ê´‘ê³  ìˆ˜ìµ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ íë ˆì´í„°ë‹¤.
ì•„ë˜ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ê³ CPC, ê°ì • ë°˜ì‘í˜•** í‚¤ì›Œë“œë¥¼ ì œì•ˆí•˜ë¼.

ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„°:
{trends_data}

**ì¤‘ìš”: ìœ„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ ê·¸ëŒ€ë¡œ keywordë¡œ ì‚¬ìš©í•˜ë¼. ì ˆëŒ€ ì¬í•´ì„í•˜ê±°ë‚˜ ì¬ì‘ì„±í•˜ì§€ ë§ ê²ƒ.**

ëª©í‘œ:
í•œêµ­ì–´ / ì˜ì–´ / ì¼ë³¸ì–´ ê°ê°ì—ì„œ
**ë¶ˆì•ˆ, ë¶„ë…¸, ê¶ê¸ˆì¦**ì„ ìœ ë°œí•˜ëŠ” í‚¤ì›Œë“œë§Œ ì œì•ˆí•˜ë¼.

ê¸ˆì§€:
- ì¶”ìƒì ì¸ íŠ¸ë Œë“œ ìš”ì•½ ("AI íŠ¸ë Œë“œ", "ìƒˆë¡œìš´ ê¸°ìˆ ")
- êµìœ¡/ì •ë³´ì„± í‚¤ì›Œë“œ ("~í•˜ëŠ” ë°©ë²•", "~ë€ ë¬´ì—‡ì¸ê°€")
- ê¸ì •ì ì´ê³  í‰í™”ë¡œìš´ í‚¤ì›Œë“œ
- **Queryë¥¼ ì¬í•´ì„í•˜ê±°ë‚˜ ë‹¤ì‹œ ì“°ëŠ” ê²ƒ**

ì¶œë ¥ í˜•ì‹:
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼.

[
  {{
    "keyword": "ìœ„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì¬í•´ì„ ê¸ˆì§€)",
    "raw_search_title": "ì‚¬ìš©ìê°€ êµ¬ê¸€ì— ê²€ìƒ‰í•  ë•Œ ì •í™•íˆ ì…ë ¥í•˜ëŠ” ê²€ìƒ‰ì–´ (keywordì™€ ë™ì¼í•˜ê²Œ)",
    "editorial_title": "ê¸°ì‚¬ ì œëª© í˜•ì‹ì˜ ë…ì ì¹œí™”ì  ì œëª©",
    "core_fear_question": "ì‚¬ìš©ìì˜ í•µì‹¬ ë‘ë ¤ì›€ì„ ë‹´ì€ ì§ˆë¬¸ í•œ ë¬¸ì¥",
    "language": "ko",
    "category": "tech",  # or: business, lifestyle, society, entertainment, sports, finance, education
    "search_intent": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ ë‹¹ì¥ ê²€ìƒ‰í•˜ëŠ” ì´ìœ  (í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì—‡ì„ ìƒëŠ”ì§€)",
    "angle": "ì´ í‚¤ì›Œë“œë¥¼ ë‹¤ë£° ë•Œì˜ ê´€ì ",
    "competition_level": "low",
    "why_it_works": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ì˜êµ¬ì ìœ¼ë¡œ ë¬´ì—‡ì„ ìƒëŠ”ì§€ (ë§ˆê°/ê¸°íšŒ ì†ì‹¤ ì¤‘ì‹¬)",
    "purpose": "high competitionì¸ ê²½ìš°ì—ë§Œ: Traffic acquisition / Brand positioning / Viral content ì¤‘ í•˜ë‚˜",
    "keyword_type": "trend",
    "priority": 7,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "intent_signal": "STATE_CHANGE"
  }}
]

ì¤‘ìš”:
- keyword_typeì€ ë¬´ì¡°ê±´ "trend"ë§Œ ì‚¬ìš© (evergreen ê¸ˆì§€)
- categoryëŠ” "tech", "business", "lifestyle", "society", "entertainment", "sports", "finance", "education" ì¤‘ í•˜ë‚˜ (8ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ)
- languageëŠ” "en", "ko", "ja" ì¤‘ í•˜ë‚˜ (3ê°œ ì–¸ì–´ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ)
- competition_levelì€ "low", "medium", "high" ì¤‘ í•˜ë‚˜
- priorityëŠ” 1-10 ì‚¬ì´ì˜ ìˆ«ì (ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
- risk_levelì€ "safe", "caution", "high_risk" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "safe")
- name_policyëŠ” "no_real_names", "generic_only" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "no_real_names")
- intent_signalì€ "STATE_CHANGE", "PROMISE_BROKEN", "SILENCE", "DEADLINE_LOST", "COMPARISON" ì¤‘ í•˜ë‚˜
- ì§€ê¸ˆ ì‹œì (2026ë…„ 1ì›”)ì—ì„œ í˜„ì‹¤ì ì¸ í‚¤ì›Œë“œë§Œ ì œì•ˆ
- ì˜ˆì‹œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì‹¤ì œ ê²€ìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ í‚¤ì›Œë“œë§Œ ì œì•ˆ
- **ì¤‘ìš”**: ìœ„ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ keyword í•„ë“œì— ê·¸ëŒ€ë¡œ ë³µì‚¬í•  ê²ƒ
- **keyword í•„ë“œëŠ” ì ˆëŒ€ ì¬ì‘ì„±í•˜ì§€ ë§ê³  Queryë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- **ì¤‘ìš”**: 8ê°œ ì¹´í…Œê³ ë¦¬(tech, business, lifestyle, society, entertainment, sports, finance, education)ë¥¼ ë°˜ë“œì‹œ ê³ ë¥´ê²Œ ë¶„ë°°í•  ê²ƒ

ì–¸ì–´ë³„ í†¤ ì°¨ì´:
- ğŸ‡ºğŸ‡¸ English: rights, compensation, legal leverage, lawsuits ì¤‘ì‹¬
- ğŸ‡°ğŸ‡· Korean: ë¶ˆê³µì •, ì¢Œì ˆ, ì†Œë¹„ì ë³´í˜¸, ì±…ì„ ì¶”ê¶ ì¤‘ì‹¬
- ğŸ‡¯ğŸ‡µ Japanese: ë¶ˆíˆ¬ëª…ì„±, ê³µì‹ ì ˆì°¨, ì ì ˆí•œ ëŒ€ì‘ ë°©ë²• ì¤‘ì‹¬

**ğŸ”´ ì•ˆì „ ê°€ì´ë“œë¼ì¸:**

ì£¼ì˜ì‚¬í•­:
- ëª…ì˜ˆí›¼ì†/ë¹„ë‚œ/ë¹„ë°© í‘œí˜„ ê¸ˆì§€
- ì‚¬ì‹¤ ê¸°ë°˜ì˜ trending í‚¤ì›Œë“œëŠ” ì‹¤ëª… ì‚¬ìš© ê°€ëŠ¥

ê° í‚¤ì›Œë“œì— ë¦¬ìŠ¤í¬ ë ˆë²¨ í‘œì‹œ:
- "risk_level": "safe" (ê¸°ë³¸ê°’)
- "risk_level": "caution" (ë…¼ë€ ê°€ëŠ¥ì„± ìˆìŒ)

ê° í‚¤ì›Œë“œì— ì‹¤ëª… ì •ì±… í‘œì‹œ:
- "name_policy": "no_real_names" (ì‹¤ëª… ë¶ˆí•„ìš”)
- "name_policy": "real_names_ok" (trending ë‰´ìŠ¤ ë“± ì‹¤ëª… í¬í•¨ ê°€ëŠ¥)

**ì¤‘ë³µ ë°©ì§€ ê·œì¹™:**
- Intent signals: STATE_CHANGE, PROMISE_BROKEN, SILENCE, DEADLINE_LOST, COMPARISON
- ê°™ì€ signalì„ ê°€ì§„ í‚¤ì›Œë“œëŠ” ì–¸ì–´ë‹¹ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ
- 5ê°œ signalì„ ì–¸ì–´ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°

**ë°˜ë“œì‹œ ì •í™•íˆ {count}ê°œì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ë¼:**
- ì˜ì–´(en): {per_lang}ê°œ
- í•œêµ­ì–´(ko): {per_lang}ê°œ
- ì¼ë³¸ì–´(ja): {per_lang}ê°œ
- ì´í•©: ì •í™•íˆ {count}ê°œ

ê° ì–¸ì–´ ë‚´ì—ì„œ 5ê°œ ì¹´í…Œê³ ë¦¬(tech, business, lifestyle, society, entertainment)ë¥¼ ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ë˜,
ë°˜ë“œì‹œ ì´ {count}ê°œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ìµœìš°ì„ ì´ë‹¤."""


class KeywordCurator:
    def __init__(self, api_key: str = None, google_api_key: str = None, google_cx: str = None):
        """Initialize keyword curator with Claude API and Google Custom Search"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found")

        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")

        if not self.google_api_key or not self.google_cx:
            safe_print("âš ï¸  Google Custom Search credentials not found")
            safe_print("   Set GOOGLE_API_KEY and GOOGLE_CX environment variables")
            safe_print("   Falling back to Claude-only mode")

        self.client = Anthropic(api_key=self.api_key)
        self.model = "claude-sonnet-4-20250514"

        # Load existing queue
        self.queue_path = Path("data/topics_queue.json")
        self.queue_data = self._load_queue()

    def _load_queue(self) -> Dict:
        """Load existing topic queue"""
        if not self.queue_path.exists():
            return {"topics": []}

        with open(self.queue_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_queue(self):
        """Save updated topic queue"""
        with open(self.queue_path, 'w', encoding='utf-8') as f:
            json.dump(self.queue_data, f, indent=2, ensure_ascii=False)

    def detect_intent_signals(self, query: str) -> list:
        """Detect intent signals from query for deduplication"""
        signals = []

        # State transition patterns
        if any(word in query.lower() for word in ["after", "ê°‘ìê¸°", "suddenly", "çªç„¶", "overnight"]):
            signals.append("STATE_CHANGE")

        # Promise broken patterns
        if any(word in query.lower() for word in ["promised", "supposed to", "ì•½ì†", "ç™ºè¡¨", "denied", "ê±°ë¶€", "æ‹’å¦"]):
            signals.append("PROMISE_BROKEN")

        # Silence patterns
        if any(word in query.lower() for word in ["no response", "ignored", "èª¬æ˜ãªã—", "ë¬´ì‘ë‹µ", "ì¹¨ë¬µ"]):
            signals.append("SILENCE")

        # Deadline/time loss patterns
        if any(word in query.lower() for word in ["deadline", "too late", "ë§ˆê°", "æœŸé™", "ë†“ì¹¨", "é€ƒã—"]):
            signals.append("DEADLINE_LOST")

        # Comparison/injustice patterns
        if any(word in query.lower() for word in ["others got", "only me", "ë‚˜ë§Œ", "è‡ªåˆ†ã ã‘"]):
            signals.append("COMPARISON")

        return signals if signals else ["GENERAL"]

    def fetch_trending_from_rss(self) -> List[str]:
        """Fetch trending topics from Google Trends RSS feeds"""
        import xml.etree.ElementTree as ET

        rss_urls = {
            "KR": "https://trends.google.co.kr/trending/rss?geo=KR",
            "US": "https://trends.google.co.kr/trending/rss?geo=US",
            "JP": "https://trends.google.co.kr/trending/rss?geo=JP"
        }

        trending_queries = []

        for geo, url in rss_urls.items():
            try:
                response = requests.get(url, timeout=10)
                response.raise_for_status()

                # Parse XML
                root = ET.fromstring(response.content)

                # Find all items (trending topics)
                items = root.findall('.//item')

                for item in items[:5]:  # Top 5 per region (15 total)
                    title_elem = item.find('title')
                    if title_elem is not None and title_elem.text:
                        trending_queries.append(title_elem.text.strip())

                safe_print(f"  âœ“ Found {min(len(items), 5)} trends from {geo}")

            except Exception as e:
                safe_print(f"  âš ï¸  RSS fetch error for {geo}: {str(e)}")
                continue

        return trending_queries

    def fetch_trending_topics(self) -> str:
        """Fetch trending topics using Google Trends RSS feeds"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ”¥ Fetching REAL-TIME trending topics from Google Trends RSS...")
        safe_print(f"{'='*60}\n")

        # Try RSS feeds first (most reliable method)
        search_queries = self.fetch_trending_from_rss()

        if search_queries:
            safe_print(f"\n  ğŸ‰ Total {len(search_queries)} real-time trending topics from RSS!\n")
        else:
            safe_print("  âš ï¸  RSS feeds failed. Falling back to pattern-based queries...\n")
            # Fallback to pattern queries
            search_queries = [
                "account banned after update no response",
                "service outage promised compensation denied",
                "ì•± ì—…ë°ì´íŠ¸ í›„ ê°‘ìê¸° ë¨¹í†µ",
                "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåœæ­¢ ç†ç”±èª¬æ˜ãªã—",
                "class action deadline passed too late",
                "refund promised but denied suddenly",
                "ì§‘ë‹¨ì†Œì†¡ ì‹ ì²­ ë§ˆê° ë†“ì¹¨",
                "è¿”é‡‘ç´„æŸã—ãŸãŒ æ‹’å¦ã•ã‚ŒãŸ",
                "government support supposed to but denied",
                "new policy suddenly stricter than announced",
                "ì •ë¶€ì§€ì› ì¡°ê±´ ë°œí‘œì™€ ë‹¤ë¦„",
                "æ”¿åºœæ”¯æ´ çªç„¶ æ¡ä»¶å³ã—ã",
                "celebrity apology issued but backlash continues",
                "idol agency promised explanation ignored fans",
                "ì‚¬ê³¼ë¬¸ ëƒˆì§€ë§Œ ë…¼ë€ ê³„ì†",
                "è¬ç½ªæ–‡å‡ºã—ãŸãŒ ç‚ä¸Šç¶šã",
                "product recall announced but no refund",
                "food contamination others got compensated only me",
                "ë¦¬ì½œ ë°œí‘œí–ˆëŠ”ë° í™˜ë¶ˆ ê±°ë¶€",
                "ãƒªã‚³ãƒ¼ãƒ«ç™ºè¡¨ è¿”é‡‘å¯¾å¿œãªã—"
            ]

        # If no Google Custom Search API, skip search results
        if not self.google_api_key or not self.google_cx:
            safe_print("  âš ï¸  Google Custom Search not configured")
            safe_print("  ğŸ“Œ Will use trending keywords directly\n")
            self.search_results = []
            return "\n\n".join([f"Trending: {q}" for q in search_queries[:30]])

        all_results = []
        for query in search_queries:
            try:
                url = "https://www.googleapis.com/customsearch/v1"
                params = {
                    "key": self.google_api_key,
                    "cx": self.google_cx,
                    "q": query,
                    "num": 2,  # Get top 2 results per query (21 Ã— 2 = 42 queries)
                    "dateRestrict": "d7",  # Last 7 days only (ìµœì‹  ë‰´ìŠ¤)
                    "sort": "date"  # Sort by date (ìµœì‹ ìˆœ)
                }

                # Add delay to avoid rate limiting (max 1 QPS)
                time.sleep(1.0)

                response = requests.get(url, params=params)
                response.raise_for_status()

                data = response.json()

                if "items" in data:
                    # Detect intent signals for this query
                    signals = self.detect_intent_signals(query)

                    for item in data["items"]:
                        all_results.append({
                            "query": query,
                            "signals": signals,  # Add intent signals
                            "title": item.get("title", ""),
                            "snippet": item.get("snippet", ""),
                            "link": item.get("link", ""),
                            "source": item.get("displayLink", "")  # Add source domain
                        })

                safe_print(f"  âœ“ Fetched {len(data.get('items', []))} results for: {query}")

            except requests.exceptions.RequestException as e:
                safe_print(f"  âš ï¸  Error fetching results for '{query}': {str(e)}")
                continue

        safe_print(f"\nâœ… Total {len(all_results)} trending topics fetched\n")

        # Store results for reference extraction
        self.search_results = all_results

        # Format results for Claude
        trends_summary = "\n\n".join([
            f"Query: {r['query']}\nTitle: {r['title']}\nSnippet: {r['snippet']}\n"
            for r in all_results
        ])

        return trends_summary

    def filter_by_risk(self, candidates: List[Dict]) -> List[Dict]:
        """Filter out high-risk keywords automatically"""
        safe_candidates = []
        filtered_count = 0

        for kw in candidates:
            # Auto-reject high-risk
            if kw.get("risk_level") == "high_risk":
                filtered_count += 1
                safe_print(f"  ğŸ”´ Filtered high-risk: {kw.get('keyword', 'unknown')}")
                continue

            # Flag caution items for manual review
            if kw.get("risk_level") == "caution":
                kw["needs_review"] = True
                safe_print(f"  ğŸŸ¡ Caution flagged: {kw.get('keyword', 'unknown')}")

            safe_candidates.append(kw)

        if filtered_count > 0:
            safe_print(f"\nâš ï¸  {filtered_count} high-risk keywords filtered out\n")

        return safe_candidates

    def extract_references(self, all_results: List[Dict], keyword: str, lang: str) -> List[Dict]:
        """Extract top 3 references for a keyword based on search results"""
        # Find relevant results for this keyword
        # Match by language and keyword similarity
        relevant = []

        for result in all_results:
            query = result.get("query", "").lower()
            # Simple matching: if keyword words appear in query
            keyword_words = set(keyword.lower().split())
            query_words = set(query.split())

            # Check language match (simple heuristic)
            is_relevant = len(keyword_words & query_words) > 0

            if is_relevant:
                relevant.append(result)

        # Take top 3 unique sources
        references = []
        seen_domains = set()

        for result in relevant[:10]:  # Check first 10 relevant results
            link = result.get("link", "")
            source = result.get("source", "")
            title = result.get("title", "")

            if link and source and source not in seen_domains:
                references.append({
                    "title": title[:100],  # Truncate long titles
                    "url": link,
                    "source": source
                })
                seen_domains.add(source)

            if len(references) >= 2:  # Get only 2 references per keyword
                break

        return references

    def generate_candidates(self, count: int = 15) -> List[Dict]:
        """Generate keyword candidates using Claude API with trending data"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ” Generating {count} keyword candidates...")
        safe_print(f"{'='*60}\n")

        # Fetch trending topics from Google (store for reference extraction)
        self.search_results = []  # Store search results
        trends_data = self.fetch_trending_topics()

        # Calculate per-language count
        per_lang = count // 3  # Distribute evenly across 3 languages

        # Generate prompt with trending data
        prompt = CURATION_PROMPT_WITH_TRENDS.format(
            trends_data=trends_data,
            count=count,
            per_lang=per_lang
        )

        response = self.client.messages.create(
            model=self.model,
            max_tokens=16000,  # Increased for 30+ keywords
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        # Parse JSON response
        content = response.content[0].text.strip()

        # Extract JSON from markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        try:
            candidates = json.loads(content)
        except json.JSONDecodeError as e:
            safe_print(f"âŒ Failed to parse JSON response: {str(e)}")
            safe_print(f"Raw response:\n{content[:500]}")
            sys.exit(1)

        safe_print(f"âœ… Generated {len(candidates)} candidates\n")

        # Apply risk filtering
        filtered_candidates = self.filter_by_risk(candidates)

        # Extract references for each candidate
        safe_print(f"ğŸ“š Extracting references for {len(filtered_candidates)} candidates...\n")
        for candidate in filtered_candidates:
            keyword = candidate.get("keyword", "")
            lang = candidate.get("language", "en")
            references = self.extract_references(self.search_results, keyword, lang)
            candidate["references"] = references
            if references:
                safe_print(f"  âœ“ {len(references)} refs for: {keyword[:50]}...")

        safe_print("")
        return filtered_candidates

    def display_candidates(self, candidates: List[Dict]):
        """Display candidates with numbered list"""
        safe_print(f"{'='*60}")
        safe_print(f"  ğŸ“‹ Keyword Candidates")
        safe_print(f"{'='*60}\n")

        # Group by language
        by_lang = {"en": [], "ko": [], "ja": []}
        for c in candidates:
            lang = c.get("language", "en")
            by_lang[lang].append(c)

        idx = 1
        lang_names = {"en": "English", "ko": "Korean", "ja": "Japanese"}

        for lang in ["en", "ko", "ja"]:
            if by_lang[lang]:
                safe_print(f"\n[{lang_names[lang]}]")
                safe_print("-" * 60)

                for candidate in by_lang[lang]:
                    type_emoji = "ğŸ”¥" if candidate.get("keyword_type") == "trend" else "ğŸŒ²"
                    comp_emoji = {
                        "low": "ğŸŸ¢",
                        "medium": "ğŸŸ¡",
                        "high": "ğŸ”´"
                    }.get(candidate.get("competition_level", "medium"), "âšª")

                    safe_print(f"\n{idx}. {type_emoji} {candidate['keyword']}")
                    safe_print(f"   Category: {candidate['category']} | Competition: {comp_emoji} {candidate.get('competition_level', 'N/A')}")
                    safe_print(f"   Intent: {candidate['search_intent']}")
                    safe_print(f"   Angle: {candidate['angle']}")
                    safe_print(f"   Why: {candidate.get('why_it_works', 'N/A')[:80]}...")

                    idx += 1

        safe_print(f"\n{'='*60}\n")

    def interactive_selection(self, candidates: List[Dict]) -> List[Dict]:
        """Interactive selection of keywords"""
        safe_print("ì–´ë–¤ í‚¤ì›Œë“œë¥¼ íì— ì¶”ê°€í• ê¹Œìš”?")
        safe_print("ìˆ«ìë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3,5,7,10)")
        safe_print("ë˜ëŠ” 'all'ì„ ì…ë ¥í•˜ë©´ ì „ë¶€ ì¶”ê°€ë©ë‹ˆë‹¤.")
        safe_print("'q'ë¥¼ ì…ë ¥í•˜ë©´ ì·¨ì†Œí•©ë‹ˆë‹¤.\n")

        while True:
            user_input = input("ì„ íƒ: ").strip()

            if user_input.lower() == 'q':
                safe_print("âŒ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return []

            if user_input.lower() == 'all':
                return candidates

            try:
                # Parse selected indices
                selected_indices = [int(x.strip()) for x in user_input.split(',')]

                # Validate indices
                if any(idx < 1 or idx > len(candidates) for idx in selected_indices):
                    safe_print(f"âš ï¸  ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(candidates)} ë²”ìœ„ë¡œ ì…ë ¥í•˜ì„¸ìš”.\n")
                    continue

                # Convert to 0-based index and return selected candidates
                selected = [candidates[idx - 1] for idx in selected_indices]
                return selected

            except ValueError:
                safe_print("âš ï¸  ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. ì˜ˆ: 1,3,5\n")

    def add_to_queue(self, selected: List[Dict]):
        """Add selected keywords to topic queue"""
        if not selected:
            safe_print("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ’¾ íì— {len(selected)}ê°œ í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
        safe_print(f"{'='*60}\n")

        # Get next ID
        existing_ids = [int(t['id'].split('-')[0]) for t in self.queue_data['topics'] if t['id'].split('-')[0].isdigit()]
        next_id = max(existing_ids) + 1 if existing_ids else 1

        added_count = 0
        for candidate in selected:
            # Generate topic ID
            topic_id = f"{next_id:03d}-{candidate['language']}-{candidate['category']}-{candidate['keyword'][:20].replace(' ', '-')}"

            # Create topic entry
            topic = {
                "id": topic_id,
                "keyword": candidate['keyword'],
                "category": candidate['category'],
                "lang": candidate['language'],
                "priority": candidate.get('priority', 7),
                "status": "pending",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "keyword_type": candidate.get('keyword_type', 'evergreen'),
                "search_intent": candidate.get('search_intent', ''),
                "angle": candidate.get('angle', ''),
                "competition_level": candidate.get('competition_level', 'medium')
            }

            # Add expiry_days for trend keywords
            if topic['keyword_type'] == 'trend':
                topic['expiry_days'] = 3  # 3 days expiry for trending keywords

            self.queue_data['topics'].append(topic)

            type_label = "ğŸ”¥ Trend" if topic['keyword_type'] == 'trend' else "ğŸŒ² Evergreen"
            safe_print(f"  âœ“ Added: {type_label} | {candidate['keyword']}")

            added_count += 1
            next_id += 1

        # Save queue
        self._save_queue()

        safe_print(f"\nâœ… {added_count}ê°œ í‚¤ì›Œë“œê°€ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        safe_print(f"ğŸ“Š Total topics in queue: {len(self.queue_data['topics'])}")

        # Show statistics
        self._show_queue_stats()

    def _show_queue_stats(self):
        """Show queue statistics"""
        topics = self.queue_data['topics']

        # Count by status
        by_status = {"pending": 0, "in_progress": 0, "completed": 0}
        for t in topics:
            by_status[t.get('status', 'pending')] += 1

        # Count by type
        by_type = {"trend": 0, "evergreen": 0, "unknown": 0}
        for t in topics:
            ktype = t.get('keyword_type', 'unknown')
            by_type[ktype] = by_type.get(ktype, 0) + 1

        # Count by language
        by_lang = {"en": 0, "ko": 0, "ja": 0}
        for t in topics:
            lang = t.get('lang', 'en')
            by_lang[lang] = by_lang.get(lang, 0) + 1

        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ“Š Queue Statistics")
        safe_print(f"{'='*60}")
        safe_print(f"  Status: Pending={by_status['pending']}, In Progress={by_status['in_progress']}, Completed={by_status['completed']}")
        safe_print(f"  Type: ğŸ”¥ Trend={by_type['trend']}, ğŸŒ² Evergreen={by_type['evergreen']}, Unknown={by_type['unknown']}")
        safe_print(f"  Language: EN={by_lang['en']}, KO={by_lang['ko']}, JA={by_lang['ja']}")
        safe_print(f"{'='*60}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Keyword Curator for blog content")
    parser.add_argument('--count', type=int, default=15, help="Number of candidates to generate (default: 15)")
    parser.add_argument('--auto', action='store_true', help="Automatically add all candidates without interactive selection")
    args = parser.parse_args()

    # Check API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        safe_print("Error: ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)

    # Initialize curator
    curator = KeywordCurator()

    # Generate candidates
    candidates = curator.generate_candidates(count=args.count)

    # Display candidates
    curator.display_candidates(candidates)

    # Selection
    if args.auto:
        # Auto mode: add all candidates
        safe_print("\nğŸ¤– Auto mode: Adding all candidates to queue...\n")
        selected = candidates
    else:
        # Interactive mode: ask user
        selected = curator.interactive_selection(candidates)

    # Add to queue
    if selected:
        curator.add_to_queue(selected)

    safe_print("\nâœ¨ Done!\n")


if __name__ == "__main__":
    main()
