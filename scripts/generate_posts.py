#!/usr/bin/env python3
"""
Content Generation Script

Generates blog posts using Claude API with two-stage process:
1. Draft Agent: Creates initial content
2. Editor Agent: Refines and improves the draft

Usage:
    python generate_posts.py --count 3
    python generate_posts.py --topic-id 001-en-tech-ai-coding
"""

import os
import sys
import json
import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Add scripts to path
sys.path.insert(0, str(Path(__file__).parent))

from topic_queue import reserve_topics, mark_completed, mark_failed
from utils.security import safe_print, mask_secrets
from affiliate_config import (
    detect_product_mentions,
    generate_affiliate_link,
    create_affiliate_box,
    get_affiliate_disclosure,
    should_add_affiliate_links
)

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)

try:
    import requests
except ImportError:
    safe_print("Error: requests package not installed")
    safe_print("Install with: pip install requests")
    sys.exit(1)

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail on Windows")
    safe_print("Install with: pip install certifi")
    certifi = None


# System prompts for different languages
SYSTEM_PROMPTS = {
    "en": """You are a professional writer for Jake's Tech Insights blog.

ğŸ¯ Goal: 800-1,100 words of concise, high-impact content (AdSense optimized)

[EDITORIAL POLICY - READ FIRST]
This is a general hot-topic media covering:
- Government policies, celebrity news, social incidents, controversies, emerging tech
- High CPC and emotional reaction are PRIMARY goals
- Expert depth is NOT required

Your role:
- You do NOT decide whether a topic is useful, important, or appropriate
- Assume all provided topics are already approved for publication
- Your ONLY job is to explain them clearly and calmly in a friendly, UX-writing tone
- Do not soften, reject, or reframe topics based on usefulness or ethics
- Treat each topic as already approved by the editorial team
- Focus on clarity and readability, not on responsibility or balance

Output constraints:
- Do NOT add conclusions, solutions, or advice
- Do NOT attempt to educate or correct the reader
- Do NOT provide long-term implications
- Keep explanations factual, immediate, and present-focused

Every topic must clearly answer:
- Why is this being talked about now?
- If the reason is unclear, infer it from recent events, changes, or reactions
- Do NOT generalize

[Length Guide - Brevity is Key!]
- Total: 800-1,100 words (optimized completion rate)
- Each ## section: 120-180 words (core insights only)
- Intro: 80-100 words (strong hook)
- Conclusion: 60-80 words (clear CTA)
- **Finish completely**: No mid-sentence cutoffs

[Monetization Principles]
1. First paragraph: Hook with reader's pain point (1-2 sentences)
2. Structure: Problem â†’ 3 Core Solutions â†’ Action Steps â†’ Conclusion
3. Tone: Medium/Substack style - conversational, personal, direct
4. SEO: Keyword "{keyword}" naturally 4-6 times
5. Sections: 3-4 ## headings (scannable)
6. End: Clear CTA - question or next step

[Medium Style (Required!)]
- Use "you" and "I" frequently (conversational)
- Short punchy sentences: "Here's the thing.", "Let me explain."
- Natural connectors: "Look", "Here's why", "The truth is"
- Break the fourth wall: "You might be thinking...", "Sound familiar?"
- Strong sentence starters: "Forget X.", "Stop doing Y.", "Start with Z."

[Style - Completion Optimized]
- Active voice, short sentences (1-2 lines)
- Core value only (cut fluff)
- Specific numbers/examples (1-2 selective)
- Bullet points for scannability
- End with punch: "Here's the bottom line."

[Absolutely Avoid]
- Redundancy: repeating same points âŒ
- AI tells: "certainly", "it's important to note", "moreover", "furthermore"
- Academic tone: formal, distant language
- Abstract buzzwords: "revolutionary", "game-changer", "cutting-edge"
- Excessive emojis, unnecessary case studies
- Aggro triggers: "shock", "expose", "truth revealed", "jaw-dropping", "unbelievable"

[Headline Patterns - Aggro-Lite (Use ONLY these patterns)]
A. Change: "Why [topic] has changed recently"
B. Hidden problem: "Why [topic] is raising questions"
C. Cost/Loss: "Why [topic] might be more burdensome than expected"
D. Timing: "Why [topic] is being discussed again lately"

âš ï¸ Core: Complete 800-1,100 word article. Plenty of headroom in 12,000 tokens!""",

    "ko": """ë‹¹ì‹ ì€ Jake's Tech Insights ë¸”ë¡œê·¸ì˜ ì „ë¬¸ ì‘ê°€ì…ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ëª©í‘œ: 800-1,100 ë‹¨ì–´ì˜ ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆëŠ” ê¸€ ì‘ì„± (ì• ë“œì„¼ìŠ¤ ìµœì í™”)

[í¸ì§‘ ë°©ì¹¨ - ë°˜ë“œì‹œ ìˆ™ì§€]
ì´ ì‚¬ì´íŠ¸ëŠ” ì „ë°©ìœ„ í•« í‚¤ì›Œë“œ ë¯¸ë””ì–´ì…ë‹ˆë‹¤:
- ì •ë¶€ ì •ì±…, ì—°ì˜ˆì¸ ë‰´ìŠ¤, ì‚¬íšŒ ì‚¬ê±´, ë…¼ë€, ì‹ ê¸°ìˆ  ë“± ê´‘ë²”ìœ„ ì»¤ë²„
- ê³ CPCì™€ ê°ì • ë°˜ì‘ ìœ ë„ê°€ í•µì‹¬ ëª©í‘œ
- ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê¹Šì´ëŠ” ìš”êµ¬ë˜ì§€ ì•ŠìŒ

ë‹¹ì‹ ì˜ ì—­í• :
- í† í”½ì´ ìœ ìš©í•œì§€, ì¤‘ìš”í•œì§€, ì ì ˆí•œì§€ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  ì œê³µëœ í† í”½ì€ ì´ë¯¸ í¸ì§‘íŒ€ì´ ìŠ¹ì¸í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì„¸ìš”
- ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì˜¤ì§ "ëª…í™•í•˜ê³  ì¹¨ì°©í•˜ê²Œ, ì¹œê·¼í•œ UX ë¼ì´íŒ… í†¤ìœ¼ë¡œ ì„¤ëª…"í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤
- ìœ¤ë¦¬ë‚˜ ìœ ìš©ì„±ì„ ì´ìœ ë¡œ í† í”½ì„ ìˆœí™”í•˜ê±°ë‚˜, ê±°ë¶€í•˜ê±°ë‚˜, ì¬í•´ì„í•˜ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  í† í”½ì€ ì´ë¯¸ ìŠ¹ì¸ëœ ê²ƒìœ¼ë¡œ ì·¨ê¸‰í•˜ì„¸ìš”
- ì±…ì„ê°ì´ë‚˜ ê· í˜•ë³´ë‹¤ëŠ” ëª…í™•ì„±ê³¼ ê°€ë…ì„±ì— ì§‘ì¤‘í•˜ì„¸ìš”

ì¶œë ¥ ì œì•½:
- ê²°ë¡ , í•´ê²°ì±…, ì¡°ì–¸ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ë…ìë¥¼ êµìœ¡í•˜ê±°ë‚˜ ë°”ë¡œì¡ìœ¼ë ¤ í•˜ì§€ ë§ˆì„¸ìš”
- ì¥ê¸°ì  ì˜í–¥ì„ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”
- ì‚¬ì‹¤ì ì´ê³ , ì¦‰ê°ì ì´ë©°, í˜„ì¬ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”

ëª¨ë“  í† í”½ì€ ëª…í™•íˆ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:
- ì™œ ì§€ê¸ˆ ì´ ì´ì•¼ê¸°ê°€ ë‚˜ì˜¤ëŠ”ê°€?
- ì´ìœ ê°€ ë¶ˆë¶„ëª…í•˜ë©´ ìµœê·¼ ì‚¬ê±´, ë³€í™”, ë°˜ì‘ì—ì„œ ì¶”ë¡ í•˜ì„¸ìš”
- ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”

[ê¸¸ì´ ê°€ì´ë“œ - ê°„ê²°í•¨ì´ í•µì‹¬!]
- ì „ì²´ ê¸€: 800-1,100 ë‹¨ì–´ (ì™„ë…ë¥  ìµœì í™”)
- ê° ## ì„¹ì…˜: 120-180 ë‹¨ì–´ (í•µì‹¬ë§Œ ì „ë‹¬)
- ë„ì…ë¶€: 80-100 ë‹¨ì–´ (ê°•ë ¥í•œ í›„í‚¹)
- ê²°ë¡ : 60-80 ë‹¨ì–´ (ëª…í™•í•œ CTA)
- **ë§ˆì§€ë§‰ ë¬¸ì¥ê¹Œì§€ ë°˜ë“œì‹œ ì™„ì„±**: ëŠê¹€ ì—†ì´ ì™„ê²°í•˜ì„¸ìš”

[ìˆ˜ìµí™” ìµœì í™” ì›ì¹™]
1. ì²« ë¬¸ë‹¨: ë…ìì˜ pain point ê³µê° (1-2ë¬¸ì¥ìœ¼ë¡œ ê°•ë ¬í•˜ê²Œ)
2. êµ¬ì¡°: ë¬¸ì œ ì œê¸° â†’ í•µì‹¬ í•´ê²°ì±… 3ê°€ì§€ â†’ ì‹¤ì „ íŒ â†’ ê²°ë¡ 
3. í†¤: í† ìŠ¤(Toss) ìŠ¤íƒ€ì¼ - ì „ë¬¸ì ì´ì§€ë§Œ í¸ì•ˆí•œ ì¹œêµ¬ ê°™ì€ ëŠë‚Œ
4. SEO: í‚¤ì›Œë“œ "{keyword}"ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ 4-6íšŒ í¬í•¨
5. ì„¹ì…˜: 3-4ê°œ ## í—¤ë”© (ê° ì„¹ì…˜ì€ ì½ê¸° ì‰½ê²Œ)
6. ë: ëª…í™•í•œ CTA - ì§ˆë¬¸ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

[í† ìŠ¤ ìŠ¤íƒ€ì¼ ë§íˆ¬ (í•„ìˆ˜!)]
- "~í•´ìš”" ë°˜ë§ ì¡´ëŒ“ë§ ì‚¬ìš© (ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤ âŒ)
- "ì–´ë–¤ê°€ìš”?", "í•œë²ˆ ë³¼ê¹Œìš”?", "ê¶ê¸ˆí•˜ì§€ ì•Šìœ¼ì„¸ìš”?" ê°™ì€ ì¹œê·¼í•œ ì§ˆë¬¸
- "ì‚¬ì‹¤", "ì‹¤ì œë¡œ", "ê·¸ëŸ°ë°", "ì°¸ê³ ë¡œ" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì ‘ì†ì‚¬
- ìˆ«ìë¥¼ ì¹œê·¼í•˜ê²Œ: "10ê°œ â†’ ì—´ ê°œ", "50% â†’ ì ˆë°˜", "3ë°° â†’ ì„¸ ë°°"
- ì§§ê³  ê°•ë ¬í•œ ë¬¸ì¥: "ë†€ëì£ ?", "ë§ì•„ìš”.", "ì´ê²Œ í•µì‹¬ì´ì—ìš”."

[ìŠ¤íƒ€ì¼ - ì™„ë…ë¥  ìµœì í™”]
- ëŠ¥ë™íƒœ ìœ„ì£¼, ì§§ì€ ë¬¸ì¥ (1-2ì¤„)
- í•µì‹¬ë§Œ ì „ë‹¬ (ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°)
- êµ¬ì²´ì  ìˆ«ì/ì˜ˆì‹œ (1-2ê°œë§Œ ì„ íƒì ìœ¼ë¡œ)
- ë¶ˆë¦¿ í¬ì¸íŠ¸ ì ê·¹ í™œìš© (ìŠ¤ìº” ê°€ëŠ¥í•˜ê²Œ)
- ë¬¸ë‹¨ ë ê°•ì¡°: "ì™œ ê·¸ëŸ´ê¹Œìš”?", "ì´ê²Œ í•µì‹¬ì´ì—ìš”."

[ì ˆëŒ€ ê¸ˆì§€]
- ì¤‘ì–¸ë¶€ì–¸: ê°™ì€ ë‚´ìš© ë°˜ë³µ âŒ
- AI í‹°: "ë¬¼ë¡ ", "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤"
- ë”±ë”±í•œ ë¬¸ì²´: "~ìŠµë‹ˆë‹¤/~í•©ë‹ˆë‹¤" (í•´ìš”ì²´ë§Œ!)
- ì¶”ìƒì  í‘œí˜„: "í˜ì‹ ì ", "ê²Œì„ì²´ì¸ì €", "ì£¼ëª©í•  ë§Œí•œ"
- ê³¼ë„í•œ ì´ëª¨ì§€, ë¶ˆí•„ìš”í•œ ì‚¬ë¡€ ë‚˜ì—´
- ì–´ê·¸ë¡œ ë‹¨ì–´: "ì¶©ê²©", "í­ë¡œ", "ì‹¤ì²´", "ì§„ì‹¤", "ì†Œë¦„", "ì¶©ê²©ì ", "ì™„ë²½ ì •ë¦¬", "í•œ ë²ˆì— ì´í•´"

[í—¤ë“œë¼ì¸ íŒ¨í„´ - Aggro-Lite (ì´ íŒ¨í„´ë§Œ ì‚¬ìš©)]
A. ë³€í™”: "ìµœê·¼ ~ì— ë³€í™”ê°€ ìƒê¸´ ì´ìœ "
B. ì€íí˜• ë¬¸ì œ: "~ì„ ë‘ê³  ë§ì´ ë‚˜ì˜¤ëŠ” ì´ìœ "
C. ì†í•´/ë¹„ìš©: "~ì´ ìƒê°ë³´ë‹¤ ë¶€ë‹´ì´ ë˜ëŠ” ì´ìœ "
D. ì‹œì : "ì™œ ìš”ì¦˜ ~ ì´ì•¼ê¸°ê°€ ë‹¤ì‹œ ë‚˜ì˜¤ëŠ” ê±¸ê¹Œ"

âš ï¸ í•µì‹¬: 800-1,100 ë‹¨ì–´ë¡œ ì™„ê²°ëœ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”. 12,000 í† í° ë‚´ì—ì„œ ì—¬ìœ ìˆê²Œ!""",

    "ja": """ã‚ãªãŸã¯Jake's Tech Insightsãƒ–ãƒ­ã‚°ã®ãƒ—ãƒ­ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚

ğŸ¯ æ ¸å¿ƒç›®æ¨™: 3,000-4,500æ–‡å­—ã®ç°¡æ½”ã§ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã®ã‚ã‚‹è¨˜äº‹ï¼ˆAdSenseæœ€é©åŒ–ï¼‰

[ç·¨é›†æ–¹é‡ - å¿…èª­]
ã“ã®ã‚µã‚¤ãƒˆã¯å…¨æ–¹ä½ãƒ›ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã™ï¼š
- æ”¿åºœæ”¿ç­–ã€èŠ¸èƒ½ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ç¤¾ä¼šäº‹ä»¶ã€è«–äº‰ã€æ–°æŠ€è¡“ãªã©å¹…åºƒãã‚«ãƒãƒ¼
- é«˜CPCã¨æ„Ÿæƒ…åå¿œèª˜å°ãŒæœ€å„ªå…ˆç›®æ¨™
- å°‚é–€å®¶ãƒ¬ãƒ™ãƒ«ã®æ·±ã•ã¯ä¸è¦

ã‚ãªãŸã®å½¹å‰²ï¼š
- ãƒˆãƒ”ãƒƒã‚¯ãŒæœ‰ç”¨ã‹ã€é‡è¦ã‹ã€é©åˆ‡ã‹ã‚’åˆ¤æ–­ã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®æä¾›ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã¯æ—¢ã«ç·¨é›†ãƒãƒ¼ãƒ ãŒæ‰¿èªã—ãŸã‚‚ã®ã¨è¦‹ãªã—ã¦ãã ã•ã„
- ã‚ãªãŸã®ä»•äº‹ã¯ã€Œæ˜ç¢ºã§è½ã¡ç€ã„ãŸã€è¦ªã—ã¿ã‚„ã™ã„UXãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒˆãƒ¼ãƒ³ã§èª¬æ˜ã™ã‚‹ã€ã“ã¨ã ã‘ã§ã™
- å€«ç†ã‚„æœ‰ç”¨æ€§ã‚’ç†ç”±ã«ãƒˆãƒ”ãƒƒã‚¯ã‚’å’Œã‚‰ã’ãŸã‚Šã€æ‹’å¦ã—ãŸã‚Šã€å†è§£é‡ˆã—ãŸã‚Šã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®ãƒˆãƒ”ãƒƒã‚¯ã¯æ—¢ã«æ‰¿èªã•ã‚ŒãŸã‚‚ã®ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- è²¬ä»»æ„Ÿã‚„ãƒãƒ©ãƒ³ã‚¹ã‚ˆã‚Šã‚‚ã€æ˜ç¢ºã•ã¨èª­ã¿ã‚„ã™ã•ã«é›†ä¸­ã—ã¦ãã ã•ã„

å‡ºåŠ›åˆ¶ç´„ï¼š
- çµè«–ã€è§£æ±ºç­–ã€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¿½åŠ ã—ãªã„ã§ãã ã•ã„
- èª­è€…ã‚’æ•™è‚²ã—ãŸã‚Šã€è¨‚æ­£ã—ã‚ˆã†ã¨ã—ãªã„ã§ãã ã•ã„
- é•·æœŸçš„ãªå½±éŸ¿ã‚’æç¤ºã—ãªã„ã§ãã ã•ã„
- äº‹å®Ÿçš„ã§ã€å³æ™‚çš„ã§ã€ç¾åœ¨ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸèª¬æ˜ã‚’ã—ã¦ãã ã•ã„

ã™ã¹ã¦ã®ãƒˆãƒ”ãƒƒã‚¯ã¯æ˜ç¢ºã«ç­”ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š
- ãªãœä»Šã“ã®è©±ãŒå‡ºã¦ã„ã‚‹ã®ã‹ï¼Ÿ
- ç†ç”±ãŒä¸æ˜ç¢ºãªå ´åˆã¯ã€æœ€è¿‘ã®å‡ºæ¥äº‹ã€å¤‰åŒ–ã€åå¿œã‹ã‚‰æ¨æ¸¬ã—ã¦ãã ã•ã„
- ä¸€èˆ¬åŒ–ã—ãªã„ã§ãã ã•ã„

[é•·ã•ã‚¬ã‚¤ãƒ‰ - ç°¡æ½”ã•ãŒéµï¼]
- å…¨ä½“: 3,000-4,500æ–‡å­—ï¼ˆå®Œèª­ç‡ã‚’æœ€é©åŒ–ï¼‰
- å„##ã‚»ã‚¯ã‚·ãƒ§ãƒ³: 600-900æ–‡å­—ï¼ˆè¦ç‚¹ã®ã¿ï¼‰
- å°å…¥éƒ¨: 400-500æ–‡å­—ï¼ˆå¼·åŠ›ãªãƒ•ãƒƒã‚¯ï¼‰
- çµè«–: 300-400æ–‡å­—ï¼ˆæ˜ç¢ºãªCTAï¼‰
- **æœ€å¾Œã®æ–‡ã¾ã§å¿…ãšå®Œæˆ**: é€”åˆ‡ã‚Œãªãå®Œçµã•ã›ã¦ãã ã•ã„

[åç›ŠåŒ–æœ€é©åŒ–ã®åŸå‰‡]
1. æœ€åˆã®æ®µè½: èª­è€…ã®æ‚©ã¿ã«å…±æ„Ÿï¼ˆ1-2æ–‡ã§å¼·çƒˆã«ï¼‰
2. æ§‹é€ : å•é¡Œæèµ· â†’ æ ¸å¿ƒè§£æ±ºç­–3ã¤ â†’ å®Ÿè·µãƒ’ãƒ³ãƒˆ â†’ çµè«–
3. ãƒˆãƒ¼ãƒ³: SmartNews/NewsPicks/æ—¥çµŒCOMEMOé¢¨ - æƒ…å ±å¯†åº¦é«˜ãã€èª­ã¿ã‚„ã™ãã€ç›´æ¥çš„
4. SEO: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"{keyword}"ã‚’è‡ªç„¶ã«4-6å›å«ã‚ã‚‹
5. ã‚»ã‚¯ã‚·ãƒ§ãƒ³: 3-4å€‹ã®##è¦‹å‡ºã—ï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯èª­ã¿ã‚„ã™ãï¼‰
6. çµ‚ã‚ã‚Š: æ˜ç¢ºãªCTA - è³ªå•ã¾ãŸã¯æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

[ç¾ä»£çš„UXãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆå¿…é ˆï¼ï¼‰- SmartNews/NewsPicksèª¿]
- çµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ: æœ€åˆã«ç­”ãˆã‚’æç¤ºã—ã€ãã®å¾Œã«è©³ç´°èª¬æ˜
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§ã‚ã‚ŠãªãŒã‚‰ç°¡æ½”ã§åˆ‡ã‚Œå‘³ã®ã‚ã‚‹æ–‡ä½“
- æƒ…å ±å¯†åº¦ã‚’é«˜ã‚ã‚‹: å…·ä½“çš„ãªæ•°å­—ã€ãƒ‡ãƒ¼ã‚¿ã€äº‹ä¾‹ã‚’å„ªå…ˆ
- ä½™è¨ˆãªä¿®é£¾èªã‚’å‰Šé™¤: ã€Œã€œã¨ã„ã†ã€ã€Œã€œã®ã‚ˆã†ãªã€ã‚’æœ€å°é™ã«
- ç®‡æ¡æ›¸ãã¨è¡¨ã‚’ç©æ¥µæ´»ç”¨: ã‚¹ã‚­ãƒ£ãƒ³ã—ã‚„ã™ã„æ§‹æˆ
- æ¥ç¶šè©ã¯æœ€å°é™: "å®Ÿã¯", "ã¡ãªã¿ã«" ãªã©é›°å›²æ°—ä½œã‚Šã®æ¥ç¶šè©ã‚’æ¸›ã‚‰ã™
- **éåº¦ãªè³ªå•å½¢ã‚’é¿ã‘ã‚‹**: "ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ", "æ°—ã«ãªã‚Šã¾ã›ã‚“ã‹ï¼Ÿ" ãªã©ã® rhetorical questions ã¯1è¨˜äº‹ã«1-2å›ã¾ã§
- æ–­å®šçš„ã«ä¼ãˆã‚‹: "ã€œã¨è¨€ãˆã¾ã™", "ã€œã§ã™" ãªã©æ˜ç¢ºãªèªå°¾

[æ®µè½æ§‹æˆ - æƒ…å ±ã‚’å‰ã«]
- å„æ®µè½ã®æœ€åˆã®1-2æ–‡ã§çµè«–ã‚’è¿°ã¹ã‚‹
- ãã®å¾Œã«ç†ç”±ãƒ»æ ¹æ‹ ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®
- ä¸è¦ãªå°å…¥ã‚„å‰ç½®ãã‚’å‰Šé™¤
- ã€Œè¦ã™ã‚‹ã«ã€ã€Œãƒã‚¤ãƒ³ãƒˆã¯ã€ãªã©ã§æ ¸å¿ƒã‚’å¼·èª¿

[ã‚¹ã‚¿ã‚¤ãƒ« - å®Œèª­ç‡æœ€é©åŒ–]
- èƒ½å‹•æ…‹ä¸­å¿ƒã€çŸ­ã„æ–‡ï¼ˆ1-2è¡Œï¼‰
- è¦ç‚¹ã®ã¿ä¼é”ï¼ˆä¸è¦ãªèª¬æ˜å‰Šé™¤ï¼‰
- å…·ä½“çš„ãªæ•°å­—/ä¾‹ï¼ˆ1-2å€‹ã®ã¿é¸æŠçš„ã«ï¼‰
- ç®‡æ¡æ›¸ãç©æ¥µæ´»ç”¨ï¼ˆã‚¹ã‚­ãƒ£ãƒ³å¯èƒ½ã«ï¼‰
- æ®µè½ã®çµ‚ã‚ã‚Šã¯æ–­å®šå½¢: "ã“ã‚ŒãŒç¾çŠ¶ã§ã™ã€‚", "ã“ã®ç‚¹ãŒé‡è¦ã§ã™ã€‚"

[çµ¶å¯¾ç¦æ­¢]
- å†—é•·è¡¨ç¾: åŒã˜å†…å®¹ã®ç¹°ã‚Šè¿”ã— âŒ
- AIçš„è¡¨ç¾: "ã‚‚ã¡ã‚ã‚“", "ã€œã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™"
- ç¡¬ã„æ–‡ä½“: æ•™ç§‘æ›¸ã®ã‚ˆã†ãªèª¬æ˜èª¿
- æŠ½è±¡çš„: "é©æ–°çš„", "ã‚²ãƒ¼ãƒ ãƒã‚§ãƒ³ã‚¸ãƒ£ãƒ¼", "æ³¨ç›®ã™ã¹ã"
- éåº¦ãªçµµæ–‡å­—ã€ä¸è¦ãªäº‹ä¾‹ã®ç¾…åˆ—
- ã‚¢ã‚°ãƒ­å˜èª: "è¡æ’ƒ", "æš´éœ²", "çœŸå®Ÿ", "å®Œå…¨ç†è§£", "é©šæ„•", "ä¿¡ã˜ã‚‰ã‚Œãªã„"
- **éåº¦ãªå€‹äººçš„è³ªå•**: "ã€œã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ", "ã€œã§ã—ã‚‡ã†ã‹ï¼Ÿ" ã®é€£ç™ºï¼ˆ1è¨˜äº‹ã«æœ€å¤§2å›ã¾ã§ï¼‰
- **å…±æ„Ÿã‚’è£…ã£ãŸå‰ç½®ã**: "çš†ã•ã‚“ã‚‚çµŒé¨“ã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒ", "ã‚ˆãã‚ã‚‹æ‚©ã¿ã§ã™ã‚ˆã­" ãªã©

[ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ - Aggro-Lite (ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ä½¿ç”¨)]
A. å¤‰åŒ–: "æœ€è¿‘~ã«å¤‰åŒ–ãŒèµ·ããŸç†ç”±"
B. éš ã•ã‚ŒãŸå•é¡Œ: "~ã‚’ã‚ãã£ã¦è©±ãŒå‡ºã¦ã„ã‚‹ç†ç”±"
C. æå¤±/ã‚³ã‚¹ãƒˆ: "~ãŒæ€ã£ãŸã‚ˆã‚Šè² æ‹…ã«ãªã‚‹ç†ç”±"
D. ã‚¿ã‚¤ãƒŸãƒ³ã‚°: "ãªãœæœ€è¿‘~ã®è©±ãŒå†ã³å‡ºã¦ã„ã‚‹ã®ã‹"

âš ï¸ æ ¸å¿ƒ: 3,000-4,500æ–‡å­—ã§å®Œçµã—ãŸè¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚12,000ãƒˆãƒ¼ã‚¯ãƒ³å†…ã§ä½™è£•ã‚’æŒã£ã¦ï¼"""
}


class ContentGenerator:
    def __init__(self, api_key: Optional[str] = None, unsplash_key: Optional[str] = None):
        """Initialize content generator with Claude API and Unsplash API"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("âŒ ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable or pass to constructor")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError(
                "ANTHROPIC_API_KEY not found. Set it as environment variable or pass to constructor."
            )

        # Initialize with Prompt Caching beta header
        try:
            self.client = Anthropic(
                api_key=self.api_key,
                default_headers={
                    "anthropic-beta": "prompt-caching-2024-07-31"
                }
            )
            self.model = "claude-sonnet-4-20250514"
            safe_print("  âœ“ Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"âŒ ERROR: Failed to initialize Anthropic client: {mask_secrets(str(e))}")
            raise

        # Unsplash API (optional)
        self.unsplash_key = unsplash_key or os.environ.get("UNSPLASH_ACCESS_KEY")
        if self.unsplash_key:
            safe_print("  ğŸ–¼ï¸  Unsplash API enabled")
        else:
            safe_print("  âš ï¸  Unsplash API key not found (images will be skipped)")
            safe_print("     Set UNSPLASH_ACCESS_KEY environment variable to enable")

    def generate_draft(self, topic: Dict) -> str:
        """Generate initial draft using Draft Agent with Prompt Caching"""
        keyword = topic['keyword']
        lang = topic['lang']
        category = topic['category']
        references = topic.get('references', [])  # Get references from topic

        system_prompt = SYSTEM_PROMPTS[lang].format(keyword=keyword)

        # User prompt with references
        user_prompt = self._get_draft_prompt(keyword, category, lang, references)

        safe_print(f"  ğŸ“ Generating draft for: {keyword}")

        # Use Prompt Caching: cache the system prompt
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                system=[
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  âŒ ERROR: API call failed during draft generation")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  âŒ ERROR: Empty response from API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API")

        draft = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  ğŸ’¾ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  ğŸ’¾ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  â„¹ï¸  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  âœ“ Draft generated ({len(draft)} chars)")
        return draft

    def edit_draft(self, draft: str, topic: Dict) -> str:
        """Refine draft using Editor Agent with Prompt Caching"""
        lang = topic['lang']

        safe_print(f"  âœï¸  Editing draft...")

        if not draft or len(draft.strip()) == 0:
            safe_print(f"  âš ï¸  WARNING: Empty draft provided for editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Cannot edit empty draft")

        editor_prompt = self._get_editor_prompt(lang)

        # Use Prompt Caching: cache the editor instructions
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": editor_prompt,
                                "cache_control": {"type": "ephemeral"}
                            },
                            {
                                "type": "text",
                                "text": f"\n\n---\n\n{draft}"
                            }
                        ]
                    }
                ]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  âŒ ERROR: API call failed during draft editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Draft length: {len(draft)} chars")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  âŒ ERROR: Empty response from editing API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API during editing")

        edited = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  ğŸ’¾ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  ğŸ’¾ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  â„¹ï¸  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  âœ“ Draft edited ({len(edited)} chars)")
        return edited

    def _get_draft_prompt(self, keyword: str, category: str, lang: str, references: List[Dict] = None) -> str:
        """Get draft generation prompt based on language"""
        # Get current date in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        today = datetime.now(kst)
        current_date = today.strftime("%Yë…„ %mì›” %dì¼")  # Korean format
        current_date_en = today.strftime("%B %d, %Y")  # English format
        current_year = today.year

        # Format references for prompt
        refs_section = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- [{ref.get('title', 'Source')}]({ref.get('url', '')}) - {ref.get('source', '')}"
                for ref in references[:3]
            ])
            refs_section = f"\n\nğŸ“š USE THESE REFERENCES:\n{refs_list}\n"

        prompts = {
            "en": f"""ğŸ“… TODAY'S DATE: {current_date_en}
âš ï¸ IMPORTANT: You are writing this article as of TODAY ({current_date_en}). All information must be current as of {current_year}. Do NOT use outdated information from 2024 or earlier years.

Write a comprehensive blog post about: {keyword}{refs_section}

Category: {category}

â±ï¸ Reading Time Target: 4-5 minutes
- Write 3-4 main sections (## headings)
- Each section: 1-2 minutes to read, one key point
- Short paragraphs (2-4 sentences each)
- End with a thought-provoking question

ğŸ¯ HOOKING STRATEGY (Critical!):
1. **Opening Hook** (First 2-3 sentences):
   - Start with a PROBLEM SITUATION that readers face
   - Use empathy: "You adopted X, but employees don't use it..."
   - Include specific failure stat: "60% of X projects fail because..."
   - NOT generic intro like "X is becoming popular..."

2. **Real Success/Failure Cases**:
   - Include 1-2 SPECIFIC company/person examples
   - "A shopping mall tried X for everything and failed, but when they focused on Y..."
   - Show what DOESN'T work, not just what works
   - Avoid abstract: "Many companies..." â†’ Use: "One e-commerce startup..."

3. **Limitations & Pitfalls**:
   - Dedicate 1 section to "When X Actually Hurts"
   - "In these 3 situations, X is counterproductive..."
   - This makes content feel authentic and trustworthy

4. **Data-Driven**:
   - Include 2-3 specific statistics (even if approximate)
   - "2024 survey shows 60% failure rate..."
   - "Companies saw 35% productivity increase..."

Content Guidelines:
- Target audience: Decision-makers seeking practical advice
- Focus on "What to avoid" as much as "What to do"
- Concrete examples over abstract concepts
- Mention current trends (2025-2026)
- Be concise and impactful - avoid unnecessary explanations

ğŸ“š REFERENCES SECTION:
- If references were provided above in the prompt, you MUST add a "## References" section at the end
- Use those EXACT URLs - do not modify or create new ones
- Format: `- [Source Title](URL) - Organization/Publisher`
- Example:
  ## References
  - [The State of AI in 2025](https://example.com/ai-report) - McKinsey & Company
  - [Remote Work Statistics 2025](https://example.com/remote) - Buffer
- **IMPORTANT**: If NO references were provided above, DO NOT add a References section at all

**This section is REQUIRED for all posts - even Entertainment/Society topics!**

Write the complete blog post now (body only, no title or metadata):""",

            "ko": f"""ğŸ“… ì˜¤ëŠ˜ ë‚ ì§œ: {current_date}
âš ï¸ ì¤‘ìš”: ì´ ê¸€ì€ ì˜¤ëŠ˜({current_date}) ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. ëª¨ë“  ì •ë³´ëŠ” {current_year}ë…„ í˜„ì¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. 2024ë…„ ì´í•˜ì˜ ì˜¤ë˜ëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

ë‹¤ìŒ ì£¼ì œë¡œ í¬ê´„ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”: {keyword}{refs_section}

ì¹´í…Œê³ ë¦¬: {category}

â±ï¸ ì½ê¸° ì‹œê°„ ëª©í‘œ: 4-5ë¶„
- 3-4ê°œì˜ ì£¼ìš” ì„¹ì…˜ (## í—¤ë”©) ì‘ì„±
- ê° ì„¹ì…˜: 1-2ë¶„ ì½ê¸° ë¶„ëŸ‰, í•˜ë‚˜ì˜ í•µì‹¬ í¬ì¸íŠ¸
- ì§§ì€ ë¬¸ë‹¨ ì‚¬ìš© (2-4 ë¬¸ì¥ì”©)
- ìƒê°ì„ ìê·¹í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬

ğŸ¯ í›„í‚¹ ì „ëµ (í•„ìˆ˜!):
1. **ì˜¤í”„ë‹ í›„í‚¹** (ì²« 2-3ë¬¸ì¥):
   - ë…ìê°€ ì§ë©´í•œ ë¬¸ì œ ìƒí™©ìœ¼ë¡œ ì‹œì‘
   - ê³µê° ìœ ë„: "íšŒì‚¬ì—ì„œ Xë¥¼ ë„ì…í–ˆëŠ”ë° ì§ì›ë“¤ì´ ì“°ì§€ ì•Šê³ ..."
   - êµ¬ì²´ì  ì‹¤íŒ¨ í†µê³„ í¬í•¨: "X í”„ë¡œì íŠ¸ì˜ 60%ê°€ ì‹¤íŒ¨í•˜ëŠ” ì´ìœ ëŠ”..."
   - ì¼ë°˜ì  ì‹œì‘ ê¸ˆì§€: "Xê°€ ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤..." âŒ

2. **ì‹¤ì œ ì„±ê³µ/ì‹¤íŒ¨ ì‚¬ë¡€**:
   - êµ¬ì²´ì ì¸ íšŒì‚¬/ì‚¬ëŒ ì‚¬ë¡€ 1-2ê°œ í¬í•¨
   - "í•œ ì‡¼í•‘ëª°ì€ Xë¥¼ ëª¨ë“  ê²ƒì— ì ìš©í–ˆë‹¤ê°€ ì‹¤íŒ¨í–ˆì§€ë§Œ, Yì—ë§Œ ì§‘ì¤‘í•˜ë‹ˆê¹Œ..."
   - ì•ˆ ë˜ëŠ” ê²ƒë„ ë³´ì—¬ì£¼ê¸° (ì„±ê³µë§Œ ë§í•˜ì§€ ë§ê¸°)
   - ì¶”ìƒì  í‘œí˜„ ê¸ˆì§€: "ë§ì€ ê¸°ì—…ë“¤..." â†’ "í•œ ìŠ¤íƒ€íŠ¸ì—…ì€..." âœ…

3. **í•œê³„ì ê³¼ í•¨ì •**:
   - "Xê°€ ì˜¤íˆë ¤ ì—­íš¨ê³¼ì¸ ê²½ìš°" ì„¹ì…˜ 1ê°œ í• ì• 
   - "ì´ 3ê°€ì§€ ìƒí™©ì—ì„œëŠ” Xê°€ ë¹„íš¨ìœ¨ì ..."
   - ì´ê²ƒì´ ì§„ì •ì„±ê³¼ ì‹ ë¢°ë¥¼ ë§Œë“¦

4. **ë°ì´í„° ê¸°ë°˜**:
   - êµ¬ì²´ì  í†µê³„ 2-3ê°œ í¬í•¨ (ëŒ€ëµì ì´ì–´ë„ OK)
   - "2024ë…„ ì¡°ì‚¬ì— ë”°ë¥´ë©´ 60% ì‹¤íŒ¨ìœ¨..."
   - "ê¸°ì—…ë“¤ì´ 35% ìƒì‚°ì„± ì¦ê°€ ê²½í—˜..."

ì½˜í…ì¸  ê°€ì´ë“œë¼ì¸:
- ëŒ€ìƒ ë…ì: ì‹¤ìš©ì  ì¡°ì–¸ì„ ì°¾ëŠ” ì˜ì‚¬ê²°ì •ì
- "í”¼í•´ì•¼ í•  ê²ƒ"ì„ "í•´ì•¼ í•  ê²ƒ"ë§Œí¼ ê°•ì¡°
- ì¶”ìƒì  ê°œë…ë³´ë‹¤ êµ¬ì²´ì  ì˜ˆì‹œ
- í˜„ì¬ íŠ¸ë Œë“œ ì–¸ê¸‰ (2025-2026ë…„)
- ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆê²Œ - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°

ğŸ“š ì°¸ê³ ìë£Œ ì„¹ì…˜:
- ìœ„ í”„ë¡¬í”„íŠ¸ì— ì°¸ê³ ìë£Œê°€ ì œê³µëœ ê²½ìš°, ë°˜ë“œì‹œ ê¸€ ë§ˆì§€ë§‰ì— "## ì°¸ê³ ìë£Œ" ì„¹ì…˜ ì¶”ê°€
- ì œê³µëœ URLì„ ì •í™•íˆ ì‚¬ìš© - ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆë¡œ ë§Œë“¤ì§€ ë§ ê²ƒ
- í˜•ì‹: `- [ì¶œì²˜ ì œëª©](URL) - ì¡°ì§/ì¶œíŒì‚¬`
- ì˜ˆì‹œ:
  ## ì°¸ê³ ìë£Œ
  - [2025 AI í˜„í™© ë³´ê³ ì„œ](https://example.com/ai-report) - ë§¥í‚¨ì§€ì•¤ì»´í¼ë‹ˆ
  - [ì›ê²© ê·¼ë¬´ í†µê³„ 2025](https://example.com/remote) - Buffer
- **ì¤‘ìš”**: ìœ„ì— ì°¸ê³ ìë£Œê°€ ì œê³µë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì°¸ê³ ìë£Œ ì„¹ì…˜ì„ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

ì§€ê¸ˆ ë°”ë¡œ ì™„ì „í•œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš” (ë³¸ë¬¸ë§Œ, ì œëª©ì´ë‚˜ ë©”íƒ€ë°ì´í„° ì œì™¸):""",

            "ja": f"""ğŸ“… æœ¬æ—¥ã®æ—¥ä»˜: {current_date}
âš ï¸ é‡è¦: ã“ã®è¨˜äº‹ã¯æœ¬æ—¥({current_date})ã®æ™‚ç‚¹ã§æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚ã™ã¹ã¦ã®æƒ…å ±ã¯{current_year}å¹´ç¾åœ¨ã‚’åŸºæº–ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚2024å¹´ä»¥å‰ã®å¤ã„æƒ…å ±ã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

æ¬¡ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦åŒ…æ‹¬çš„ãªãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„: {keyword}{refs_section}

ã‚«ãƒ†ã‚´ãƒª: {category}

â±ï¸ èª­ã‚€æ™‚é–“ã®ç›®æ¨™: 4-5åˆ†
- 3-4å€‹ã®ä¸»è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ (##è¦‹å‡ºã—) ã‚’ä½œæˆ
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³: 1-2åˆ†ã§èª­ã‚ã‚‹åˆ†é‡ã€1ã¤ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ
- çŸ­ã„æ®µè½ã‚’ä½¿ç”¨ (2-4æ–‡ãšã¤)
- è€ƒãˆã•ã›ã‚‹è³ªå•ã§ç· ã‚ããã‚‹

ğŸ¯ ãƒ•ãƒƒã‚­ãƒ³ã‚°æˆ¦ç•¥ (å¿…é ˆ!):
1. **ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ãƒ•ãƒƒã‚¯** (æœ€åˆã®2-3æ–‡):
   - èª­è€…ãŒç›´é¢ã™ã‚‹å•é¡ŒçŠ¶æ³ã‹ã‚‰å§‹ã‚ã‚‹
   - å…±æ„Ÿã‚’èª˜ã†: "ä¼šç¤¾ã§Xã‚’å°å…¥ã—ãŸã®ã«ç¤¾å“¡ãŒä½¿ã‚ãªã„..."
   - å…·ä½“çš„ãªå¤±æ•—çµ±è¨ˆã‚’å«ã‚€: "Xãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®60%ãŒå¤±æ•—ã™ã‚‹ç†ç”±ã¯..."
   - ä¸€èˆ¬çš„ãªå§‹ã¾ã‚Šæ–¹ç¦æ­¢: "XãŒäººæ°—ã«ãªã£ã¦ã„ã¾ã™..." âŒ

2. **å®Ÿéš›ã®æˆåŠŸ/å¤±æ•—äº‹ä¾‹**:
   - å…·ä½“çš„ãªä¼šç¤¾/äººç‰©ã®ä¾‹ã‚’1-2å€‹å«ã‚€
   - "ã‚ã‚‹ECã‚µã‚¤ãƒˆã¯Xã‚’å…¨ã¦ã«é©ç”¨ã—ã¦å¤±æ•—ã—ãŸãŒã€Yã ã‘ã«é›†ä¸­ã—ãŸã‚‰..."
   - ã†ã¾ãã„ã‹ãªã„ã“ã¨ã‚‚è¦‹ã›ã‚‹ (æˆåŠŸã ã‘èªã‚‰ãªã„)
   - æŠ½è±¡çš„è¡¨ç¾ç¦æ­¢: "å¤šãã®ä¼æ¥­ãŒ..." â†’ "ã‚ã‚‹ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã¯..." âœ…

3. **é™ç•Œç‚¹ã¨è½ã¨ã—ç©´**:
   - "XãŒã‹ãˆã£ã¦é€†åŠ¹æœã«ãªã‚‹å ´åˆ" ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’1ã¤è¨­ã‘ã‚‹
   - "ã“ã®3ã¤ã®çŠ¶æ³ã§ã¯Xã¯éåŠ¹ç‡çš„..."
   - ã“ã‚ŒãŒçœŸå®Ÿå‘³ã¨ä¿¡é ¼ã‚’ç”Ÿã‚€

4. **ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³**:
   - å…·ä½“çš„ãªçµ±è¨ˆã‚’2-3å€‹å«ã‚€ (ãŠãŠã‚ˆãã§ã‚‚OK)
   - "2024å¹´ã®èª¿æŸ»ã§ã¯60%ã®å¤±æ•—ç‡..."
   - "ä¼æ¥­ã¯35%ã®ç”Ÿç”£æ€§å‘ä¸Šã‚’çµŒé¨“..."

ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
- å¯¾è±¡èª­è€…: å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ±‚ã‚ã‚‹æ„æ€æ±ºå®šè€…
- "é¿ã‘ã‚‹ã¹ãã“ã¨"ã‚’"ã™ã¹ãã“ã¨"ã¨åŒã˜ãã‚‰ã„å¼·èª¿
- æŠ½è±¡çš„ãªæ¦‚å¿µã‚ˆã‚Šå…·ä½“ä¾‹
- ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã«è¨€åŠ (2025-2026å¹´)
- ç°¡æ½”ã§ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã®ã‚ã‚‹å†…å®¹ - ä¸è¦ãªèª¬æ˜ã‚’å‰Šé™¤

ğŸ“š å‚è€ƒè³‡æ–™ã‚»ã‚¯ã‚·ãƒ§ãƒ³:
- ä¸Šè¨˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å‚è€ƒè³‡æ–™ãŒæä¾›ã•ã‚ŒãŸå ´åˆã€è¨˜äº‹ã®æœ€å¾Œã«å¿…ãš"## å‚è€ƒè³‡æ–™"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
- æä¾›ã•ã‚ŒãŸURLã‚’æ­£ç¢ºã«ä½¿ç”¨ - ä¿®æ­£ã—ãŸã‚Šæ–°è¦ä½œæˆã—ãŸã‚Šã—ãªã„ã“ã¨
- å½¢å¼: `- [æƒ…å ±æºã‚¿ã‚¤ãƒˆãƒ«](URL) - çµ„ç¹”/å‡ºç‰ˆç¤¾`
- ä¾‹ç¤º:
  ## å‚è€ƒè³‡æ–™
  - [2025å¹´AIå‹•å‘ãƒ¬ãƒãƒ¼ãƒˆ](https://example.com/ai-report) - ãƒãƒƒã‚­ãƒ³ã‚¼ãƒ¼ãƒ»ã‚¢ãƒ³ãƒ‰ãƒ»ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼
  - [ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ2025](https://example.com/remote) - Buffer
- **é‡è¦**: ä¸Šè¨˜ã§å‚è€ƒè³‡æ–™ãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã€å‚è€ƒè³‡æ–™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«è¿½åŠ ã—ãªã„ã§ãã ã•ã„

ä»Šã™ãå®Œå…¨ãªãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æ›¸ã„ã¦ãã ã•ã„ï¼ˆæœ¬æ–‡ã®ã¿ã€ã‚¿ã‚¤ãƒˆãƒ«ã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰:"""
        }

        return prompts[lang]

    def _get_editor_prompt(self, lang: str) -> str:
        """Get editor prompt based on language"""
        prompts = {
            "en": """You are an expert editor. Transform this into Medium-style content with authentic human touch:

ğŸ“ Length Requirements (Target: 700-1200 words for 5-7 min read):
- If draft is under 700 words: EXPAND with examples, explanations, context to reach 700-1200 words
- If draft is 700-1200 words: MAINTAIN the same length (ideal range)
- If draft is 1200-1800 words: COMPRESS to 1100-1300 words by removing redundancy
- If draft is over 1800 words: COMPRESS aggressively to 1100-1300 words

ğŸ¯ CRITICAL ENHANCEMENTS:
1. **Strengthen Opening Hook**:
   - If opening is generic, rewrite to start with problem/pain point
   - Add empathy: "You've been there, right?"
   - Make it personal and relatable

2. **Add Authenticity Markers** (NO personal anecdotes):
   - Use authoritative references: "Industry reports show...", "According to recent data..."
   - Add failure acknowledgment: "This approach can fail when..."
   - Show balanced perspective: "This isn't always the answer..."
   - AVOID: "In my experience...", "I spoke with...", "I thought..." (credibility issues on anonymous blogs)

3. **Enhance Examples**:
   - Make vague examples specific: "Many companies" â†’ "One fintech startup" or "A Silicon Valley tech company"
   - Add concrete details: numbers, outcomes, timelines
   - Include what went WRONG, not just success stories
   - AVOID: "I worked with", "I spoke to" â†’ Use: "Case studies show", "Reports indicate"

4. **Balance Perspective**:
   - Ensure there's a "When this doesn't work" section
   - Add nuance: "This works IF...", "But in these cases..."
   - Avoid absolute claims: "always", "never", "guaranteed"

Tasks:
1. **Medium style conversion**: Add "you/I", conversational tone
2. **Eliminate all AI tells**: "certainly", "moreover", "it's important to note"
3. **Natural connectors**: "Look", "Here's why", "The truth is"
4. **Break fourth wall**: "You might be thinking...", "Sound familiar?"
5. **Punchy sentences**: "Here's the thing.", "Let me explain.", "Stop it."
6. **Smooth transitions**: "Now", "Here's where it gets interesting"
7. Keep all factual information intact
8. **Complete ending**: Finish conclusion fully

Return improved version (body only, no title):""",

            "ko": """ë‹¹ì‹ ì€ ì „ë¬¸ ì—ë””í„°ì…ë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ ê¸€ì„ ì§„ì§œ ì‚¬ëŒì´ ì“´ ê²ƒ ê°™ì€ í† ìŠ¤ ìŠ¤íƒ€ì¼ë¡œ ê°œì„ í•˜ì„¸ìš”:

ğŸ“ ê¸¸ì´ ìš”êµ¬ì‚¬í•­ (ëª©í‘œ: 5-7ë¶„ ì½ê¸° = 700-1,200ë‹¨ì–´):
- ì´ˆì•ˆì´ 700ë‹¨ì–´ ë¯¸ë§Œ: ì˜ˆì‹œ, ì„¤ëª…, ë§¥ë½ ì¶”ê°€ë¡œ 700-1,200ë‹¨ì–´ê¹Œì§€ í™•ì¥
- ì´ˆì•ˆì´ 700-1,200ë‹¨ì–´: ê°™ì€ ê¸¸ì´ ìœ ì§€ (ì´ìƒì  ë²”ìœ„)
- ì´ˆì•ˆì´ 1,200-1,800ë‹¨ì–´: 1,100-1,300ë‹¨ì–´ë¡œ ì••ì¶• (ì¤‘ë³µ ì œê±°)
- ì´ˆì•ˆì´ 1,800ë‹¨ì–´ ì´ˆê³¼: 1,100-1,300ë‹¨ì–´ë¡œ ëŒ€í­ ì••ì¶•

ğŸ¯ í•µì‹¬ ê°œì„ ì‚¬í•­:
1. **ì˜¤í”„ë‹ ê°•í™”**:
   - ì¼ë°˜ì  ì‹œì‘ì´ë©´ ë¬¸ì œ/ê³ ë¯¼ ìƒí™©ìœ¼ë¡œ ì¬ì‘ì„±
   - ê³µê° ì¶”ê°€: "ì´ëŸ° ê²½í—˜ ìˆìœ¼ì‹œì£ ?"
   - ê°œì¸ì ì´ê³  ê³µê° ê°€ëŠ¥í•˜ê²Œ

2. **ì •ë³´ ë°€ë„ ìµœìš°ì„ ** (í•œêµ­ ë…ì = ë¹ ë¥¸ ì •ë³´ ì„ í˜¸):
   - í•µì‹¬ ì •ë³´ ë¨¼ì €: ìˆ˜ì¹˜, ë‹¨ê³„, ë°©ë²•
   - ì‹¤ìš© ì •ë³´ ì¦‰ì‹œ ì œê³µ: "ê³„ì‚°ë²•: 1) ~ 2) ~"
   - "ì˜ì™¸ë¡œ...", "ë†€ëê²Œë„..." ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
   - í•œê³„ ì–¸ê¸‰: "í•­ìƒ ë‹µì€ ì•„ë‹ˆì—ìš”..."

3. **ì˜ˆì‹œ êµ¬ì²´í™”** (ê°œì¸ ê²½í—˜ ë°°ì œ):
   - ì¶”ìƒì  ì˜ˆì‹œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ: "ë§ì€ íšŒì‚¬ë“¤" â†’ "í•œ í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…ì€" ë˜ëŠ” "í† ìŠ¤ì˜ ê²½ìš°"
   - êµ¬ì²´ì  ë””í…Œì¼: ìˆ«ì, ê²°ê³¼, íƒ€ì„ë¼ì¸
   - ì‹¤íŒ¨í•œ ê²ƒë„ í¬í•¨: ì„±ê³µë§Œ ë§í•˜ì§€ ë§ê¸°
   - í”¼í•  ê²ƒ: "ì œ ê²½í—˜ìƒ", "ì œê°€ ë´¤ì„ ë•Œ" â†’ ëŒ€ì‹ : "ì‚¬ë¡€ ì—°êµ¬ì— ë”°ë¥´ë©´", "ë°ì´í„°ëŠ” ë³´ì—¬ì¤ë‹ˆë‹¤"

4. **ê· í˜•ì¡íŒ ê´€ì **:
   - "ì´ëŸ° ê²½ìš°ì—” ì•ˆ í†µí•´ìš”" ì„¹ì…˜ í™•ì¸/ì¶”ê°€
   - ë‰˜ì•™ìŠ¤: "ì´ê²Œ í†µí•˜ë ¤ë©´...", "í•˜ì§€ë§Œ ì´ëŸ° ê²½ìš°ì—”..."
   - ì ˆëŒ€ì  í‘œí˜„ í”¼í•˜ê¸°: "í•­ìƒ", "ì ˆëŒ€", "ë¬´ì¡°ê±´"

ì‘ì—…:
1. **í† ìŠ¤ ë§íˆ¬ë¡œ ë³€í™˜**: "~ìŠµë‹ˆë‹¤" â†’ "~í•´ìš”", ì¹œê·¼í•œ ì§ˆë¬¸í˜• ì¶”ê°€
2. AI ëŠë‚Œ ì™„ì „ ì œê±°: "ë¬¼ë¡ ", "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "ì¤‘ìš”í•©ë‹ˆë‹¤" ëª¨ë‘ ì‚­ì œ
3. ìì—°ìŠ¤ëŸ¬ìš´ ì ‘ì†ì‚¬: "ì‚¬ì‹¤", "ì‹¤ì œë¡œ", "ê·¸ëŸ°ë°", "ì°¸ê³ ë¡œ"
4. ìˆ«ìë¥¼ ì¹œê·¼í•˜ê²Œ: "50% â†’ ì ˆë°˜", "3ë°° â†’ ì„¸ ë°°"
5. ì§§ê³  ê°•ë ¬í•œ ë¬¸ì¥ ì¶”ê°€: "ë†€ëì£ ?", "ë§ì•„ìš”.", "ì´ê²Œ í•µì‹¬ì´ì—ìš”."
6. ì„¹ì…˜ ê°„ ë§¤ë„ëŸ¬ìš´ ì „í™˜: "ì, ì´ì œ ~", "ê·¸ëŸ¼ ~"
7. ëª¨ë“  ì‚¬ì‹¤ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
8. **ë§ˆì§€ë§‰ ë¬¸ì¥ê¹Œì§€ ì™„ê²°**: ê²°ë¡ ì„ ë°˜ë“œì‹œ ì™„ì„±

ê°œì„ ëœ ë²„ì „ì„ ë°˜í™˜í•˜ì„¸ìš” (ë³¸ë¬¸ë§Œ, ì œëª© ì œì™¸):""",

            "ja": """ã‚ãªãŸã¯å°‚é–€ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§ã™ã€‚ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æœ¬ç‰©ã®äººé–“ãŒæ›¸ã„ãŸã‚ˆã†ãªè‡ªç„¶ãªä¼šè©±èª¿ã«æ”¹å–„ã—ã¦ãã ã•ã„:

ğŸ“ æ–‡å­—æ•°è¦ä»¶ (ç›®æ¨™: 5-7åˆ† = 2,800-4,200æ–‡å­—):
- ä¸‹æ›¸ããŒ2,800æ–‡å­—æœªæº€: ä¾‹ã€èª¬æ˜ã€æ–‡è„ˆã‚’è¿½åŠ ã—ã¦2,800-4,200æ–‡å­—ã«æ‹¡å¼µ
- ä¸‹æ›¸ããŒ2,800-4,200æ–‡å­—: åŒã˜é•·ã•ã‚’ç¶­æŒ (ç†æƒ³çš„ãªç¯„å›²)
- ä¸‹æ›¸ããŒ4,200-7,000æ–‡å­—: 3,500-4,000æ–‡å­—ã«åœ§ç¸® (å†—é•·æ€§å‰Šé™¤)
- ä¸‹æ›¸ããŒ7,000æ–‡å­—è¶…: 3,500-4,000æ–‡å­—ã«å¤§å¹…åœ§ç¸®

ğŸ¯ é‡è¦ãªæ”¹å–„ç‚¹:
1. **ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ã®å¼·åŒ–**:
   - ä¸€èˆ¬çš„ãªå§‹ã¾ã‚Šãªã‚‰å•é¡Œ/æ‚©ã¿çŠ¶æ³ã«æ›¸ãç›´ã—
   - å…±æ„Ÿã‚’è¿½åŠ : "ã“ã‚“ãªçµŒé¨“ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ"
   - å€‹äººçš„ã§å…±æ„Ÿã§ãã‚‹ã‚ˆã†ã«

2. **çµè«–ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ + ã‚¹ãƒšãƒƒã‚¯å„ªå…ˆ** (æ—¥æœ¬èª­è€…ã®å¥½ã¿):
   - æœ€åˆã«çµè«–: "çµè«–ï¼šã€‡ã€‡ã‚’é¸ã¶ã¹ãç†ç”±"
   - ã‚¹ãƒšãƒƒã‚¯è¡¨å¿…é ˆï¼ˆTech/Financeï¼‰: æ¯”è¼ƒè¡¨ã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿
   - "æ„å¤–ã«ã‚‚...", "é©šã„ãŸã“ã¨ã«..." ã®ã‚ˆã†ãªè‡ªç„¶ãªè¡¨ç¾
   - é™ç•Œã®è¨€åŠ: "ã“ã‚ŒãŒå¸¸ã«ç­”ãˆã¨ã¯é™ã‚Šã¾ã›ã‚“..."

3. **ä¾‹ã®å…·ä½“åŒ–** (å€‹äººçµŒé¨“æ’é™¤):
   - æ›–æ˜§ãªä¾‹ã‚’å…·ä½“çš„ã«: "å¤šãã®ä¼æ¥­" â†’ "ã‚ã‚‹ãƒ•ã‚£ãƒ³ãƒ†ãƒƒã‚¯ä¼æ¥­" ã¾ãŸã¯ "ãƒ¡ãƒ«ã‚«ãƒªã®äº‹ä¾‹"
   - å…·ä½“çš„ãªè©³ç´°: æ•°å­—ã€çµæœã€ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
   - å¤±æ•—ã—ãŸã“ã¨ã‚‚å«ã‚ã‚‹: æˆåŠŸã ã‘èªã‚‰ãªã„
   - é¿ã‘ã‚‹ã¹ã: "ç§ã®çµŒé¨“ã§ã¯", "ç§ãŒè¦‹ãŸã¨ã“ã‚" â†’ ä»£ã‚ã‚Šã«: "ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã«ã‚ˆã‚‹ã¨", "ãƒ‡ãƒ¼ã‚¿ãŒç¤ºã—ã¦ã„ã¾ã™"

4. **ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¦–ç‚¹**:
   - "ã“ã†ã„ã†å ´åˆã¯ã†ã¾ãã„ãã¾ã›ã‚“" ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºèª/è¿½åŠ 
   - ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹: "ã“ã‚ŒãŒã†ã¾ãã„ãã«ã¯...", "ãŸã ã—ã“ã‚“ãªå ´åˆã¯..."
   - çµ¶å¯¾çš„ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹: "å¸¸ã«", "çµ¶å¯¾ã«", "å¿…ãš"

ã‚¿ã‚¹ã‚¯:
1. **ä¼šè©±èª¿ã«å¤‰æ›**: "ã€œã§ã™ã­", "ã€œã¾ã™ã‚ˆã­", "ã€œã§ã—ã‚‡ã†" ãªã©æŸ”ã‚‰ã‹ã„èªå°¾ã«
2. AIçš„ãªè¡¨ç¾ã‚’å®Œå…¨å‰Šé™¤: "ã‚‚ã¡ã‚ã‚“", "ã€œã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™", "ã€œã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™"
3. è‡ªç„¶ãªæ¥ç¶šè©: "å®Ÿã¯", "ã¡ãªã¿ã«", "ã•ã¦", "ãã‚Œã§"
4. ææ¡ˆå½¢ã‚’è¿½åŠ : "ã€œã—ã¦ã¿ã¾ã—ã‚‡ã†", "ã€œã—ã¦ã¿ã¦ãã ã•ã„"
5. è³ªå•å½¢ã§å¼•ãè¾¼ã‚€: "ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ", "æ°—ã«ãªã‚Šã¾ã›ã‚“ã‹ï¼Ÿ"
6. çŸ­ã„æ„Ÿå˜†: "é©šãã§ã™ã­ã€‚", "é¢ç™½ã„ã§ã™ã‚ˆã­ã€‚"
7. ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ç§»è¡Œ: "ã§ã¯ã€è©³ã—ãè¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
8. ã™ã¹ã¦ã®äº‹å®Ÿæƒ…å ±ã¯ãã®ã¾ã¾ä¿æŒ
9. **æœ€å¾Œã®æ–‡ã¾ã§å®Œçµ**: çµè«–ã‚’å¿…ãšå®Œæˆ

æ”¹å–„ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆæœ¬æ–‡ã®ã¿ã€ã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰:"""
        }

        return prompts[lang]

    def generate_title(self, content: str, keyword: str, lang: str, references: List[Dict] = None) -> str:
        """Generate SEO-friendly title based on actual content and references"""
        # Get current year in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        current_year = datetime.now(kst).year

        # Extract strategic samples from content for better context
        # Take beginning (intro), middle (main content), and end (conclusion)
        content_length = len(content)
        if content_length <= 1200:
            content_preview = content
        else:
            # Get first 500, middle 400, last 300 chars
            beginning = content[:500]
            middle_start = content_length // 2 - 200
            middle = content[middle_start:middle_start + 400]
            ending = content[-300:]
            content_preview = f"{beginning}\n\n[...middle section...]\n{middle}\n\n[...conclusion...]\n{ending}"

        # Format references if available
        refs_context = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- {ref.get('title', 'Source')}"
                for ref in references[:3]
            ])
            refs_context = f"\n\nREFERENCE TOPICS:\n{refs_list}\n"

        prompts = {
            "en": f"Generate a catchy, SEO-friendly blog title (50-60 chars) for this post about '{keyword}'.\n\nCONTENT SAMPLES (beginning, middle, end):\n{content_preview}{refs_context}\nIMPORTANT:\n- Title MUST accurately reflect the MAIN TOPIC throughout the entire content\n- Read all content samples (beginning, middle, end) to understand the main theme\n- If beginning discusses subscription but main content is about rankings/fighters, focus on rankings/fighters\n- Include the keyword '{keyword}' naturally\n- Current year is {current_year}, use it if mentioning years\n- Return ONLY the title, nothing else",
            "ko": f"'{keyword}'ì— ëŒ€í•œ ì´ ë¸”ë¡œê·¸ ê¸€ì˜ ë§¤ë ¥ì ì´ê³  SEO ì¹œí™”ì ì¸ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš” (50-60ì).\n\në³¸ë¬¸ ìƒ˜í”Œ (ì‹œì‘, ì¤‘ê°„, ë):\n{content_preview}{refs_context}\nì¤‘ìš”:\n- ì œëª©ì€ ë³¸ë¬¸ ì „ì²´ì˜ í•µì‹¬ ì£¼ì œë¥¼ ì •í™•íˆ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤\n- ëª¨ë“  ë³¸ë¬¸ ìƒ˜í”Œ(ì‹œì‘, ì¤‘ê°„, ë)ì„ ì½ê³  í•µì‹¬ ì£¼ì œë¥¼ íŒŒì•…í•˜ì„¸ìš”\n- ì‹œì‘ë¶€ë¶„ì´ êµ¬ë…ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ì§€ë§Œ ë³¸ë¬¸ ëŒ€ë¶€ë¶„ì´ ë­í‚¹/ì„ ìˆ˜ì— ê´€í•œ ê²ƒì´ë¼ë©´ ë­í‚¹/ì„ ìˆ˜ì— ì§‘ì¤‘í•˜ì„¸ìš”\n- '{keyword}' í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”\n- í˜„ì¬ ì—°ë„ëŠ” {current_year}ë…„ì…ë‹ˆë‹¤\n- ì œëª©ë§Œ ë°˜í™˜í•˜ì„¸ìš”",
            "ja": f"'{keyword}'ã«é–¢ã™ã‚‹ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã®é­…åŠ›çš„ã§SEOãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆ50-60æ–‡å­—ï¼‰ã€‚\n\næœ¬æ–‡ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå†’é ­ã€ä¸­ç›¤ã€çµ‚ç›¤ï¼‰:\n{content_preview}{refs_context}\né‡è¦:\n- ã‚¿ã‚¤ãƒˆãƒ«ã¯æœ¬æ–‡å…¨ä½“ã®æ ¸å¿ƒãƒ†ãƒ¼ãƒã‚’æ­£ç¢ºã«åæ˜ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n- ã™ã¹ã¦ã®æœ¬æ–‡ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå†’é ­ã€ä¸­ç›¤ã€çµ‚ç›¤ï¼‰ã‚’èª­ã‚“ã§æ ¸å¿ƒãƒ†ãƒ¼ãƒã‚’æŠŠæ¡ã—ã¦ãã ã•ã„\n- å†’é ­ãŒã‚µãƒ–ã‚¹ã‚¯ã«ã¤ã„ã¦è©±ã—ã¦ã„ã¦ã‚‚ã€æœ¬æ–‡ã®å¤§éƒ¨åˆ†ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°/é¸æ‰‹ã«é–¢ã™ã‚‹ã‚‚ã®ãªã‚‰ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°/é¸æ‰‹ã«é›†ä¸­ã—ã¦ãã ã•ã„\n- '{keyword}'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªç„¶ã«å«ã‚ã¦ãã ã•ã„\n- ç¾åœ¨ã®å¹´ã¯{current_year}å¹´ã§ã™\n- ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„"
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        generated_title = response.content[0].text.strip().strip('"').strip("'")

        # Validate title-content alignment
        validation_prompts = {
            "en": f"Does this title accurately reflect the main content?\n\nTITLE: {generated_title}\n\nCONTENT: {content_preview}\n\nAnswer ONLY 'yes' or 'no'. If no, briefly explain the mismatch (max 20 words).",
            "ko": f"ì´ ì œëª©ì´ ë³¸ë¬¸ ë‚´ìš©ì„ ì •í™•íˆ ë°˜ì˜í•©ë‹ˆê¹Œ?\n\nì œëª©: {generated_title}\n\në³¸ë¬¸: {content_preview}\n\n'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ì•„ë‹ˆì˜¤ë¼ë©´ ë¶ˆì¼ì¹˜ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš” (ìµœëŒ€ 20ë‹¨ì–´).",
            "ja": f"ã“ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯æœ¬æ–‡å†…å®¹ã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\n\nã‚¿ã‚¤ãƒˆãƒ«: {generated_title}\n\næœ¬æ–‡: {content_preview}\n\nã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚ã„ã„ãˆã®å ´åˆã€ä¸ä¸€è‡´ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼ˆæœ€å¤§20èªï¼‰ã€‚"
        }

        validation_response = self.client.messages.create(
            model=self.model,
            max_tokens=50,
            messages=[{
                "role": "user",
                "content": validation_prompts[lang]
            }]
        )

        validation_result = validation_response.content[0].text.strip().lower()

        # If validation fails, log warning (but still use the title)
        if not validation_result.startswith('yes') and not validation_result.startswith('ì˜ˆ') and not validation_result.startswith('ã¯ã„'):
            safe_print(f"  âš ï¸  Title-content mismatch detected: {validation_result}")
            safe_print(f"     Title: {generated_title}")

        return generated_title

    def generate_description(self, content: str, keyword: str, lang: str) -> str:
        """Generate meta description"""
        prompts = {
            "en": f"Generate a compelling meta description (150-160 chars) for a blog post about '{keyword}'. Return ONLY the description.",
            "ko": f"'{keyword}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ì˜ ë§¤ë ¥ì ì¸ ë©”íƒ€ ì„¤ëª…ì„ ìƒì„±í•˜ì„¸ìš” (150-160ì). ì„¤ëª…ë§Œ ë°˜í™˜í•˜ì„¸ìš”.",
            "ja": f"'{keyword}'ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹ã®é­…åŠ›çš„ãªãƒ¡ã‚¿èª¬æ˜ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆ150-160æ–‡å­—ï¼‰ã€‚èª¬æ˜ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        return response.content[0].text.strip().strip('"').strip("'")

    def translate_to_english(self, text: str) -> str:
        """Translate non-English keywords to English for Unsplash search"""
        # Simple keyword translations for common tech/business/lifestyle terms
        translations = {
            # Korean
            'ì±—ë´‡': 'chatbot', 'AI': 'artificial intelligence', 'ë„ì…': 'implementation',
            'ì‹¤íŒ¨': 'failure', 'ì´ìœ ': 'reasons', 'ë…¸ì½”ë“œ': 'no-code', 'íˆ´': 'tool',
            'í•œê³„ì ': 'limitations', 'ì¬íƒê·¼ë¬´': 'remote work', 'í•˜ì´ë¸Œë¦¬ë“œ': 'hybrid',
            'ê·¼ë¬´': 'work', 'íš¨ìœ¨ì„±': 'efficiency', 'MZì„¸ëŒ€': 'gen z', 'ê´€ë¦¬': 'management',
            'ë°©ë²•': 'method', 'ì‚¬ë¡€': 'case', 'ë¯¸ë‹ˆë©€': 'minimal', 'ë¼ì´í”„': 'lifestyle',
            'ì¤‘ë‹¨': 'quit', 'ìƒì‚°ì„±': 'productivity', 'íŒ': 'tips',
            # Japanese
            'ã‚³ãƒ¼ãƒ‰': 'code', 'é–‹ç™º': 'development', 'é™ç•Œç‚¹': 'limitations',
            'ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯': 'telework', 'ã‚ªãƒ•ã‚£ã‚¹': 'office', 'å‹¤å‹™': 'work',
            'ç”Ÿç”£æ€§': 'productivity', 'æ¯”è¼ƒ': 'comparison', 'ãƒãƒ¼': 'no',
            'ã‚µãƒ–ã‚¹ã‚¯': 'subscription', 'ç–²ã‚Œ': 'fatigue', 'è§£ç´„': 'cancel',
            'ç†ç”±': 'reason', 'Zä¸–ä»£': 'gen z', 'ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': 'management',
            'èª¤è§£': 'misconception', 'DX': 'digital transformation', 'æ¨é€²': 'promotion',
            'å¤±æ•—': 'failure', 'è¦å› ': 'factors', 'ãƒ’ãƒ³ãƒˆ': 'tips',
            'ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹': 'work life balance', 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—': 'startup',
            'è³‡é‡‘èª¿é”': 'fundraising', 'æˆ¦ç•¥': 'strategy', 'AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°': 'AI coding',
            'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ': 'assistant', 'ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯': 'remote work'
        }

        # Split and translate each word
        words = text.split()
        translated_words = []
        for word in words:
            # Try exact match first
            if word in translations:
                translated_words.append(translations[word])
            else:
                # Check if word contains any translatable substring
                found = False
                for kr, en in translations.items():
                    if kr in word:
                        translated_words.append(en)
                        found = True
                        break
                if not found:
                    # Keep as-is if ASCII (likely already English)
                    try:
                        word.encode('ascii')
                        translated_words.append(word)
                    except UnicodeEncodeError:
                        pass  # Skip non-ASCII untranslatable words

        return ' '.join(translated_words) if translated_words else 'technology'

    def fetch_featured_image(self, keyword: str, category: str) -> Optional[Dict]:
        """Fetch featured image from Unsplash API"""
        if not self.unsplash_key:
            return None

        try:
            # Clean keyword for better Unsplash search
            # Remove years (2020-2030) to avoid year-specific images
            import re
            clean_keyword = re.sub(r'20[2-3][0-9]ë…„?', '', keyword)  # Match years + optional ë…„ (Korean year)
            # Remove common prefixes/suffixes that reduce search quality
            clean_keyword = re.sub(r'ã€.*?ã€‘', '', clean_keyword)  # Remove ã€bracketsã€‘
            clean_keyword = re.sub(r'\[.*?\]', '', clean_keyword)  # Remove [brackets]
            clean_keyword = clean_keyword.strip()

            # Translation dictionary for meaningful keywords
            keyword_translations = {
                # Korean - AI/Jobs/Employment
                'AI': 'artificial intelligence',
                'ì¸ê³µì§€ëŠ¥': 'artificial intelligence',
                'ëŒ€ì²´': 'replacement automation',
                'ì¼ìë¦¬': 'job employment work',
                'ì‹¤ì—…': 'unemployment jobless',
                'ì§ì—…': 'occupation career profession',
                'ì·¨ì—…': 'employment hiring recruitment',
                'ìë™í™”': 'automation robot',
                'ê¸°ìˆ ': 'technology tech',
                'ë””ì§€í„¸': 'digital technology',
                'ë¡œë´‡': 'robot automation',
                'ë¯¸ë˜': 'future',
                'ë³€í™”': 'change transformation',
                'ìœ„í—˜': 'risk danger',
                # Korean - Finance/Business
                'ë‚˜ë¼ì‚¬ë‘ì¹´ë“œ': 'patriot card credit card',
                'ì¹´ë“œ': 'card credit',
                'ì—°ë ¹': 'age limit',
                'ì œí•œ': 'restriction limit',
                'ì „ì„¸': 'housing lease deposit',
                'ë³´ì¦ê¸ˆ': 'deposit guarantee',
                'ë°°ë‹¬': 'delivery food',
                'ìˆ˜ìˆ˜ë£Œ': 'fee commission',
                'ìì˜ì—…': 'small business owner',
                'íì—…': 'business closure bankruptcy',
                'ì§€ì›ê¸ˆ': 'subsidy support fund',
                'ì •ë¶€': 'government policy',
                'ì‹ ì²­': 'application registration',
                'í˜œíƒ': 'benefit advantage',
                # Korean - Entertainment/Society
                'ì‚¬ê³¼ë¬¸': 'apology statement',
                'íŒ¬': 'fan supporter',
                'ë“±ëŒë¦¼': 'backlash criticism',
                'ìŠ¤ë§ˆíŠ¸í°': 'smartphone mobile',
                'ê±´ê°•': 'health wellness',
                # Japanese - AI/Jobs/Employment
                'äººå·¥çŸ¥èƒ½': 'artificial intelligence',
                'å¤±æ¥­': 'unemployment jobless',
                'ãƒªã‚¹ã‚¯': 'risk danger threat',
                'è·æ¥­': 'occupation job',
                'ä»£æ›¿': 'replacement substitute',
                'é›‡ç”¨': 'employment hiring',
                'è‡ªå‹•åŒ–': 'automation robot',
                'ãƒ‡ã‚¸ã‚¿ãƒ«': 'digital technology',
                'ãƒ­ãƒœãƒƒãƒˆ': 'robot automation',
                'æœªæ¥': 'future',
                'å¤‰åŒ–': 'change transformation',
                # Japanese - Finance/Business
                'å¥¨å­¦é‡‘': 'scholarship student loan',
                'è¿”æ¸ˆ': 'repayment debt',
                'å…é™¤': 'exemption forgiveness',
                'æŠ•è³‡': 'investment financial',
                'è©æ¬º': 'fraud scam',
                'ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼è³': 'academy award',
                'å—è³': 'award winner',
                'ä½å®…ãƒ­ãƒ¼ãƒ³': 'home mortgage loan',
                'å¯©æŸ»': 'screening examination',
                'æ‰¿èª': 'approval authorization',
            }

            # Extract meaningful keywords from title
            title_words = clean_keyword.split()
            translated_keywords = []

            # Try to find and translate key phrases
            for ko_word, en_translation in keyword_translations.items():
                if ko_word in clean_keyword:
                    translated_keywords.append(en_translation)

            # If no translation found, extract meaningful words (skip common noise and non-ASCII)
            if not translated_keywords:
                noise_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']
                for word in title_words[:3]:  # Take first 3 words
                    # Filter out non-ASCII words to prevent non-English queries
                    try:
                        word.encode('ascii')
                        is_ascii = True
                    except UnicodeEncodeError:
                        is_ascii = False

                    if is_ascii and len(word) > 2 and word.lower() not in noise_words:
                        translated_keywords.append(word)

            # Add category context
            category_context = {
                'tech': 'technology digital',
                'business': 'business professional',
                'finance': 'finance money',
                'society': 'society community',
                'entertainment': 'entertainment culture',
                'lifestyle': 'lifestyle daily',
                'sports': 'sports athletic',
                'education': 'education learning'
            }

            # Build flexible, contextual query
            if translated_keywords:
                base_keywords = ' '.join(translated_keywords[:2])
            else:
                # Fallback to pure category context if no English keywords found
                base_keywords = category_context.get(category, 'technology')

            context = category_context.get(category, category)
            query = f"{base_keywords} {context}".strip()

            # Unsplash API endpoint
            url = "https://api.unsplash.com/search/photos"
            headers = {
                "Authorization": f"Client-ID {self.unsplash_key}"
            }
            params = {
                "query": query,
                "per_page": 30,  # Increased from 5 to 30 for larger image pool
                "orientation": "landscape"
            }

            safe_print(f"  ğŸ” Searching Unsplash for: {query}")

            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            data = response.json()

            # Load used images tracking file
            used_images_file = Path(__file__).parent.parent / "data" / "used_images.json"
            used_images_meta_file = Path(__file__).parent.parent / "data" / "used_images_metadata.json"

            # Load metadata (tracks when each image was used)
            used_images_meta = {}
            if used_images_meta_file.exists():
                try:
                    with open(used_images_meta_file, 'r') as f:
                        used_images_meta = json.load(f)
                except:
                    pass

            # Clean up images older than 30 days
            from datetime import datetime, timedelta
            current_time = datetime.now().timestamp()
            cutoff_time = (datetime.now() - timedelta(days=30)).timestamp()

            cleaned_meta = {}
            for img_id, timestamp in used_images_meta.items():
                if timestamp > cutoff_time:
                    cleaned_meta[img_id] = timestamp

            # Update set of used images (only keep recent ones)
            used_images = set(cleaned_meta.keys())

            # Save cleaned metadata
            if cleaned_meta != used_images_meta:
                used_images_meta_file.parent.mkdir(parents=True, exist_ok=True)
                with open(used_images_meta_file, 'w') as f:
                    json.dump(cleaned_meta, f, indent=2)
                if len(used_images_meta) > len(cleaned_meta):
                    safe_print(f"  ğŸ—‘ï¸  Cleaned up {len(used_images_meta) - len(cleaned_meta)} images older than 30 days")

            used_images_meta = cleaned_meta

            # Find first unused image from results
            photo = None
            if data.get('results'):
                for result in data['results']:
                    image_id = result['id']
                    if image_id not in used_images:
                        photo = result
                        used_images.add(image_id)
                        used_images_meta[image_id] = current_time
                        break
            else:
                safe_print(f"  âš ï¸  No images found for '{query}'")

            # If no results or all images are used, try with generic category query
            if photo is None:
                safe_print(f"  âš ï¸  All images for '{query}' already used, trying generic category search...")
                generic_query = category_context.get(category, 'technology')
                params['query'] = generic_query

                response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
                response.raise_for_status()
                data = response.json()

                if data.get('results'):
                    for result in data['results']:
                        image_id = result['id']
                        if image_id not in used_images:
                            photo = result
                            used_images.add(image_id)
                            used_images_meta[image_id] = current_time
                            safe_print(f"  âœ“ Found unused image with generic search: {generic_query}")
                            break

                # If still no unused image found, return None (use placeholder)
                if photo is None:
                    safe_print(f"  âŒ No unused images available for category '{category}'")
                    return None

            # Save used images (legacy file for backward compatibility)
            used_images_file.parent.mkdir(parents=True, exist_ok=True)
            with open(used_images_file, 'w') as f:
                json.dump(list(used_images), f)

            # Save metadata with timestamps
            with open(used_images_meta_file, 'w') as f:
                json.dump(used_images_meta, f, indent=2)

            image_info = {
                'url': photo['urls']['regular'],
                'download_url': photo['links']['download_location'],
                'photographer': photo['user']['name'],
                'photographer_url': photo['user']['links']['html'],
                'unsplash_url': photo['links']['html'],
                'image_id': photo['id']
            }

            safe_print(f"  âœ“ Found image by {image_info['photographer']}")
            return image_info

        except requests.exceptions.Timeout as e:
            safe_print(f"  âš ï¸  Unsplash API timeout: Request took too long")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  âš ï¸  Unsplash API HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.RequestException as e:
            safe_print(f"  âš ï¸  Unsplash API network error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except json.JSONDecodeError as e:
            safe_print(f"  âš ï¸  Unsplash API response parsing failed")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: Invalid JSON response")
            return None
        except Exception as e:
            safe_print(f"  âš ï¸  Image fetch failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def download_image(self, image_info: Dict, keyword: str) -> Optional[str]:
        """Download optimized image to static/images/ directory"""
        if not image_info:
            return None

        try:
            # Create images directory
            images_dir = Path("static/images")
            images_dir.mkdir(parents=True, exist_ok=True)

            # Generate filename
            slug = keyword.lower()
            slug = ''.join(c if c.isalnum() or c.isspace() else '' for c in slug)
            slug = slug.replace(' ', '-')[:30]
            # Use KST for image filename
            from datetime import timezone, timedelta
            kst = timezone(timedelta(hours=9))
            date_str = datetime.now(kst).strftime("%Y%m%d")
            filename = f"{date_str}-{slug}.jpg"
            filepath = images_dir / filename

            # Trigger Unsplash download tracking (required by API terms)
            if image_info.get('download_url'):
                verify_ssl = certifi.where() if certifi else True
                requests.get(
                    image_info['download_url'],
                    headers={"Authorization": f"Client-ID {self.unsplash_key}"},
                    timeout=5,
                    verify=verify_ssl
                )

            # Download optimized image (1200px width, quality 85)
            # Use Unsplash's regular URL which already includes optimization
            download_url = image_info.get('url', '')
            # Add additional optimization parameters
            if '?' in download_url:
                optimized_url = f"{download_url}&w=1200&q=85&fm=jpg"
            else:
                optimized_url = f"{download_url}?w=1200&q=85&fm=jpg"

            safe_print(f"  ğŸ“¥ Downloading optimized image (1200px, q85)...")
            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(optimized_url, timeout=15, verify=verify_ssl)
            response.raise_for_status()

            # Save image
            with open(filepath, 'wb') as f:
                f.write(response.content)

            size_kb = len(response.content) / 1024
            safe_print(f"  âœ“ Image saved: {filepath} ({size_kb:.1f} KB)")

            # Return relative path for Hugo
            return f"/images/{filename}"

        except requests.exceptions.Timeout as e:
            safe_print(f"  âš ï¸  Image download timeout")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     URL: {optimized_url[:80]}...")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  âš ï¸  Image download HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            return None
        except IOError as e:
            safe_print(f"  âš ï¸  File system error during image save")
            safe_print(f"     Path: {filepath}")
            safe_print(f"     Error: {str(e)}")
            return None
        except Exception as e:
            safe_print(f"  âš ï¸  Image download failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def save_post(self, topic: Dict, title: str, description: str, content: str, image_path: Optional[str] = None, image_credit: Optional[Dict] = None) -> Path:
        """Save post to Hugo content directory"""
        lang = topic['lang']
        category = topic['category']
        keyword = topic['keyword']

        # Generate filename from keyword
        slug = keyword.lower()
        # Remove special characters, keep alphanumeric and spaces
        slug = ''.join(c if c.isalnum() or c.isspace() else '' for c in slug)
        slug = slug.replace(' ', '-')[:50]

        # Create directory
        content_dir = Path(f"content/{lang}/{category}")
        content_dir.mkdir(parents=True, exist_ok=True)

        # Generate filename with date in KST
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        date_str = datetime.now(kst).strftime("%Y-%m-%d")
        filename = f"{date_str}-{slug}.md"
        filepath = content_dir / filename

        # Hugo frontmatter with required image field
        # Use placeholder if no Unsplash image available
        if not image_path:
            # Use category-based placeholder
            image_path = f"/images/placeholder-{category}.jpg"

        # Use KST timezone for date
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        now_kst = datetime.now(kst)

        frontmatter = f"""---
title: "{title}"
date: {now_kst.strftime("%Y-%m-%dT%H:%M:%S%z")}
draft: false
categories: ["{category}"]
tags: {json.dumps(keyword.split()[:3])}
description: "{description}"
image: "{image_path}"
---

"""

        # Add hero image at the top of content if available
        hero_image = ""
        if image_path and image_credit:
            hero_image = f"![{keyword}]({image_path})\n\n"

        # Add image credit at the end of content if available
        credit_line = ""
        if image_credit:
            credit_line = f"\n\n---\n\n*Photo by [{image_credit['photographer']}]({image_credit['photographer_url']}) on [Unsplash]({image_credit['unsplash_url']})*\n"

        # Validate References section and remove if it contains fake URLs
        def has_fake_reference_url(url: str) -> bool:
            """Check if URL is a fake reference"""
            fake_patterns = [
                r'example\.com',
                r'example\.org',
                r'\.gov/[a-z-]+-202[0-9]',
                r'\.org/[a-z-]+-survey',
                r'\.gov/[a-z-]+-compliance',
                r'\.gov/[a-z-]+-report',
            ]
            for pattern in fake_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return True
            return False

        # Check if References section exists - if not, just skip it (don't add fake references)
        ref_headers = {
            'en': '## References',
            'ko': '## ì°¸ê³ ìë£Œ',
            'ja': '## å‚è€ƒæ–‡çŒ®'
        }
        ref_header = ref_headers.get(lang, '## References')

        # First, normalize any non-standard reference formats to standard format
        # Remove bold "**References:**" format if exists (common Claude output)
        bold_ref_patterns = [
            (r'\*\*References?:\*\*\n', ''),  # **References:**
            (r'\*\*å‚è€ƒ(?:æ–‡çŒ®|è³‡æ–™):\*\*\n', ''),  # **å‚è€ƒæ–‡çŒ®:** or **å‚è€ƒè³‡æ–™:**
            (r'\*\*ì°¸ê³ ìë£Œ:\*\*\n', ''),  # **ì°¸ê³ ìë£Œ:**
        ]
        for pattern, replacement in bold_ref_patterns:
            content = re.sub(pattern, replacement, content)

        # Extract References section if exists
        has_references = ref_header in content or '## Reference' in content or '## ì°¸ê³ ' in content or '## å‚è€ƒ' in content

        if has_references:
            # Extract URLs from References section using regex
            # Pattern: [text](url) or bare URLs
            url_pattern = r'https?://[^\s\)\]<>"]+'  
            urls_in_content = re.findall(url_pattern, content)

            # Check if any URLs are fake
            fake_urls = [url for url in urls_in_content if has_fake_reference_url(url)]

            if fake_urls:
                safe_print(f"  âš ï¸  Fake reference URLs detected: {len(fake_urls)} found")
                safe_print(f"      Examples: {fake_urls[:3]}")

                # Remove References section entirely
                # Match from any References header to the next ## header or end of content
                ref_pattern = r'\n## (?:References?|å‚è€ƒ(?:æ–‡çŒ®|è³‡æ–™)|ì°¸ê³ ìë£Œ)\n.*?(?=\n## |\Z)'
                content = re.sub(ref_pattern, '', content, flags=re.DOTALL)
                safe_print(f"  ğŸ—‘ï¸  Removed References section with fake URLs")
                has_references = False  # Mark as no valid references
            else:
                safe_print(f"  âœ… References section validated ({len(urls_in_content)} URLs)")

        # If no valid References section exists, add from queue
        if not has_references and topic.get('references'):
            references = topic['references']
            safe_print(f"  â„¹ï¸  No References section in content, adding from queue ({len(references)} refs)")

            # Build References section
            ref_section = f"\n\n{ref_header}\n\n"
            for i, ref in enumerate(references, 1):
                ref_section += f"{i}. [{ref['title']}]({ref['url']})\n"

            # Append to content
            content = content.rstrip() + ref_section
            safe_print(f"  âœ… Added {len(references)} references from queue")
        elif not has_references:
            safe_print(f"  â„¹ï¸  No references available (neither in content nor queue)")

        # Add affiliate links if applicable
        affiliate_programs_used = []
        if should_add_affiliate_links(category):
            safe_print(f"  ğŸ”— Checking for product mentions to add affiliate links...")

            # Detect products mentioned in content
            detected_products = detect_product_mentions(content, lang, category)

            if detected_products:
                safe_print(f"  ğŸ“¦ Detected {len(detected_products)} products: {', '.join(detected_products[:3])}")

                # Add affiliate link for the first detected product only (to avoid being too commercial)
                primary_product = detected_products[0]
                link_data = generate_affiliate_link(primary_product, lang)

                if link_data:
                    # Find insertion point: after first ## section
                    sections = content.split('\n## ')
                    if len(sections) > 1:
                        # Insert after first section
                        affiliate_box = create_affiliate_box(primary_product, lang, link_data)
                        sections[1] = sections[1] + '\n' + affiliate_box
                        content = '\n## '.join(sections)

                        affiliate_programs_used.append(link_data['program'])
                        safe_print(f"  âœ… Added affiliate link for '{primary_product}' ({link_data['program']})")
                    else:
                        safe_print(f"  âš ï¸  Could not find insertion point for affiliate link")
                else:
                    safe_print(f"  â„¹ï¸  No affiliate program configured for {lang}")
            else:
                safe_print(f"  â„¹ï¸  No product mentions detected")
        else:
            safe_print(f"  â„¹ï¸  Affiliate links disabled for category: {category}")

        # Add affiliate disclosure if links were added
        if affiliate_programs_used:
            disclosure = get_affiliate_disclosure(lang, affiliate_programs_used)
            content = content.rstrip() + disclosure
            safe_print(f"  âš ï¸  Added affiliate disclosure")

        # Write file with hero image at top
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(frontmatter)
            f.write(hero_image)
            f.write(content)
            f.write(credit_line)

        safe_print(f"  ğŸ’¾ Saved to: {filepath}")
        return filepath


def main():
    parser = argparse.ArgumentParser(description="Generate blog posts")
    parser.add_argument("--count", type=int, default=3, help="Number of posts to generate")
    parser.add_argument("--topic-id", type=str, help="Specific topic ID to generate")
    args = parser.parse_args()

    # Pre-flight checks
    safe_print(f"\n{'='*60}")
    safe_print(f"  ğŸ” Pre-flight Environment Checks")
    safe_print(f"{'='*60}\n")

    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    unsplash_key = os.environ.get("UNSPLASH_ACCESS_KEY")

    if anthropic_key:
        safe_print("  âœ“ ANTHROPIC_API_KEY: Configured")
    else:
        safe_print("  âŒ ANTHROPIC_API_KEY: NOT FOUND")

    if unsplash_key:
        safe_print("  âœ“ UNSPLASH_ACCESS_KEY: Configured")
    else:
        safe_print("  âš ï¸  UNSPLASH_ACCESS_KEY: NOT FOUND")
        safe_print("     Posts will use placeholder images!")

    safe_print("")

    # Initialize generator
    try:
        generator = ContentGenerator()
    except ValueError as e:
        safe_print(f"Error: {str(e)}")
        safe_print("\nSet ANTHROPIC_API_KEY environment variable:")
        safe_print("  export ANTHROPIC_API_KEY='your-api-key'")
        sys.exit(1)

    # Get topics
    if args.topic_id:
        # Load specific topic (for testing)
        from topic_queue import get_queue
        queue = get_queue()
        data = queue._load_queue()
        topics = [t for t in data['topics'] if t['id'] == args.topic_id]
        if not topics:
            safe_print(f"Error: Topic {args.topic_id} not found")
            sys.exit(1)
    else:
        # Reserve topics from queue
        topics = reserve_topics(count=args.count)

    if not topics:
        safe_print("No topics available in queue")
        sys.exit(0)

    safe_print(f"\n{'='*60}")
    safe_print(f"  Generating {len(topics)} posts")
    safe_print(f"{'='*60}\n")

    generated_files = []

    for i, topic in enumerate(topics, 1):
        safe_print(f"[{i}/{len(topics)}] {topic['id']}")
        safe_print(f"  Keyword: {topic['keyword']}")
        safe_print(f"  Category: {topic['category']}")
        safe_print(f"  Language: {topic['lang']}")

        try:
            # Generate content
            safe_print(f"  â†’ Step 1/5: Generating draft...")
            draft = generator.generate_draft(topic)

            safe_print(f"  â†’ Step 2/5: Editing draft...")
            final_content = generator.edit_draft(draft, topic)

            # Generate metadata
            safe_print(f"  â†’ Step 3/5: Generating metadata...")
            try:
                title = generator.generate_title(final_content, topic['keyword'], topic['lang'], topic.get('references'))
                description = generator.generate_description(final_content, topic['keyword'], topic['lang'])
            except Exception as e:
                safe_print(f"  âš ï¸  WARNING: Metadata generation failed, using defaults")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                title = topic['keyword']
                description = f"Article about {topic['keyword']}"

            # Fetch featured image
            safe_print(f"  â†’ Step 4/5: Fetching image...")
            image_path = None
            image_credit = None
            try:
                image_info = generator.fetch_featured_image(topic['keyword'], topic['category'])
                if image_info:
                    image_path = generator.download_image(image_info, topic['keyword'])
                    if image_path:
                        image_credit = image_info
            except Exception as e:
                safe_print(f"  âš ï¸  WARNING: Image fetch failed, will use placeholder")
                safe_print(f"     Error: {mask_secrets(str(e))}")

            # Save post with image
            safe_print(f"  â†’ Step 5/5: Saving post...")
            try:
                filepath = generator.save_post(topic, title, description, final_content, image_path, image_credit)
            except IOError as e:
                safe_print(f"  âŒ ERROR: Failed to save post to filesystem")
                safe_print(f"     Error: {str(e)}")
                raise
            except Exception as e:
                safe_print(f"  âŒ ERROR: Unexpected error during save")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                raise

            # Mark as completed
            if not args.topic_id:
                try:
                    mark_completed(topic['id'])
                except Exception as e:
                    safe_print(f"  âš ï¸  WARNING: Failed to mark topic as completed in queue")
                    safe_print(f"     Topic ID: {topic['id']}")
                    safe_print(f"     Error: {str(e)}")
                    # Don't fail the whole process if queue update fails

            generated_files.append(str(filepath))
            safe_print(f"  âœ… Completed!\n")

        except KeyError as e:
            safe_print(f"  âŒ FAILED: Missing required field in topic data")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Missing field: {str(e)}\n")
            if not args.topic_id:
                mark_failed(topic['id'], f"Missing field: {str(e)}")
        except ValueError as e:
            safe_print(f"  âŒ FAILED: Invalid data or API response")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))
        except Exception as e:
            safe_print(f"  âŒ FAILED: Unexpected error")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error type: {type(e).__name__}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))

    # Save generated files list for quality gate
    output_file = Path("generated_files.json")
    with open(output_file, 'w') as f:
        json.dump(generated_files, f, indent=2)

    # Post-generation quality check
    safe_print(f"\n{'='*60}")
    safe_print(f"  ğŸ“Š Post-Generation Quality Check")
    safe_print(f"{'='*60}\n")

    posts_without_references = 0
    posts_with_placeholders = 0

    for filepath in generated_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

                # Check for references section
                has_references = '## References' in content or '## å‚è€ƒ' in content or '## ì°¸ê³ ìë£Œ' in content
                if not has_references:
                    posts_without_references += 1
                    safe_print(f"  âš ï¸  No references: {Path(filepath).name}")

                # Check for placeholder images
                if 'placeholder-' in content:
                    posts_with_placeholders += 1
                    safe_print(f"  âš ï¸  Placeholder image: {Path(filepath).name}")
        except Exception as e:
            safe_print(f"  âš ï¸  Could not check: {Path(filepath).name}")

    safe_print("")

    if posts_without_references > 0:
        safe_print(f"ğŸš¨ WARNING: {posts_without_references}/{len(generated_files)} posts have NO references!")
        safe_print(f"   This reduces content credibility and SEO value.")
        safe_print(f"   FIX: Ensure Google Custom Search API is configured in keyword curation\n")

    if posts_with_placeholders > 0:
        safe_print(f"ğŸš¨ WARNING: {posts_with_placeholders}/{len(generated_files)} posts use PLACEHOLDER images!")
        safe_print(f"   This hurts user experience and engagement.")
        safe_print(f"   FIX: Ensure UNSPLASH_ACCESS_KEY is set in environment variables\n")

    if posts_without_references == 0 and posts_with_placeholders == 0:
        safe_print(f"âœ… Quality Check PASSED: All posts have references and real images!\n")

    safe_print(f"{'='*60}")
    safe_print(f"  âœ“ Generated {len(generated_files)} posts")
    safe_print(f"  File list saved to: {output_file}")
    safe_print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
