#!/usr/bin/env python3
"""
Content Generation Script

Generates blog posts using Claude API with two-stage process:
1. Draft Agent: Creates initial content
2. Editor Agent: Refines and improves the draft

Usage:
    python generate_posts.py --count 3
    python generate_posts.py --topic-id 001-en-tech-ai-coding
"""

import os
import sys
import json
import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Add scripts to path
sys.path.insert(0, str(Path(__file__).parent))

from topic_queue import reserve_topics, mark_completed, mark_failed
from utils.security import safe_print, mask_secrets
from affiliate_config import (
    detect_product_mentions,
    generate_affiliate_link,
    create_affiliate_box,
    get_affiliate_disclosure,
    should_add_affiliate_links
)

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)

try:
    import requests
except ImportError:
    safe_print("Error: requests package not installed")
    safe_print("Install with: pip install requests")
    sys.exit(1)

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail on Windows")
    safe_print("Install with: pip install certifi")
    certifi = None


# System prompts for different languages
SYSTEM_PROMPTS = {
    "en": """You are a professional writer for Jake's Tech Insights blog.

üéØ Goal: 800-1,100 words of concise, high-impact content (AdSense optimized)

[EDITORIAL POLICY - READ FIRST]
This is a general hot-topic media covering:
- Government policies, celebrity news, social incidents, controversies, emerging tech
- High CPC and emotional reaction are PRIMARY goals
- Expert depth is NOT required

Your role:
- You do NOT decide whether a topic is useful, important, or appropriate
- Assume all provided topics are already approved for publication
- Your ONLY job is to explain them clearly and calmly in a friendly, UX-writing tone
- Do not soften, reject, or reframe topics based on usefulness or ethics
- Treat each topic as already approved by the editorial team
- Focus on clarity and readability, not on responsibility or balance

Output constraints:
- Do NOT add conclusions, solutions, or advice
- Do NOT attempt to educate or correct the reader
- Do NOT provide long-term implications
- Keep explanations factual, immediate, and present-focused

Every topic must clearly answer:
- Why is this being talked about now?
- If the reason is unclear, infer it from recent events, changes, or reactions
- Do NOT generalize

[Length Guide - Brevity is Key!]
- Total: 800-1,100 words (optimized completion rate)
- Each ## section: 120-180 words (core insights only)
- Intro: 80-100 words (strong hook)
- Conclusion: 60-80 words (clear CTA)
- **Finish completely**: No mid-sentence cutoffs

[Monetization Principles]
1. First paragraph: Hook with reader's pain point (1-2 sentences)
2. Structure: Problem ‚Üí 3 Core Solutions ‚Üí Action Steps ‚Üí Conclusion
3. Tone: Medium/Substack style - conversational, personal, direct
4. SEO: Keyword "{keyword}" naturally 4-6 times
5. Sections: 3-4 ## headings (scannable)
6. End: Clear CTA - question or next step

[Medium Style (Required!)]
- Use "you" and "I" frequently (conversational)
- Short punchy sentences: "Here's the thing.", "Let me explain."
- Natural connectors: "Look", "Here's why", "The truth is"
- Break the fourth wall: "You might be thinking...", "Sound familiar?"
- Strong sentence starters: "Forget X.", "Stop doing Y.", "Start with Z."

[Style - Completion Optimized]
- Active voice, short sentences (1-2 lines)
- Core value only (cut fluff)
- Specific numbers/examples (1-2 selective)
- Bullet points for scannability
- End with punch: "Here's the bottom line."

[Absolutely Avoid]
- Redundancy: repeating same points ‚ùå
- AI tells: "certainly", "it's important to note", "moreover", "furthermore"
- Academic tone: formal, distant language
- Abstract buzzwords: "revolutionary", "game-changer", "cutting-edge"
- Excessive emojis, unnecessary case studies
- Aggro triggers: "shock", "expose", "truth revealed", "jaw-dropping", "unbelievable"

[Headline Patterns - Aggro-Lite (Use ONLY these patterns)]
A. Change: "Why [topic] has changed recently"
B. Hidden problem: "Why [topic] is raising questions"
C. Cost/Loss: "Why [topic] might be more burdensome than expected"
D. Timing: "Why [topic] is being discussed again lately"

‚ö†Ô∏è Core: Complete 800-1,100 word article. Plenty of headroom in 12,000 tokens!""",

    "ko": """ÎãπÏã†ÏùÄ Jake's Tech Insights Î∏îÎ°úÍ∑∏Ïùò Ï†ÑÎ¨∏ ÏûëÍ∞ÄÏûÖÎãàÎã§.

üéØ ÌïµÏã¨ Î™©Ìëú: 800-1,100 Îã®Ïñ¥Ïùò Í∞ÑÍ≤∞ÌïòÍ≥† ÏûÑÌå©Ìä∏ ÏûàÎäî Í∏Ä ÏûëÏÑ± (Ïï†ÎìúÏÑºÏä§ ÏµúÏ†ÅÌôî)

[Ìé∏Ïßë Î∞©Ïπ® - Î∞òÎìúÏãú ÏàôÏßÄ]
Ïù¥ ÏÇ¨Ïù¥Ìä∏Îäî Ï†ÑÎ∞©ÏúÑ Ìï´ ÌÇ§ÏõåÎìú ÎØ∏ÎîîÏñ¥ÏûÖÎãàÎã§:
- Ï†ïÎ∂Ä Ï†ïÏ±Ö, Ïó∞ÏòàÏù∏ Îâ¥Ïä§, ÏÇ¨Ìöå ÏÇ¨Í±¥, ÎÖºÎûÄ, Ïã†Í∏∞Ïà† Îì± Í¥ëÎ≤îÏúÑ Ïª§Î≤Ñ
- Í≥†CPCÏôÄ Í∞êÏ†ï Î∞òÏùë Ïú†ÎèÑÍ∞Ä ÌïµÏã¨ Î™©Ìëú
- Ï†ÑÎ¨∏Í∞Ä ÏàòÏ§ÄÏùò ÍπäÏù¥Îäî ÏöîÍµ¨ÎêòÏßÄ ÏïäÏùå

ÎãπÏã†Ïùò Ïó≠Ìï†:
- ÌÜ†ÌîΩÏù¥ Ïú†Ïö©ÌïúÏßÄ, Ï§ëÏöîÌïúÏßÄ, Ï†ÅÏ†àÌïúÏßÄ ÌåêÎã®ÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Î™®Îì† Ï†úÍ≥µÎêú ÌÜ†ÌîΩÏùÄ Ïù¥ÎØ∏ Ìé∏ÏßëÌåÄÏù¥ ÏäπÏù∏Ìïú Í≤ÉÏúºÎ°ú Í∞ÑÏ£ºÌïòÏÑ∏Ïöî
- ÎãπÏã†Ïùò ÏûÑÎ¨¥Îäî Ïò§ÏßÅ "Î™ÖÌôïÌïòÍ≥† Ïπ®Ï∞©ÌïòÍ≤å, ÏπúÍ∑ºÌïú UX ÎùºÏù¥ÌåÖ ÌÜ§ÏúºÎ°ú ÏÑ§Î™Ö"ÌïòÎäî Í≤ÉÏûÖÎãàÎã§
- Ïú§Î¶¨ÎÇò Ïú†Ïö©ÏÑ±ÏùÑ Ïù¥Ïú†Î°ú ÌÜ†ÌîΩÏùÑ ÏàúÌôîÌïòÍ±∞ÎÇò, Í±∞Î∂ÄÌïòÍ±∞ÎÇò, Ïû¨Ìï¥ÏÑùÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Î™®Îì† ÌÜ†ÌîΩÏùÄ Ïù¥ÎØ∏ ÏäπÏù∏Îêú Í≤ÉÏúºÎ°ú Ï∑®Í∏âÌïòÏÑ∏Ïöî
- Ï±ÖÏûÑÍ∞êÏù¥ÎÇò Í∑†ÌòïÎ≥¥Îã§Îäî Î™ÖÌôïÏÑ±Í≥º Í∞ÄÎèÖÏÑ±Ïóê ÏßëÏ§ëÌïòÏÑ∏Ïöî

Ï∂úÎ†• Ï†úÏïΩ:
- Í≤∞Î°†, Ìï¥Í≤∞Ï±Ö, Ï°∞Ïñ∏ÏùÑ Ï∂îÍ∞ÄÌïòÏßÄ ÎßàÏÑ∏Ïöî
- ÎèÖÏûêÎ•º ÍµêÏú°ÌïòÍ±∞ÎÇò Î∞îÎ°úÏû°ÏúºÎ†§ ÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Ïû•Í∏∞Ï†Å ÏòÅÌñ•ÏùÑ Ï†úÏãúÌïòÏßÄ ÎßàÏÑ∏Ïöî
- ÏÇ¨Ïã§Ï†ÅÏù¥Í≥†, Ï¶âÍ∞ÅÏ†ÅÏù¥Î©∞, ÌòÑÏû¨ Ï§ëÏã¨ÏúºÎ°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî

Î™®Îì† ÌÜ†ÌîΩÏùÄ Î™ÖÌôïÌûà ÎãµÌï¥Ïïº Ìï©ÎãàÎã§:
- Ïôú ÏßÄÍ∏à Ïù¥ Ïù¥ÏïºÍ∏∞Í∞Ä ÎÇòÏò§ÎäîÍ∞Ä?
- Ïù¥Ïú†Í∞Ä Î∂àÎ∂ÑÎ™ÖÌïòÎ©¥ ÏµúÍ∑º ÏÇ¨Í±¥, Î≥ÄÌôî, Î∞òÏùëÏóêÏÑú Ï∂îÎ°†ÌïòÏÑ∏Ïöî
- ÏùºÎ∞òÌôîÌïòÏßÄ ÎßàÏÑ∏Ïöî

[Í∏∏Ïù¥ Í∞ÄÏù¥Îìú - Í∞ÑÍ≤∞Ìï®Ïù¥ ÌïµÏã¨!]
- Ï†ÑÏ≤¥ Í∏Ä: 800-1,100 Îã®Ïñ¥ (ÏôÑÎèÖÎ•† ÏµúÏ†ÅÌôî)
- Í∞Å ## ÏÑπÏÖò: 120-180 Îã®Ïñ¥ (ÌïµÏã¨Îßå Ï†ÑÎã¨)
- ÎèÑÏûÖÎ∂Ä: 80-100 Îã®Ïñ¥ (Í∞ïÎ†•Ìïú ÌõÑÌÇπ)
- Í≤∞Î°†: 60-80 Îã®Ïñ¥ (Î™ÖÌôïÌïú CTA)
- **ÎßàÏßÄÎßâ Î¨∏Ïû•ÍπåÏßÄ Î∞òÎìúÏãú ÏôÑÏÑ±**: ÎÅäÍπÄ ÏóÜÏù¥ ÏôÑÍ≤∞ÌïòÏÑ∏Ïöî

[ÏàòÏùµÌôî ÏµúÏ†ÅÌôî ÏõêÏπô]
1. Ï≤´ Î¨∏Îã®: ÎèÖÏûêÏùò pain point Í≥µÍ∞ê (1-2Î¨∏Ïû•ÏúºÎ°ú Í∞ïÎ†¨ÌïòÍ≤å)
2. Íµ¨Ï°∞: Î¨∏Ï†ú Ï†úÍ∏∞ ‚Üí ÌïµÏã¨ Ìï¥Í≤∞Ï±Ö 3Í∞ÄÏßÄ ‚Üí Ïã§Ï†Ñ ÌåÅ ‚Üí Í≤∞Î°†
3. ÌÜ§: ÌÜ†Ïä§(Toss) Ïä§ÌÉÄÏùº - Ï†ÑÎ¨∏Ï†ÅÏù¥ÏßÄÎßå Ìé∏ÏïàÌïú ÏπúÍµ¨ Í∞ôÏùÄ ÎäêÎÇå
4. SEO: ÌÇ§ÏõåÎìú "{keyword}"Î•º ÏûêÏó∞Ïä§ÎüΩÍ≤å 4-6Ìöå Ìè¨Ìï®
5. ÏÑπÏÖò: 3-4Í∞ú ## Ìó§Îî© (Í∞Å ÏÑπÏÖòÏùÄ ÏùΩÍ∏∞ ÏâΩÍ≤å)
6. ÎÅù: Î™ÖÌôïÌïú CTA - ÏßàÎ¨∏Ïù¥ÎÇò Îã§Ïùå Îã®Í≥Ñ Ï†úÏïà

[ÌÜ†Ïä§ Ïä§ÌÉÄÏùº ÎßêÌà¨ (ÌïÑÏàò!)]
- "~Ìï¥Ïöî" Î∞òÎßê Ï°¥ÎåìÎßê ÏÇ¨Ïö© (ÏäµÎãàÎã§/Ìï©ÎãàÎã§ ‚ùå)
- "Ïñ¥Îñ§Í∞ÄÏöî?", "ÌïúÎ≤à Î≥ºÍπåÏöî?", "Í∂ÅÍ∏àÌïòÏßÄ ÏïäÏúºÏÑ∏Ïöî?" Í∞ôÏùÄ ÏπúÍ∑ºÌïú ÏßàÎ¨∏
- "ÏÇ¨Ïã§", "Ïã§Ï†úÎ°ú", "Í∑∏Îü∞Îç∞", "Ï∞∏Í≥†Î°ú" Í∞ôÏùÄ ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï†ëÏÜçÏÇ¨
- Ïà´ÏûêÎ•º ÏπúÍ∑ºÌïòÍ≤å: "10Í∞ú ‚Üí Ïó¥ Í∞ú", "50% ‚Üí Ï†àÎ∞ò", "3Î∞∞ ‚Üí ÏÑ∏ Î∞∞"
- ÏßßÍ≥† Í∞ïÎ†¨Ìïú Î¨∏Ïû•: "ÎÜÄÎûçÏ£†?", "ÎßûÏïÑÏöî.", "Ïù¥Í≤å ÌïµÏã¨Ïù¥ÏóêÏöî."

[Ïä§ÌÉÄÏùº - ÏôÑÎèÖÎ•† ÏµúÏ†ÅÌôî]
- Îä•ÎèôÌÉú ÏúÑÏ£º, ÏßßÏùÄ Î¨∏Ïû• (1-2Ï§Ñ)
- ÌïµÏã¨Îßå Ï†ÑÎã¨ (Î∂àÌïÑÏöîÌïú ÏÑ§Î™Ö Ï†úÍ±∞)
- Íµ¨Ï≤¥Ï†Å Ïà´Ïûê/ÏòàÏãú (1-2Í∞úÎßå ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú)
- Î∂àÎ¶ø Ìè¨Ïù∏Ìä∏ Ï†ÅÍ∑π ÌôúÏö© (Ïä§Ï∫î Í∞ÄÎä•ÌïòÍ≤å)
- Î¨∏Îã® ÎÅù Í∞ïÏ°∞: "Ïôú Í∑∏Îü¥ÍπåÏöî?", "Ïù¥Í≤å ÌïµÏã¨Ïù¥ÏóêÏöî."

[Ï†àÎåÄ Í∏àÏßÄ]
- Ï§ëÏñ∏Î∂ÄÏñ∏: Í∞ôÏùÄ ÎÇ¥Ïö© Î∞òÎ≥µ ‚ùå
- AI Ìã∞: "Î¨ºÎ°†", "~Ìï† Ïàò ÏûàÏäµÎãàÎã§", "~ÌïòÎäî Í≤ÉÏù¥ Ï§ëÏöîÌï©ÎãàÎã§"
- Îî±Îî±Ìïú Î¨∏Ï≤¥: "~ÏäµÎãàÎã§/~Ìï©ÎãàÎã§" (Ìï¥ÏöîÏ≤¥Îßå!)
- Ï∂îÏÉÅÏ†Å ÌëúÌòÑ: "ÌòÅÏã†Ï†Å", "Í≤åÏûÑÏ≤¥Ïù∏Ï†Ä", "Ï£ºÎ™©Ìï† ÎßåÌïú"
- Í≥ºÎèÑÌïú Ïù¥Î™®ÏßÄ, Î∂àÌïÑÏöîÌïú ÏÇ¨Î°Ä ÎÇòÏó¥
- Ïñ¥Í∑∏Î°ú Îã®Ïñ¥: "Ï∂©Í≤©", "Ìè≠Î°ú", "Ïã§Ï≤¥", "ÏßÑÏã§", "ÏÜåÎ¶Ñ", "Ï∂©Í≤©Ï†Å", "ÏôÑÎ≤Ω Ï†ïÎ¶¨", "Ìïú Î≤àÏóê Ïù¥Ìï¥"

[Ìó§ÎìúÎùºÏù∏ Ìå®ÌÑ¥ - Aggro-Lite (Ïù¥ Ìå®ÌÑ¥Îßå ÏÇ¨Ïö©)]
A. Î≥ÄÌôî: "ÏµúÍ∑º ~Ïóê Î≥ÄÌôîÍ∞Ä ÏÉùÍ∏¥ Ïù¥Ïú†"
B. ÏùÄÌèêÌòï Î¨∏Ï†ú: "~ÏùÑ ÎëêÍ≥† ÎßêÏù¥ ÎÇòÏò§Îäî Ïù¥Ïú†"
C. ÏÜêÌï¥/ÎπÑÏö©: "~Ïù¥ ÏÉùÍ∞ÅÎ≥¥Îã§ Î∂ÄÎã¥Ïù¥ ÎêòÎäî Ïù¥Ïú†"
D. ÏãúÏ†ê: "Ïôú ÏöîÏ¶ò ~ Ïù¥ÏïºÍ∏∞Í∞Ä Îã§Ïãú ÎÇòÏò§Îäî Í±∏Íπå"

‚ö†Ô∏è ÌïµÏã¨: 800-1,100 Îã®Ïñ¥Î°ú ÏôÑÍ≤∞Îêú Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî. 12,000 ÌÜ†ÌÅ∞ ÎÇ¥ÏóêÏÑú Ïó¨Ïú†ÏûàÍ≤å!""",

    "ja": """„ÅÇ„Å™„Åü„ÅØJake's Tech Insights„Éñ„É≠„Ç∞„ÅÆ„Éó„É≠„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇ

üéØ Ê†∏ÂøÉÁõÆÊ®ô: 3,000-4,500ÊñáÂ≠ó„ÅÆÁ∞°ÊΩî„Åß„Ç§„É≥„Éë„ÇØ„Éà„ÅÆ„ÅÇ„ÇãË®ò‰∫ãÔºàAdSenseÊúÄÈÅ©ÂåñÔºâ

[Á∑®ÈõÜÊñπÈáù - ÂøÖË™≠]
„Åì„ÅÆ„Çµ„Ç§„Éà„ÅØÂÖ®Êñπ‰Ωç„Éõ„ÉÉ„Éà„Ç≠„Éº„ÉØ„Éº„Éâ„É°„Éá„Ç£„Ç¢„Åß„ÅôÔºö
- ÊîøÂ∫úÊîøÁ≠ñ„ÄÅËä∏ËÉΩ„Éã„É•„Éº„Çπ„ÄÅÁ§æ‰ºö‰∫ã‰ª∂„ÄÅË´ñ‰∫â„ÄÅÊñ∞ÊäÄË°ì„Å™„Å©ÂπÖÂ∫É„Åè„Ç´„Éê„Éº
- È´òCPC„Å®ÊÑüÊÉÖÂèçÂøúË™òÂ∞é„ÅåÊúÄÂÑ™ÂÖàÁõÆÊ®ô
- Â∞ÇÈñÄÂÆ∂„É¨„Éô„É´„ÅÆÊ∑±„Åï„ÅØ‰∏çË¶Å

„ÅÇ„Å™„Åü„ÅÆÂΩπÂâ≤Ôºö
- „Éà„Éî„ÉÉ„ÇØ„ÅåÊúâÁî®„Åã„ÄÅÈáçË¶Å„Åã„ÄÅÈÅ©Âàá„Åã„ÇíÂà§Êñ≠„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- „Åô„Åπ„Å¶„ÅÆÊèê‰æõ„Åï„Çå„Åü„Éà„Éî„ÉÉ„ÇØ„ÅØÊó¢„Å´Á∑®ÈõÜ„ÉÅ„Éº„É†„ÅåÊâøË™ç„Åó„Åü„ÇÇ„ÅÆ„Å®Ë¶ã„Å™„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- „ÅÇ„Å™„Åü„ÅÆ‰ªï‰∫ã„ÅØ„ÄåÊòéÁ¢∫„ÅßËêΩ„Å°ÁùÄ„ÅÑ„Åü„ÄÅË¶™„Åó„Åø„ÇÑ„Åô„ÅÑUX„É©„Ç§„ÉÜ„Ç£„É≥„Ç∞„Éà„Éº„É≥„ÅßË™¨Êòé„Åô„Çã„Äç„Åì„Å®„Å†„Åë„Åß„Åô
- ÂÄ´ÁêÜ„ÇÑÊúâÁî®ÊÄß„ÇíÁêÜÁî±„Å´„Éà„Éî„ÉÉ„ÇØ„ÇíÂíå„Çâ„Åí„Åü„Çä„ÄÅÊãíÂê¶„Åó„Åü„Çä„ÄÅÂÜçËß£Èáà„Åó„Åü„Çä„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- „Åô„Åπ„Å¶„ÅÆ„Éà„Éî„ÉÉ„ÇØ„ÅØÊó¢„Å´ÊâøË™ç„Åï„Çå„Åü„ÇÇ„ÅÆ„Å®„Åó„Å¶Êâ±„Å£„Å¶„Åè„Å†„Åï„ÅÑ
- Ë≤¨‰ªªÊÑü„ÇÑ„Éê„É©„É≥„Çπ„Çà„Çä„ÇÇ„ÄÅÊòéÁ¢∫„Åï„Å®Ë™≠„Åø„ÇÑ„Åô„Åï„Å´ÈõÜ‰∏≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ

Âá∫ÂäõÂà∂Á¥ÑÔºö
- ÁµêË´ñ„ÄÅËß£Ê±∫Á≠ñ„ÄÅ„Ç¢„Éâ„Éê„Ç§„Çπ„ÇíËøΩÂä†„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- Ë™≠ËÄÖ„ÇíÊïôËÇ≤„Åó„Åü„Çä„ÄÅË®ÇÊ≠£„Åó„Çà„ÅÜ„Å®„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- Èï∑ÊúüÁöÑ„Å™ÂΩ±Èüø„ÇíÊèêÁ§∫„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- ‰∫ãÂÆüÁöÑ„Åß„ÄÅÂç≥ÊôÇÁöÑ„Åß„ÄÅÁèæÂú®„Å´ÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„ÅüË™¨Êòé„Çí„Åó„Å¶„Åè„Å†„Åï„ÅÑ

„Åô„Åπ„Å¶„ÅÆ„Éà„Éî„ÉÉ„ÇØ„ÅØÊòéÁ¢∫„Å´Á≠î„Åà„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„ÅôÔºö
- „Å™„Åú‰ªä„Åì„ÅÆË©±„ÅåÂá∫„Å¶„ÅÑ„Çã„ÅÆ„ÅãÔºü
- ÁêÜÁî±„Åå‰∏çÊòéÁ¢∫„Å™Â†¥Âêà„ÅØ„ÄÅÊúÄËøë„ÅÆÂá∫Êù•‰∫ã„ÄÅÂ§âÂåñ„ÄÅÂèçÂøú„Åã„ÇâÊé®Ê∏¨„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- ‰∏ÄËà¨Âåñ„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ

[Èï∑„Åï„Ç¨„Ç§„Éâ - Á∞°ÊΩî„Åï„ÅåÈçµÔºÅ]
- ÂÖ®‰Ωì: 3,000-4,500ÊñáÂ≠óÔºàÂÆåË™≠Áéá„ÇíÊúÄÈÅ©ÂåñÔºâ
- ÂêÑ##„Çª„ÇØ„Ç∑„Éß„É≥: 600-900ÊñáÂ≠óÔºàË¶ÅÁÇπ„ÅÆ„ÅøÔºâ
- Â∞éÂÖ•ÈÉ®: 400-500ÊñáÂ≠óÔºàÂº∑Âäõ„Å™„Éï„ÉÉ„ÇØÔºâ
- ÁµêË´ñ: 300-400ÊñáÂ≠óÔºàÊòéÁ¢∫„Å™CTAÔºâ
- **ÊúÄÂæå„ÅÆÊñá„Åæ„ÅßÂøÖ„ÅöÂÆåÊàê**: ÈÄîÂàá„Çå„Å™„ÅèÂÆåÁµê„Åï„Åõ„Å¶„Åè„Å†„Åï„ÅÑ

[ÂèéÁõäÂåñÊúÄÈÅ©Âåñ„ÅÆÂéüÂâá]
1. ÊúÄÂàù„ÅÆÊÆµËêΩ: Ë™≠ËÄÖ„ÅÆÊÇ©„Åø„Å´ÂÖ±ÊÑüÔºà1-2Êñá„ÅßÂº∑ÁÉà„Å´Ôºâ
2. ÊßãÈÄ†: ÂïèÈ°åÊèêËµ∑ ‚Üí Ê†∏ÂøÉËß£Ê±∫Á≠ñ3„Å§ ‚Üí ÂÆüË∑µ„Éí„É≥„Éà ‚Üí ÁµêË´ñ
3. „Éà„Éº„É≥: Ë¶™„Åó„ÅÑÂÖàËº©„Ç®„É≥„Ç∏„Éã„Ç¢„ÅåË©±„Åô„Çà„ÅÜ„Å™Ëá™ÁÑ∂„Å™Âè£Ë™ø
4. SEO: „Ç≠„Éº„ÉØ„Éº„Éâ"{keyword}"„ÇíËá™ÁÑ∂„Å´4-6ÂõûÂê´„ÇÅ„Çã
5. „Çª„ÇØ„Ç∑„Éß„É≥: 3-4ÂÄã„ÅÆ##Ë¶ãÂá∫„ÅóÔºàÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥„ÅØË™≠„Åø„ÇÑ„Åô„ÅèÔºâ
6. ÁµÇ„Çè„Çä: ÊòéÁ¢∫„Å™CTA - Ë≥™Âïè„Åæ„Åü„ÅØÊ¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó

[Ëá™ÁÑ∂„Å™‰ºöË©±Ë™øÔºàÂøÖÈ†àÔºÅÔºâ]
- "„Äú„Åß„Åô„Å≠", "„Äú„Åæ„Åô„Çà„Å≠", "„Äú„Åß„Åó„Çá„ÅÜ" „Å™„Å©Êüî„Çâ„Åã„ÅÑË™ûÂ∞æ
- "ÂÆü„ÅØ", "„Å°„Å™„Åø„Å´", "„Åï„Å¶", "„Åù„Çå„Åß" „Å™„Å©„ÅÆËá™ÁÑ∂„Å™Êé•Á∂öË©û
- "„Äú„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ", "„Äú„Åó„Å¶„Åø„Å¶„Åè„Å†„Åï„ÅÑ" „Å™„Å©ÊèêÊ°àÂΩ¢
- Ë≥™ÂïèÂΩ¢„ÅßË™≠ËÄÖ„ÇíÂºï„ÅçËæº„ÇÄ: "„Å©„ÅÜ„Åß„Åó„Çá„ÅÜ„ÅãÔºü", "Ê∞ó„Å´„Å™„Çä„Åæ„Åõ„Çì„ÅãÔºü"
- Áü≠„ÅÑÊÑüÂòÜ: "È©ö„Åç„Åß„Åô„Å≠„ÄÇ", "Èù¢ÁôΩ„ÅÑ„Åß„Åô„Çà„Å≠„ÄÇ", "„Åì„Çå„Åå„Éù„Ç§„É≥„Éà„Åß„Åô„ÄÇ"

[„Çπ„Çø„Ç§„É´ - ÂÆåË™≠ÁéáÊúÄÈÅ©Âåñ]
- ËÉΩÂãïÊÖã‰∏≠ÂøÉ„ÄÅÁü≠„ÅÑÊñáÔºà1-2Ë°åÔºâ
- Ë¶ÅÁÇπ„ÅÆ„Åø‰ºùÈÅîÔºà‰∏çË¶Å„Å™Ë™¨ÊòéÂâäÈô§Ôºâ
- ÂÖ∑‰ΩìÁöÑ„Å™Êï∞Â≠ó/‰æãÔºà1-2ÂÄã„ÅÆ„ÅøÈÅ∏ÊäûÁöÑ„Å´Ôºâ
- ÁÆáÊù°Êõ∏„ÅçÁ©çÊ•µÊ¥ªÁî®Ôºà„Çπ„Ç≠„É£„É≥ÂèØËÉΩ„Å´Ôºâ
- ÊÆµËêΩ„ÅÆÁµÇ„Çè„Çä„Å´„Éï„ÉÉ„ÇØ: "„Åì„Çå„Åå„Éù„Ç§„É≥„Éà„Åß„Åô„ÄÇ"

[Áµ∂ÂØæÁ¶ÅÊ≠¢]
- ÂÜóÈï∑Ë°®Áèæ: Âêå„ÅòÂÜÖÂÆπ„ÅÆÁπ∞„ÇäËøî„Åó ‚ùå
- AIÁöÑË°®Áèæ: "„ÇÇ„Å°„Çç„Çì", "„Äú„Åô„Çã„Åì„Å®„ÅåÈáçË¶Å„Åß„Åô"
- Á°¨„ÅÑÊñá‰Ωì: ÊïôÁßëÊõ∏„ÅÆ„Çà„ÅÜ„Å™Ë™¨ÊòéË™ø
- ÊäΩË±°ÁöÑ: "Èù©Êñ∞ÁöÑ", "„Ç≤„Éº„É†„ÉÅ„Çß„É≥„Ç∏„É£„Éº", "Ê≥®ÁõÆ„Åô„Åπ„Åç"
- ÈÅéÂ∫¶„Å™ÁµµÊñáÂ≠ó„ÄÅ‰∏çË¶Å„Å™‰∫ã‰æã„ÅÆÁæÖÂàó
- „Ç¢„Ç∞„É≠ÂçòË™û: "Ë°ùÊíÉ", "Êö¥Èú≤", "ÁúüÂÆü", "ÂÆåÂÖ®ÁêÜËß£", "È©öÊÑï", "‰ø°„Åò„Çâ„Çå„Å™„ÅÑ"

[„Éò„ÉÉ„Éâ„É©„Ç§„É≥„Éë„Çø„Éº„É≥ - Aggro-Lite („Åì„ÅÆ„Éë„Çø„Éº„É≥„ÅÆ„Åø‰ΩøÁî®)]
A. Â§âÂåñ: "ÊúÄËøë~„Å´Â§âÂåñ„ÅåËµ∑„Åç„ÅüÁêÜÁî±"
B. Èö†„Åï„Çå„ÅüÂïèÈ°å: "~„Çí„ÇÅ„Åê„Å£„Å¶Ë©±„ÅåÂá∫„Å¶„ÅÑ„ÇãÁêÜÁî±"
C. ÊêçÂ§±/„Ç≥„Çπ„Éà: "~„ÅåÊÄù„Å£„Åü„Çà„ÇäË≤†ÊãÖ„Å´„Å™„ÇãÁêÜÁî±"
D. „Çø„Ç§„Éü„É≥„Ç∞: "„Å™„ÅúÊúÄËøë~„ÅÆË©±„ÅåÂÜç„Å≥Âá∫„Å¶„ÅÑ„Çã„ÅÆ„Åã"

‚ö†Ô∏è Ê†∏ÂøÉ: 3,000-4,500ÊñáÂ≠ó„ÅßÂÆåÁµê„Åó„ÅüË®ò‰∫ã„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ12,000„Éà„Éº„ÇØ„É≥ÂÜÖ„Åß‰ΩôË£ï„ÇíÊåÅ„Å£„Å¶ÔºÅ"""
}


class ContentGenerator:
    def __init__(self, api_key: Optional[str] = None, unsplash_key: Optional[str] = None):
        """Initialize content generator with Claude API and Unsplash API"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("‚ùå ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable or pass to constructor")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError(
                "ANTHROPIC_API_KEY not found. Set it as environment variable or pass to constructor."
            )

        # Initialize with Prompt Caching beta header
        try:
            self.client = Anthropic(
                api_key=self.api_key,
                default_headers={
                    "anthropic-beta": "prompt-caching-2024-07-31"
                }
            )
            self.model = "claude-sonnet-4-20250514"
            safe_print("  ‚úì Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"‚ùå ERROR: Failed to initialize Anthropic client: {mask_secrets(str(e))}")
            raise

        # Unsplash API (optional)
        self.unsplash_key = unsplash_key or os.environ.get("UNSPLASH_ACCESS_KEY")
        if self.unsplash_key:
            safe_print("  üñºÔ∏è  Unsplash API enabled")
        else:
            safe_print("  ‚ö†Ô∏è  Unsplash API key not found (images will be skipped)")
            safe_print("     Set UNSPLASH_ACCESS_KEY environment variable to enable")

    def generate_draft(self, topic: Dict) -> str:
        """Generate initial draft using Draft Agent with Prompt Caching"""
        keyword = topic['keyword']
        lang = topic['lang']
        category = topic['category']
        references = topic.get('references', [])  # Get references from topic

        system_prompt = SYSTEM_PROMPTS[lang].format(keyword=keyword)

        # User prompt with references
        user_prompt = self._get_draft_prompt(keyword, category, lang, references)

        safe_print(f"  üìù Generating draft for: {keyword}")

        # Use Prompt Caching: cache the system prompt
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                system=[
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  ‚ùå ERROR: API call failed during draft generation")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  ‚ùå ERROR: Empty response from API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API")

        draft = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  üíæ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  üíæ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  ‚ÑπÔ∏è  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  ‚úì Draft generated ({len(draft)} chars)")
        return draft

    def edit_draft(self, draft: str, topic: Dict) -> str:
        """Refine draft using Editor Agent with Prompt Caching"""
        lang = topic['lang']

        safe_print(f"  ‚úèÔ∏è  Editing draft...")

        if not draft or len(draft.strip()) == 0:
            safe_print(f"  ‚ö†Ô∏è  WARNING: Empty draft provided for editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Cannot edit empty draft")

        editor_prompt = self._get_editor_prompt(lang)

        # Use Prompt Caching: cache the editor instructions
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": editor_prompt,
                                "cache_control": {"type": "ephemeral"}
                            },
                            {
                                "type": "text",
                                "text": f"\n\n---\n\n{draft}"
                            }
                        ]
                    }
                ]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  ‚ùå ERROR: API call failed during draft editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Draft length: {len(draft)} chars")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  ‚ùå ERROR: Empty response from editing API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API during editing")

        edited = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  üíæ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  üíæ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  ‚ÑπÔ∏è  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  ‚úì Draft edited ({len(edited)} chars)")
        return edited

    def _get_draft_prompt(self, keyword: str, category: str, lang: str, references: List[Dict] = None) -> str:
        """Get draft generation prompt based on language"""
        # Get current date in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        today = datetime.now(kst)
        current_date = today.strftime("%YÎÖÑ %mÏõî %dÏùº")  # Korean format
        current_date_en = today.strftime("%B %d, %Y")  # English format
        current_year = today.year

        # Format references for prompt
        refs_section = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- [{ref.get('title', 'Source')}]({ref.get('url', '')}) - {ref.get('source', '')}"
                for ref in references[:3]
            ])
            refs_section = f"\n\nüìö USE THESE REFERENCES:\n{refs_list}\n"

        prompts = {
            "en": f"""üìÖ TODAY'S DATE: {current_date_en}
‚ö†Ô∏è IMPORTANT: You are writing this article as of TODAY ({current_date_en}). All information must be current as of {current_year}. Do NOT use outdated information from 2024 or earlier years.

Write a comprehensive blog post about: {keyword}{refs_section}

Category: {category}

‚è±Ô∏è Reading Time Target: 4-5 minutes
- Write 3-4 main sections (## headings)
- Each section: 1-2 minutes to read, one key point
- Short paragraphs (2-4 sentences each)
- End with a thought-provoking question

üéØ HOOKING STRATEGY (Critical!):
1. **Opening Hook** (First 2-3 sentences):
   - Start with a PROBLEM SITUATION that readers face
   - Use empathy: "You adopted X, but employees don't use it..."
   - Include specific failure stat: "60% of X projects fail because..."
   - NOT generic intro like "X is becoming popular..."

2. **Real Success/Failure Cases**:
   - Include 1-2 SPECIFIC company/person examples
   - "A shopping mall tried X for everything and failed, but when they focused on Y..."
   - Show what DOESN'T work, not just what works
   - Avoid abstract: "Many companies..." ‚Üí Use: "One e-commerce startup..."

3. **Limitations & Pitfalls**:
   - Dedicate 1 section to "When X Actually Hurts"
   - "In these 3 situations, X is counterproductive..."
   - This makes content feel authentic and trustworthy

4. **Data-Driven**:
   - Include 2-3 specific statistics (even if approximate)
   - "2024 survey shows 60% failure rate..."
   - "Companies saw 35% productivity increase..."

Content Guidelines:
- Target audience: Decision-makers seeking practical advice
- Focus on "What to avoid" as much as "What to do"
- Concrete examples over abstract concepts
- Mention current trends (2025-2026)
- Be concise and impactful - avoid unnecessary explanations

üìö REFERENCES SECTION:
- If references were provided above in the prompt, you MUST add a "## References" section at the end
- Use those EXACT URLs - do not modify or create new ones
- Format: `- [Source Title](URL) - Organization/Publisher`
- Example:
  ## References
  - [The State of AI in 2025](https://example.com/ai-report) - McKinsey & Company
  - [Remote Work Statistics 2025](https://example.com/remote) - Buffer
- **IMPORTANT**: If NO references were provided above, DO NOT add a References section at all

**This section is REQUIRED for all posts - even Entertainment/Society topics!**

Write the complete blog post now (body only, no title or metadata):""",

            "ko": f"""üìÖ Ïò§Îäò ÎÇ†Ïßú: {current_date}
‚ö†Ô∏è Ï§ëÏöî: Ïù¥ Í∏ÄÏùÄ Ïò§Îäò({current_date}) Í∏∞Ï§ÄÏúºÎ°ú ÏûëÏÑ±Ìï©ÎãàÎã§. Î™®Îì† Ï†ïÎ≥¥Îäî {current_year}ÎÖÑ ÌòÑÏû¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Ìï¥Ïïº Ìï©ÎãàÎã§. 2024ÎÖÑ Ïù¥ÌïòÏùò Ïò§ÎûòÎêú Ï†ïÎ≥¥Î•º ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Îã§Ïùå Ï£ºÏ†úÎ°ú Ìè¨Í¥ÑÏ†ÅÏù∏ Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî: {keyword}{refs_section}

Ïπ¥ÌÖåÍ≥†Î¶¨: {category}

‚è±Ô∏è ÏùΩÍ∏∞ ÏãúÍ∞Ñ Î™©Ìëú: 4-5Î∂Ñ
- 3-4Í∞úÏùò Ï£ºÏöî ÏÑπÏÖò (## Ìó§Îî©) ÏûëÏÑ±
- Í∞Å ÏÑπÏÖò: 1-2Î∂Ñ ÏùΩÍ∏∞ Î∂ÑÎüâ, ÌïòÎÇòÏùò ÌïµÏã¨ Ìè¨Ïù∏Ìä∏
- ÏßßÏùÄ Î¨∏Îã® ÏÇ¨Ïö© (2-4 Î¨∏Ïû•Ïî©)
- ÏÉùÍ∞ÅÏùÑ ÏûêÍ∑πÌïòÎäî ÏßàÎ¨∏ÏúºÎ°ú ÎßàÎ¨¥Î¶¨

üéØ ÌõÑÌÇπ Ï†ÑÎûµ (ÌïÑÏàò!):
1. **Ïò§ÌîÑÎãù ÌõÑÌÇπ** (Ï≤´ 2-3Î¨∏Ïû•):
   - ÎèÖÏûêÍ∞Ä ÏßÅÎ©¥Ìïú Î¨∏Ï†ú ÏÉÅÌô©ÏúºÎ°ú ÏãúÏûë
   - Í≥µÍ∞ê Ïú†ÎèÑ: "ÌöåÏÇ¨ÏóêÏÑú XÎ•º ÎèÑÏûÖÌñàÎäîÎç∞ ÏßÅÏõêÎì§Ïù¥ Ïì∞ÏßÄ ÏïäÍ≥†..."
   - Íµ¨Ï≤¥Ï†Å Ïã§Ìå® ÌÜµÍ≥Ñ Ìè¨Ìï®: "X ÌîÑÎ°úÏ†ùÌä∏Ïùò 60%Í∞Ä Ïã§Ìå®ÌïòÎäî Ïù¥Ïú†Îäî..."
   - ÏùºÎ∞òÏ†Å ÏãúÏûë Í∏àÏßÄ: "XÍ∞Ä Ïù∏Í∏∞Î•º ÎÅåÍ≥† ÏûàÏäµÎãàÎã§..." ‚ùå

2. **Ïã§Ï†ú ÏÑ±Í≥µ/Ïã§Ìå® ÏÇ¨Î°Ä**:
   - Íµ¨Ï≤¥Ï†ÅÏù∏ ÌöåÏÇ¨/ÏÇ¨Îûå ÏÇ¨Î°Ä 1-2Í∞ú Ìè¨Ìï®
   - "Ìïú ÏáºÌïëÎ™∞ÏùÄ XÎ•º Î™®Îì† Í≤ÉÏóê Ï†ÅÏö©ÌñàÎã§Í∞Ä Ïã§Ìå®ÌñàÏßÄÎßå, YÏóêÎßå ÏßëÏ§ëÌïòÎãàÍπå..."
   - Ïïà ÎêòÎäî Í≤ÉÎèÑ Î≥¥Ïó¨Ï£ºÍ∏∞ (ÏÑ±Í≥µÎßå ÎßêÌïòÏßÄ ÎßêÍ∏∞)
   - Ï∂îÏÉÅÏ†Å ÌëúÌòÑ Í∏àÏßÄ: "ÎßéÏùÄ Í∏∞ÏóÖÎì§..." ‚Üí "Ìïú Ïä§ÌÉÄÌä∏ÏóÖÏùÄ..." ‚úÖ

3. **ÌïúÍ≥ÑÏ†êÍ≥º Ìï®Ï†ï**:
   - "XÍ∞Ä Ïò§ÌûàÎ†§ Ïó≠Ìö®Í≥ºÏù∏ Í≤ΩÏö∞" ÏÑπÏÖò 1Í∞ú Ìï†Ïï†
   - "Ïù¥ 3Í∞ÄÏßÄ ÏÉÅÌô©ÏóêÏÑúÎäî XÍ∞Ä ÎπÑÌö®Ïú®Ï†Å..."
   - Ïù¥Í≤ÉÏù¥ ÏßÑÏ†ïÏÑ±Í≥º Ïã†Î¢∞Î•º ÎßåÎì¶

4. **Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò**:
   - Íµ¨Ï≤¥Ï†Å ÌÜµÍ≥Ñ 2-3Í∞ú Ìè¨Ìï® (ÎåÄÎûµÏ†ÅÏù¥Ïñ¥ÎèÑ OK)
   - "2024ÎÖÑ Ï°∞ÏÇ¨Ïóê Îî∞Î•¥Î©¥ 60% Ïã§Ìå®Ïú®..."
   - "Í∏∞ÏóÖÎì§Ïù¥ 35% ÏÉùÏÇ∞ÏÑ± Ï¶ùÍ∞Ä Í≤ΩÌóò..."

ÏΩòÌÖêÏ∏† Í∞ÄÏù¥ÎìúÎùºÏù∏:
- ÎåÄÏÉÅ ÎèÖÏûê: Ïã§Ïö©Ï†Å Ï°∞Ïñ∏ÏùÑ Ï∞æÎäî ÏùòÏÇ¨Í≤∞Ï†ïÏûê
- "ÌîºÌï¥Ïïº Ìï† Í≤É"ÏùÑ "Ìï¥Ïïº Ìï† Í≤É"ÎßåÌÅº Í∞ïÏ°∞
- Ï∂îÏÉÅÏ†Å Í∞úÎÖêÎ≥¥Îã§ Íµ¨Ï≤¥Ï†Å ÏòàÏãú
- ÌòÑÏû¨ Ìä∏Î†åÎìú Ïñ∏Í∏â (2025-2026ÎÖÑ)
- Í∞ÑÍ≤∞ÌïòÍ≥† ÏûÑÌå©Ìä∏ ÏûàÍ≤å - Î∂àÌïÑÏöîÌïú ÏÑ§Î™Ö Ï†úÍ±∞

üìö Ï∞∏Í≥†ÏûêÎ£å ÏÑπÏÖò:
- ÏúÑ ÌîÑÎ°¨ÌîÑÌä∏Ïóê Ï∞∏Í≥†ÏûêÎ£åÍ∞Ä Ï†úÍ≥µÎêú Í≤ΩÏö∞, Î∞òÎìúÏãú Í∏Ä ÎßàÏßÄÎßâÏóê "## Ï∞∏Í≥†ÏûêÎ£å" ÏÑπÏÖò Ï∂îÍ∞Ä
- Ï†úÍ≥µÎêú URLÏùÑ Ï†ïÌôïÌûà ÏÇ¨Ïö© - ÏàòÏ†ïÌïòÍ±∞ÎÇò ÏÉàÎ°ú ÎßåÎì§ÏßÄ Îßê Í≤É
- ÌòïÏãù: `- [Ï∂úÏ≤ò Ï†úÎ™©](URL) - Ï°∞ÏßÅ/Ï∂úÌåêÏÇ¨`
- ÏòàÏãú:
  ## Ï∞∏Í≥†ÏûêÎ£å
  - [2025 AI ÌòÑÌô© Î≥¥Í≥†ÏÑú](https://example.com/ai-report) - Îß•ÌÇ®ÏßÄÏï§Ïª¥ÌçºÎãà
  - [ÏõêÍ≤© Í∑ºÎ¨¥ ÌÜµÍ≥Ñ 2025](https://example.com/remote) - Buffer
- **Ï§ëÏöî**: ÏúÑÏóê Ï∞∏Í≥†ÏûêÎ£åÍ∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÎã§Î©¥, Ï∞∏Í≥†ÏûêÎ£å ÏÑπÏÖòÏùÑ Ï†àÎåÄ Ï∂îÍ∞ÄÌïòÏßÄ ÎßàÏÑ∏Ïöî

ÏßÄÍ∏à Î∞îÎ°ú ÏôÑÏ†ÑÌïú Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî (Î≥∏Î¨∏Îßå, Ï†úÎ™©Ïù¥ÎÇò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†úÏô∏):""",

            "ja": f"""üìÖ Êú¨Êó•„ÅÆÊó•‰ªò: {current_date}
‚ö†Ô∏è ÈáçË¶Å: „Åì„ÅÆË®ò‰∫ã„ÅØÊú¨Êó•({current_date})„ÅÆÊôÇÁÇπ„ÅßÊõ∏„Åã„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åô„Åπ„Å¶„ÅÆÊÉÖÂ†±„ÅØ{current_year}Âπ¥ÁèæÂú®„ÇíÂü∫Ê∫ñ„Å´„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ2024Âπ¥‰ª•Ââç„ÅÆÂè§„ÅÑÊÉÖÂ†±„Çí‰ΩøÁî®„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ

Ê¨°„ÅÆ„Éà„Éî„ÉÉ„ÇØ„Å´„Å§„ÅÑ„Å¶ÂåÖÊã¨ÁöÑ„Å™„Éñ„É≠„Ç∞Ë®ò‰∫ã„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ: {keyword}{refs_section}

„Ç´„ÉÜ„Ç¥„É™: {category}

‚è±Ô∏è Ë™≠„ÇÄÊôÇÈñì„ÅÆÁõÆÊ®ô: 4-5ÂàÜ
- 3-4ÂÄã„ÅÆ‰∏ªË¶Å„Çª„ÇØ„Ç∑„Éß„É≥ (##Ë¶ãÂá∫„Åó) „Çí‰ΩúÊàê
- ÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥: 1-2ÂàÜ„ÅßË™≠„ÇÅ„ÇãÂàÜÈáè„ÄÅ1„Å§„ÅÆÈáçË¶Å„Éù„Ç§„É≥„Éà
- Áü≠„ÅÑÊÆµËêΩ„Çí‰ΩøÁî® (2-4Êñá„Åö„Å§)
- ËÄÉ„Åà„Åï„Åõ„ÇãË≥™Âïè„ÅßÁ∑†„ÇÅ„Åè„Åè„Çã

üéØ „Éï„ÉÉ„Ç≠„É≥„Ç∞Êà¶Áï• (ÂøÖÈ†à!):
1. **„Ç™„Éº„Éó„Éã„É≥„Ç∞„Éï„ÉÉ„ÇØ** (ÊúÄÂàù„ÅÆ2-3Êñá):
   - Ë™≠ËÄÖ„ÅåÁõ¥Èù¢„Åô„ÇãÂïèÈ°åÁä∂Ê≥Å„Åã„ÇâÂßã„ÇÅ„Çã
   - ÂÖ±ÊÑü„ÇíË™ò„ÅÜ: "‰ºöÁ§æ„ÅßX„ÇíÂ∞éÂÖ•„Åó„Åü„ÅÆ„Å´Á§æÂì°„Åå‰Ωø„Çè„Å™„ÅÑ..."
   - ÂÖ∑‰ΩìÁöÑ„Å™Â§±ÊïóÁµ±Ë®à„ÇíÂê´„ÇÄ: "X„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ60%„ÅåÂ§±Êïó„Åô„ÇãÁêÜÁî±„ÅØ..."
   - ‰∏ÄËà¨ÁöÑ„Å™Âßã„Åæ„ÇäÊñπÁ¶ÅÊ≠¢: "X„Åå‰∫∫Ê∞ó„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô..." ‚ùå

2. **ÂÆüÈöõ„ÅÆÊàêÂäü/Â§±Êïó‰∫ã‰æã**:
   - ÂÖ∑‰ΩìÁöÑ„Å™‰ºöÁ§æ/‰∫∫Áâ©„ÅÆ‰æã„Çí1-2ÂÄãÂê´„ÇÄ
   - "„ÅÇ„ÇãEC„Çµ„Ç§„Éà„ÅØX„ÇíÂÖ®„Å¶„Å´ÈÅ©Áî®„Åó„Å¶Â§±Êïó„Åó„Åü„Åå„ÄÅY„Å†„Åë„Å´ÈõÜ‰∏≠„Åó„Åü„Çâ..."
   - „ÅÜ„Åæ„Åè„ÅÑ„Åã„Å™„ÅÑ„Åì„Å®„ÇÇË¶ã„Åõ„Çã (ÊàêÂäü„Å†„ÅëË™û„Çâ„Å™„ÅÑ)
   - ÊäΩË±°ÁöÑË°®ÁèæÁ¶ÅÊ≠¢: "Â§ö„Åè„ÅÆ‰ºÅÊ•≠„Åå..." ‚Üí "„ÅÇ„Çã„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó„ÅØ..." ‚úÖ

3. **ÈôêÁïåÁÇπ„Å®ËêΩ„Å®„ÅóÁ©¥**:
   - "X„Åå„Åã„Åà„Å£„Å¶ÈÄÜÂäπÊûú„Å´„Å™„ÇãÂ†¥Âêà" „Çª„ÇØ„Ç∑„Éß„É≥„Çí1„Å§Ë®≠„Åë„Çã
   - "„Åì„ÅÆ3„Å§„ÅÆÁä∂Ê≥Å„Åß„ÅØX„ÅØÈùûÂäπÁéáÁöÑ..."
   - „Åì„Çå„ÅåÁúüÂÆüÂë≥„Å®‰ø°È†º„ÇíÁîü„ÇÄ

4. **„Éá„Éº„Çø„Éâ„É™„Éñ„É≥**:
   - ÂÖ∑‰ΩìÁöÑ„Å™Áµ±Ë®à„Çí2-3ÂÄãÂê´„ÇÄ („Åä„Åä„Çà„Åù„Åß„ÇÇOK)
   - "2024Âπ¥„ÅÆË™øÊüª„Åß„ÅØ60%„ÅÆÂ§±ÊïóÁéá..."
   - "‰ºÅÊ•≠„ÅØ35%„ÅÆÁîüÁî£ÊÄßÂêë‰∏ä„ÇíÁµåÈ®ì..."

„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Ç¨„Ç§„Éâ„É©„Ç§„É≥:
- ÂØæË±°Ë™≠ËÄÖ: ÂÆüÁî®ÁöÑ„Å™„Ç¢„Éâ„Éê„Ç§„Çπ„ÇíÊ±Ç„ÇÅ„ÇãÊÑèÊÄùÊ±∫ÂÆöËÄÖ
- "ÈÅø„Åë„Çã„Åπ„Åç„Åì„Å®"„Çí"„Åô„Åπ„Åç„Åì„Å®"„Å®Âêå„Åò„Åè„Çâ„ÅÑÂº∑Ë™ø
- ÊäΩË±°ÁöÑ„Å™Ê¶ÇÂøµ„Çà„ÇäÂÖ∑‰Ωì‰æã
- ÁèæÂú®„ÅÆ„Éà„É¨„É≥„Éâ„Å´Ë®ÄÂèä (2025-2026Âπ¥)
- Á∞°ÊΩî„Åß„Ç§„É≥„Éë„ÇØ„Éà„ÅÆ„ÅÇ„ÇãÂÜÖÂÆπ - ‰∏çË¶Å„Å™Ë™¨Êòé„ÇíÂâäÈô§

üìö ÂèÇËÄÉË≥áÊñô„Çª„ÇØ„Ç∑„Éß„É≥:
- ‰∏äË®ò„Éó„É≠„É≥„Éó„Éà„ÅßÂèÇËÄÉË≥áÊñô„ÅåÊèê‰æõ„Åï„Çå„ÅüÂ†¥Âêà„ÄÅË®ò‰∫ã„ÅÆÊúÄÂæå„Å´ÂøÖ„Åö"## ÂèÇËÄÉË≥áÊñô"„Çª„ÇØ„Ç∑„Éß„É≥„ÇíËøΩÂä†
- Êèê‰æõ„Åï„Çå„ÅüURL„ÇíÊ≠£Á¢∫„Å´‰ΩøÁî® - ‰øÆÊ≠£„Åó„Åü„ÇäÊñ∞Ë¶è‰ΩúÊàê„Åó„Åü„Çä„Åó„Å™„ÅÑ„Åì„Å®
- ÂΩ¢Âºè: `- [ÊÉÖÂ†±Ê∫ê„Çø„Ç§„Éà„É´](URL) - ÁµÑÁπî/Âá∫ÁâàÁ§æ`
- ‰æãÁ§∫:
  ## ÂèÇËÄÉË≥áÊñô
  - [2025Âπ¥AIÂãïÂêë„É¨„Éù„Éº„Éà](https://example.com/ai-report) - „Éû„ÉÉ„Ç≠„É≥„Çº„Éº„Éª„Ç¢„É≥„Éâ„Éª„Ç´„É≥„Éë„Éã„Éº
  - [„É™„É¢„Éº„Éà„ÉØ„Éº„ÇØÁµ±Ë®à2025](https://example.com/remote) - Buffer
- **ÈáçË¶Å**: ‰∏äË®ò„ÅßÂèÇËÄÉË≥áÊñô„ÅåÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÄÅÂèÇËÄÉË≥áÊñô„Çª„ÇØ„Ç∑„Éß„É≥„ÅØÁµ∂ÂØæ„Å´ËøΩÂä†„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ

‰ªä„Åô„ÅêÂÆåÂÖ®„Å™„Éñ„É≠„Ç∞Ë®ò‰∫ã„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑÔºàÊú¨Êñá„ÅÆ„Åø„ÄÅ„Çø„Ç§„Éà„É´„ÇÑ„É°„Çø„Éá„Éº„Çø„Å™„ÅóÔºâ:"""
        }

        return prompts[lang]

    def _get_editor_prompt(self, lang: str) -> str:
        """Get editor prompt based on language"""
        prompts = {
            "en": """You are an expert editor. Transform this into Medium-style content with authentic human touch:

üìè Length Requirements (Target: 700-1200 words for 5-7 min read):
- If draft is under 700 words: EXPAND with examples, explanations, context to reach 700-1200 words
- If draft is 700-1200 words: MAINTAIN the same length (ideal range)
- If draft is 1200-1800 words: COMPRESS to 1100-1300 words by removing redundancy
- If draft is over 1800 words: COMPRESS aggressively to 1100-1300 words

üéØ CRITICAL ENHANCEMENTS:
1. **Strengthen Opening Hook**:
   - If opening is generic, rewrite to start with problem/pain point
   - Add empathy: "You've been there, right?"
   - Make it personal and relatable

2. **Add Authenticity Markers**:
   - Include phrases like "In my experience...", "Here's what surprised me..."
   - Add failure acknowledgment: "I thought X would work, but..."
   - Show vulnerability: "This isn't always the answer..."

3. **Enhance Examples**:
   - Make vague examples specific: "Many companies" ‚Üí "A fintech startup I worked with"
   - Add concrete details: names, numbers, outcomes
   - Include what went WRONG, not just success stories

4. **Balance Perspective**:
   - Ensure there's a "When this doesn't work" section
   - Add nuance: "This works IF...", "But in these cases..."
   - Avoid absolute claims: "always", "never", "guaranteed"

Tasks:
1. **Medium style conversion**: Add "you/I", conversational tone
2. **Eliminate all AI tells**: "certainly", "moreover", "it's important to note"
3. **Natural connectors**: "Look", "Here's why", "The truth is"
4. **Break fourth wall**: "You might be thinking...", "Sound familiar?"
5. **Punchy sentences**: "Here's the thing.", "Let me explain.", "Stop it."
6. **Smooth transitions**: "Now", "Here's where it gets interesting"
7. Keep all factual information intact
8. **Complete ending**: Finish conclusion fully

Return improved version (body only, no title):""",

            "ko": """ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ ÏóêÎîîÌÑ∞ÏûÖÎãàÎã§. Ïù¥ Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ ÏßÑÏßú ÏÇ¨ÎûåÏù¥ Ïì¥ Í≤É Í∞ôÏùÄ ÌÜ†Ïä§ Ïä§ÌÉÄÏùºÎ°ú Í∞úÏÑ†ÌïòÏÑ∏Ïöî:

üìè Í∏∏Ïù¥ ÏöîÍµ¨ÏÇ¨Ìï≠ (Î™©Ìëú: 5-7Î∂Ñ ÏùΩÍ∏∞ = 700-1,200Îã®Ïñ¥):
- Ï¥àÏïàÏù¥ 700Îã®Ïñ¥ ÎØ∏Îßå: ÏòàÏãú, ÏÑ§Î™Ö, Îß•ÎùΩ Ï∂îÍ∞ÄÎ°ú 700-1,200Îã®Ïñ¥ÍπåÏßÄ ÌôïÏû•
- Ï¥àÏïàÏù¥ 700-1,200Îã®Ïñ¥: Í∞ôÏùÄ Í∏∏Ïù¥ Ïú†ÏßÄ (Ïù¥ÏÉÅÏ†Å Î≤îÏúÑ)
- Ï¥àÏïàÏù¥ 1,200-1,800Îã®Ïñ¥: 1,100-1,300Îã®Ïñ¥Î°ú ÏïïÏ∂ï (Ï§ëÎ≥µ Ï†úÍ±∞)
- Ï¥àÏïàÏù¥ 1,800Îã®Ïñ¥ Ï¥àÍ≥º: 1,100-1,300Îã®Ïñ¥Î°ú ÎåÄÌè≠ ÏïïÏ∂ï

üéØ ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠:
1. **Ïò§ÌîÑÎãù Í∞ïÌôî**:
   - ÏùºÎ∞òÏ†Å ÏãúÏûëÏù¥Î©¥ Î¨∏Ï†ú/Í≥†ÎØº ÏÉÅÌô©ÏúºÎ°ú Ïû¨ÏûëÏÑ±
   - Í≥µÍ∞ê Ï∂îÍ∞Ä: "Ïù¥Îü∞ Í≤ΩÌóò ÏûàÏúºÏãúÏ£†?"
   - Í∞úÏù∏Ï†ÅÏù¥Í≥† Í≥µÍ∞ê Í∞ÄÎä•ÌïòÍ≤å

2. **ÏßÑÏ†ïÏÑ± ÎßàÏª§ Ï∂îÍ∞Ä**:
   - "Ï†ú Í≤ΩÌóòÏÉÅ...", "ÏùòÏô∏Î°ú...", "ÎÜÄÎûçÍ≤åÎèÑ..." Í∞ôÏùÄ ÌëúÌòÑ
   - Ïã§Ìå® Ïù∏Ï†ï: "Ï≤òÏùåÏóî XÍ∞Ä Îê† Ï§Ñ ÏïåÏïòÎäîÎç∞..."
   - ÌïúÍ≥Ñ Ïñ∏Í∏â: "Ìï≠ÏÉÅ ÎãµÏùÄ ÏïÑÎãàÏóêÏöî..."

3. **ÏòàÏãú Íµ¨Ï≤¥Ìôî**:
   - Ï∂îÏÉÅÏ†Å ÏòàÏãúÎ•º Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú: "ÎßéÏùÄ ÌöåÏÇ¨Îì§" ‚Üí "Ìïú ÌïÄÌÖåÌÅ¨ Ïä§ÌÉÄÌä∏ÏóÖÏùÄ"
   - Íµ¨Ï≤¥Ï†Å ÎîîÌÖåÏùº: Ïù¥Î¶Ñ, Ïà´Ïûê, Í≤∞Í≥º
   - Ïã§Ìå®Ìïú Í≤ÉÎèÑ Ìè¨Ìï®: ÏÑ±Í≥µÎßå ÎßêÌïòÏßÄ ÎßêÍ∏∞

4. **Í∑†ÌòïÏû°Ìûå Í¥ÄÏ†ê**:
   - "Ïù¥Îü∞ Í≤ΩÏö∞Ïóî Ïïà ÌÜµÌï¥Ïöî" ÏÑπÏÖò ÌôïÏù∏/Ï∂îÍ∞Ä
   - ÎâòÏïôÏä§: "Ïù¥Í≤å ÌÜµÌïòÎ†§Î©¥...", "ÌïòÏßÄÎßå Ïù¥Îü∞ Í≤ΩÏö∞Ïóî..."
   - Ï†àÎåÄÏ†Å ÌëúÌòÑ ÌîºÌïòÍ∏∞: "Ìï≠ÏÉÅ", "Ï†àÎåÄ", "Î¨¥Ï°∞Í±¥"

ÏûëÏóÖ:
1. **ÌÜ†Ïä§ ÎßêÌà¨Î°ú Î≥ÄÌôò**: "~ÏäµÎãàÎã§" ‚Üí "~Ìï¥Ïöî", ÏπúÍ∑ºÌïú ÏßàÎ¨∏Ìòï Ï∂îÍ∞Ä
2. AI ÎäêÎÇå ÏôÑÏ†Ñ Ï†úÍ±∞: "Î¨ºÎ°†", "~Ìï† Ïàò ÏûàÏäµÎãàÎã§", "Ï§ëÏöîÌï©ÎãàÎã§" Î™®Îëê ÏÇ≠Ï†ú
3. ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï†ëÏÜçÏÇ¨: "ÏÇ¨Ïã§", "Ïã§Ï†úÎ°ú", "Í∑∏Îü∞Îç∞", "Ï∞∏Í≥†Î°ú"
4. Ïà´ÏûêÎ•º ÏπúÍ∑ºÌïòÍ≤å: "50% ‚Üí Ï†àÎ∞ò", "3Î∞∞ ‚Üí ÏÑ∏ Î∞∞"
5. ÏßßÍ≥† Í∞ïÎ†¨Ìïú Î¨∏Ïû• Ï∂îÍ∞Ä: "ÎÜÄÎûçÏ£†?", "ÎßûÏïÑÏöî.", "Ïù¥Í≤å ÌïµÏã¨Ïù¥ÏóêÏöî."
6. ÏÑπÏÖò Í∞Ñ Îß§ÎÅÑÎü¨Ïö¥ Ï†ÑÌôò: "Ïûê, Ïù¥Ï†ú ~", "Í∑∏Îüº ~"
7. Î™®Îì† ÏÇ¨Ïã§ Ï†ïÎ≥¥Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
8. **ÎßàÏßÄÎßâ Î¨∏Ïû•ÍπåÏßÄ ÏôÑÍ≤∞**: Í≤∞Î°†ÏùÑ Î∞òÎìúÏãú ÏôÑÏÑ±

Í∞úÏÑ†Îêú Î≤ÑÏ†ÑÏùÑ Î∞òÌôòÌïòÏÑ∏Ïöî (Î≥∏Î¨∏Îßå, Ï†úÎ™© Ï†úÏô∏):""",

            "ja": """„ÅÇ„Å™„Åü„ÅØÂ∞ÇÈñÄ„Ç®„Éá„Ç£„Çø„Éº„Åß„Åô„ÄÇ„Åì„ÅÆ„Éñ„É≠„Ç∞Ë®ò‰∫ã„ÇíÊú¨Áâ©„ÅÆ‰∫∫Èñì„ÅåÊõ∏„ÅÑ„Åü„Çà„ÅÜ„Å™Ëá™ÁÑ∂„Å™‰ºöË©±Ë™ø„Å´ÊîπÂñÑ„Åó„Å¶„Åè„Å†„Åï„ÅÑ:

üìè ÊñáÂ≠óÊï∞Ë¶Å‰ª∂ (ÁõÆÊ®ô: 5-7ÂàÜ = 2,800-4,200ÊñáÂ≠ó):
- ‰∏ãÊõ∏„Åç„Åå2,800ÊñáÂ≠óÊú™Ê∫Ä: ‰æã„ÄÅË™¨Êòé„ÄÅÊñáËÑà„ÇíËøΩÂä†„Åó„Å¶2,800-4,200ÊñáÂ≠ó„Å´Êã°Âºµ
- ‰∏ãÊõ∏„Åç„Åå2,800-4,200ÊñáÂ≠ó: Âêå„ÅòÈï∑„Åï„ÇíÁ∂≠ÊåÅ (ÁêÜÊÉ≥ÁöÑ„Å™ÁØÑÂõ≤)
- ‰∏ãÊõ∏„Åç„Åå4,200-7,000ÊñáÂ≠ó: 3,500-4,000ÊñáÂ≠ó„Å´ÂúßÁ∏Æ (ÂÜóÈï∑ÊÄßÂâäÈô§)
- ‰∏ãÊõ∏„Åç„Åå7,000ÊñáÂ≠óË∂Ö: 3,500-4,000ÊñáÂ≠ó„Å´Â§ßÂπÖÂúßÁ∏Æ

üéØ ÈáçË¶Å„Å™ÊîπÂñÑÁÇπ:
1. **„Ç™„Éº„Éó„Éã„É≥„Ç∞„ÅÆÂº∑Âåñ**:
   - ‰∏ÄËà¨ÁöÑ„Å™Âßã„Åæ„Çä„Å™„ÇâÂïèÈ°å/ÊÇ©„ÅøÁä∂Ê≥Å„Å´Êõ∏„ÅçÁõ¥„Åó
   - ÂÖ±ÊÑü„ÇíËøΩÂä†: "„Åì„Çì„Å™ÁµåÈ®ì„ÅÇ„Çä„Åæ„Åõ„Çì„ÅãÔºü"
   - ÂÄã‰∫∫ÁöÑ„ÅßÂÖ±ÊÑü„Åß„Åç„Çã„Çà„ÅÜ„Å´

2. **ÁúüÂÆüÂë≥„Éû„Éº„Ç´„Éº„ÅÆËøΩÂä†**:
   - "ÁßÅ„ÅÆÁµåÈ®ì„Åß„ÅØ...", "ÊÑèÂ§ñ„Å´„ÇÇ...", "È©ö„ÅÑ„Åü„Åì„Å®„Å´..." „ÅÆ„Çà„ÅÜ„Å™Ë°®Áèæ
   - Â§±Êïó„ÅÆË™çË≠ò: "ÊúÄÂàù„ÅØX„Åå„ÅÜ„Åæ„Åè„ÅÑ„Åè„Å®ÊÄù„Å£„Åü„ÅÆ„Åß„Åô„Åå..."
   - ÈôêÁïå„ÅÆË®ÄÂèä: "„Åì„Çå„ÅåÂ∏∏„Å´Á≠î„Åà„Å®„ÅØÈôê„Çä„Åæ„Åõ„Çì..."

3. **‰æã„ÅÆÂÖ∑‰ΩìÂåñ**:
   - ÊõñÊòß„Å™‰æã„ÇíÂÖ∑‰ΩìÁöÑ„Å´: "Â§ö„Åè„ÅÆ‰ºÅÊ•≠" ‚Üí "„ÅÇ„Çã„Éï„Ç£„É≥„ÉÜ„ÉÉ„ÇØ‰ºÅÊ•≠„Åß„ÅØ"
   - ÂÖ∑‰ΩìÁöÑ„Å™Ë©≥Á¥∞: ÂêçÂâç„ÄÅÊï∞Â≠ó„ÄÅÁµêÊûú
   - Â§±Êïó„Åó„Åü„Åì„Å®„ÇÇÂê´„ÇÅ„Çã: ÊàêÂäü„Å†„ÅëË™û„Çâ„Å™„ÅÑ

4. **„Éê„É©„É≥„Çπ„ÅÆÂèñ„Çå„ÅüË¶ñÁÇπ**:
   - "„Åì„ÅÜ„ÅÑ„ÅÜÂ†¥Âêà„ÅØ„ÅÜ„Åæ„Åè„ÅÑ„Åç„Åæ„Åõ„Çì" „Çª„ÇØ„Ç∑„Éß„É≥„ÇíÁ¢∫Ë™ç/ËøΩÂä†
   - „Éã„É•„Ç¢„É≥„Çπ: "„Åì„Çå„Åå„ÅÜ„Åæ„Åè„ÅÑ„Åè„Å´„ÅØ...", "„Åü„Å†„Åó„Åì„Çì„Å™Â†¥Âêà„ÅØ..."
   - Áµ∂ÂØæÁöÑ„Å™Ë°®Áèæ„ÇíÈÅø„Åë„Çã: "Â∏∏„Å´", "Áµ∂ÂØæ„Å´", "ÂøÖ„Åö"

„Çø„Çπ„ÇØ:
1. **‰ºöË©±Ë™ø„Å´Â§âÊèõ**: "„Äú„Åß„Åô„Å≠", "„Äú„Åæ„Åô„Çà„Å≠", "„Äú„Åß„Åó„Çá„ÅÜ" „Å™„Å©Êüî„Çâ„Åã„ÅÑË™ûÂ∞æ„Å´
2. AIÁöÑ„Å™Ë°®Áèæ„ÇíÂÆåÂÖ®ÂâäÈô§: "„ÇÇ„Å°„Çç„Çì", "„Äú„Åô„Çã„Åì„Å®„ÅåÈáçË¶Å„Åß„Åô", "„Äú„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„Åó„Åæ„Åô"
3. Ëá™ÁÑ∂„Å™Êé•Á∂öË©û: "ÂÆü„ÅØ", "„Å°„Å™„Åø„Å´", "„Åï„Å¶", "„Åù„Çå„Åß"
4. ÊèêÊ°àÂΩ¢„ÇíËøΩÂä†: "„Äú„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ", "„Äú„Åó„Å¶„Åø„Å¶„Åè„Å†„Åï„ÅÑ"
5. Ë≥™ÂïèÂΩ¢„ÅßÂºï„ÅçËæº„ÇÄ: "„Å©„ÅÜ„Åß„Åó„Çá„ÅÜ„ÅãÔºü", "Ê∞ó„Å´„Å™„Çä„Åæ„Åõ„Çì„ÅãÔºü"
6. Áü≠„ÅÑÊÑüÂòÜ: "È©ö„Åç„Åß„Åô„Å≠„ÄÇ", "Èù¢ÁôΩ„ÅÑ„Åß„Åô„Çà„Å≠„ÄÇ"
7. „Çª„ÇØ„Ç∑„Éß„É≥Èñì„ÅÆÁßªË°å: "„Åß„ÅØ„ÄÅË©≥„Åó„ÅèË¶ã„Å¶„ÅÑ„Åç„Åæ„Åó„Çá„ÅÜ„ÄÇ"
8. „Åô„Åπ„Å¶„ÅÆ‰∫ãÂÆüÊÉÖÂ†±„ÅØ„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ
9. **ÊúÄÂæå„ÅÆÊñá„Åæ„ÅßÂÆåÁµê**: ÁµêË´ñ„ÇíÂøÖ„ÅöÂÆåÊàê

ÊîπÂñÑ„Åï„Çå„Åü„Éê„Éº„Ç∏„Éß„É≥„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÊú¨Êñá„ÅÆ„Åø„ÄÅ„Çø„Ç§„Éà„É´„Å™„ÅóÔºâ:"""
        }

        return prompts[lang]

    def generate_title(self, content: str, keyword: str, lang: str) -> str:
        """Generate SEO-friendly title"""
        prompts = {
            "en": f"Generate a catchy, SEO-friendly blog title (50-60 chars) for a post about '{keyword}'. Return ONLY the title, nothing else.",
            "ko": f"'{keyword}'Ïóê ÎåÄÌïú Î∏îÎ°úÍ∑∏ Í∏ÄÏùò Îß§Î†•Ï†ÅÏù¥Í≥† SEO ÏπúÌôîÏ†ÅÏù∏ Ï†úÎ™©ÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî (50-60Ïûê). Ï†úÎ™©Îßå Î∞òÌôòÌïòÏÑ∏Ïöî.",
            "ja": f"'{keyword}'„Å´Èñ¢„Åô„Çã„Éñ„É≠„Ç∞Ë®ò‰∫ã„ÅÆÈ≠ÖÂäõÁöÑ„ÅßSEO„Éï„É¨„É≥„Éâ„É™„Éº„Å™„Çø„Ç§„Éà„É´„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà50-60ÊñáÂ≠óÔºâ„ÄÇ„Çø„Ç§„Éà„É´„ÅÆ„Åø„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        return response.content[0].text.strip().strip('"').strip("'")

    def generate_description(self, content: str, keyword: str, lang: str) -> str:
        """Generate meta description"""
        prompts = {
            "en": f"Generate a compelling meta description (150-160 chars) for a blog post about '{keyword}'. Return ONLY the description.",
            "ko": f"'{keyword}'Ïóê ÎåÄÌïú Î∏îÎ°úÍ∑∏ Í∏ÄÏùò Îß§Î†•Ï†ÅÏù∏ Î©îÌÉÄ ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî (150-160Ïûê). ÏÑ§Î™ÖÎßå Î∞òÌôòÌïòÏÑ∏Ïöî.",
            "ja": f"'{keyword}'„Å´Èñ¢„Åô„Çã„Éñ„É≠„Ç∞Ë®ò‰∫ã„ÅÆÈ≠ÖÂäõÁöÑ„Å™„É°„ÇøË™¨Êòé„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà150-160ÊñáÂ≠óÔºâ„ÄÇË™¨Êòé„ÅÆ„Åø„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        return response.content[0].text.strip().strip('"').strip("'")

    def translate_to_english(self, text: str) -> str:
        """Translate non-English keywords to English for Unsplash search"""
        # Simple keyword translations for common tech/business/lifestyle terms
        translations = {
            # Korean
            'Ï±óÎ¥á': 'chatbot', 'AI': 'artificial intelligence', 'ÎèÑÏûÖ': 'implementation',
            'Ïã§Ìå®': 'failure', 'Ïù¥Ïú†': 'reasons', 'ÎÖ∏ÏΩîÎìú': 'no-code', 'Ìà¥': 'tool',
            'ÌïúÍ≥ÑÏ†ê': 'limitations', 'Ïû¨ÌÉùÍ∑ºÎ¨¥': 'remote work', 'ÌïòÏù¥Î∏åÎ¶¨Îìú': 'hybrid',
            'Í∑ºÎ¨¥': 'work', 'Ìö®Ïú®ÏÑ±': 'efficiency', 'MZÏÑ∏ÎåÄ': 'gen z', 'Í¥ÄÎ¶¨': 'management',
            'Î∞©Î≤ï': 'method', 'ÏÇ¨Î°Ä': 'case', 'ÎØ∏ÎãàÎ©Ä': 'minimal', 'ÎùºÏù¥ÌîÑ': 'lifestyle',
            'Ï§ëÎã®': 'quit', 'ÏÉùÏÇ∞ÏÑ±': 'productivity', 'ÌåÅ': 'tips',
            # Japanese
            '„Ç≥„Éº„Éâ': 'code', 'ÈñãÁô∫': 'development', 'ÈôêÁïåÁÇπ': 'limitations',
            '„ÉÜ„É¨„ÉØ„Éº„ÇØ': 'telework', '„Ç™„Éï„Ç£„Çπ': 'office', 'Âã§Âãô': 'work',
            'ÁîüÁî£ÊÄß': 'productivity', 'ÊØîËºÉ': 'comparison', '„Éé„Éº': 'no',
            '„Çµ„Éñ„Çπ„ÇØ': 'subscription', 'Áñ≤„Çå': 'fatigue', 'Ëß£Á¥Ñ': 'cancel',
            'ÁêÜÁî±': 'reason', 'Z‰∏ñ‰ª£': 'gen z', '„Éû„Éç„Ç∏„É°„É≥„Éà': 'management',
            'Ë™§Ëß£': 'misconception', 'DX': 'digital transformation', 'Êé®ÈÄ≤': 'promotion',
            'Â§±Êïó': 'failure', 'Ë¶ÅÂõ†': 'factors', '„Éí„É≥„Éà': 'tips',
            '„ÉØ„Éº„ÇØ„É©„Ç§„Éï„Éê„É©„É≥„Çπ': 'work life balance', '„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó': 'startup',
            'Ë≥áÈáëË™øÈÅî': 'fundraising', 'Êà¶Áï•': 'strategy', 'AI„Ç≥„Éº„Éá„Ç£„É≥„Ç∞': 'AI coding',
            '„Ç¢„Ç∑„Çπ„Çø„É≥„Éà': 'assistant', '„É™„É¢„Éº„Éà„ÉØ„Éº„ÇØ': 'remote work'
        }

        # Split and translate each word
        words = text.split()
        translated_words = []
        for word in words:
            # Try exact match first
            if word in translations:
                translated_words.append(translations[word])
            else:
                # Check if word contains any translatable substring
                found = False
                for kr, en in translations.items():
                    if kr in word:
                        translated_words.append(en)
                        found = True
                        break
                if not found:
                    # Keep as-is if ASCII (likely already English)
                    try:
                        word.encode('ascii')
                        translated_words.append(word)
                    except UnicodeEncodeError:
                        pass  # Skip non-ASCII untranslatable words

        return ' '.join(translated_words) if translated_words else 'technology'

    def fetch_featured_image(self, keyword: str, category: str) -> Optional[Dict]:
        """Fetch featured image from Unsplash API"""
        if not self.unsplash_key:
            return None

        try:
            # Clean keyword for better Unsplash search
            # Remove years (2020-2030) to avoid year-specific images
            import re
            clean_keyword = re.sub(r'20[2-3][0-9]ÎÖÑ?', '', keyword)  # Match years + optional ÎÖÑ (Korean year)
            # Remove common prefixes/suffixes that reduce search quality
            clean_keyword = re.sub(r'„Äê.*?„Äë', '', clean_keyword)  # Remove „Äêbrackets„Äë
            clean_keyword = re.sub(r'\[.*?\]', '', clean_keyword)  # Remove [brackets]
            clean_keyword = clean_keyword.strip()

            # Translation dictionary for meaningful keywords
            keyword_translations = {
                # Korean - AI/Jobs/Employment
                'AI': 'artificial intelligence',
                'Ïù∏Í≥µÏßÄÎä•': 'artificial intelligence',
                'ÎåÄÏ≤¥': 'replacement automation',
                'ÏùºÏûêÎ¶¨': 'job employment work',
                'Ïã§ÏóÖ': 'unemployment jobless',
                'ÏßÅÏóÖ': 'occupation career profession',
                'Ï∑®ÏóÖ': 'employment hiring recruitment',
                'ÏûêÎèôÌôî': 'automation robot',
                'Í∏∞Ïà†': 'technology tech',
                'ÎîîÏßÄÌÑ∏': 'digital technology',
                'Î°úÎ¥á': 'robot automation',
                'ÎØ∏Îûò': 'future',
                'Î≥ÄÌôî': 'change transformation',
                'ÏúÑÌóò': 'risk danger',
                # Korean - Finance/Business
                'ÎÇòÎùºÏÇ¨ÎûëÏπ¥Îìú': 'patriot card credit card',
                'Ïπ¥Îìú': 'card credit',
                'Ïó∞Î†π': 'age limit',
                'Ï†úÌïú': 'restriction limit',
                'Ï†ÑÏÑ∏': 'housing lease deposit',
                'Î≥¥Ï¶ùÍ∏à': 'deposit guarantee',
                'Î∞∞Îã¨': 'delivery food',
                'ÏàòÏàòÎ£å': 'fee commission',
                'ÏûêÏòÅÏóÖ': 'small business owner',
                'ÌèêÏóÖ': 'business closure bankruptcy',
                'ÏßÄÏõêÍ∏à': 'subsidy support fund',
                'Ï†ïÎ∂Ä': 'government policy',
                'Ïã†Ï≤≠': 'application registration',
                'ÌòúÌÉù': 'benefit advantage',
                # Korean - Entertainment/Society
                'ÏÇ¨Í≥ºÎ¨∏': 'apology statement',
                'Ìå¨': 'fan supporter',
                'Îì±ÎèåÎ¶º': 'backlash criticism',
                'Ïä§ÎßàÌä∏Ìè∞': 'smartphone mobile',
                'Í±¥Í∞ï': 'health wellness',
                # Japanese - AI/Jobs/Employment
                '‰∫∫Â∑•Áü•ËÉΩ': 'artificial intelligence',
                'Â§±Ê•≠': 'unemployment jobless',
                '„É™„Çπ„ÇØ': 'risk danger threat',
                'ËÅ∑Ê•≠': 'occupation job',
                '‰ª£Êõø': 'replacement substitute',
                'ÈõáÁî®': 'employment hiring',
                'Ëá™ÂãïÂåñ': 'automation robot',
                '„Éá„Ç∏„Çø„É´': 'digital technology',
                '„É≠„Éú„ÉÉ„Éà': 'robot automation',
                'Êú™Êù•': 'future',
                'Â§âÂåñ': 'change transformation',
                # Japanese - Finance/Business
                'Â•®Â≠¶Èáë': 'scholarship student loan',
                'ËøîÊ∏à': 'repayment debt',
                'ÂÖçÈô§': 'exemption forgiveness',
                'ÊäïË≥á': 'investment financial',
                'Ë©êÊ¨∫': 'fraud scam',
                '„Ç¢„Ç´„Éá„Éü„ÉºË≥û': 'academy award',
                'ÂèóË≥û': 'award winner',
                '‰ΩèÂÆÖ„É≠„Éº„É≥': 'home mortgage loan',
                'ÂØ©Êüª': 'screening examination',
                'ÊâøË™ç': 'approval authorization',
            }

            # Extract meaningful keywords from title
            title_words = clean_keyword.split()
            translated_keywords = []

            # Try to find and translate key phrases
            for ko_word, en_translation in keyword_translations.items():
                if ko_word in clean_keyword:
                    translated_keywords.append(en_translation)

            # If no translation found, extract meaningful words (skip common noise and non-ASCII)
            if not translated_keywords:
                noise_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']
                for word in title_words[:3]:  # Take first 3 words
                    # Filter out non-ASCII words to prevent non-English queries
                    try:
                        word.encode('ascii')
                        is_ascii = True
                    except UnicodeEncodeError:
                        is_ascii = False

                    if is_ascii and len(word) > 2 and word.lower() not in noise_words:
                        translated_keywords.append(word)

            # Add category context
            category_context = {
                'tech': 'technology digital',
                'business': 'business professional',
                'finance': 'finance money',
                'society': 'society community',
                'entertainment': 'entertainment culture',
                'lifestyle': 'lifestyle daily',
                'sports': 'sports athletic',
                'education': 'education learning'
            }

            # Build flexible, contextual query
            if translated_keywords:
                base_keywords = ' '.join(translated_keywords[:2])
            else:
                # Fallback to pure category context if no English keywords found
                base_keywords = category_context.get(category, 'technology')

            context = category_context.get(category, category)
            query = f"{base_keywords} {context}".strip()

            # Unsplash API endpoint
            url = "https://api.unsplash.com/search/photos"
            headers = {
                "Authorization": f"Client-ID {self.unsplash_key}"
            }
            params = {
                "query": query,
                "per_page": 5,
                "orientation": "landscape"
            }

            safe_print(f"  üîç Searching Unsplash for: {query}")

            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            data = response.json()

            if not data.get('results'):
                safe_print(f"  ‚ö†Ô∏è  No images found for '{query}'")
                return None

            # Load used images tracking file
            used_images_file = Path(__file__).parent.parent / "data" / "used_images.json"
            used_images = set()
            if used_images_file.exists():
                try:
                    with open(used_images_file, 'r') as f:
                        used_images = set(json.load(f))
                except:
                    pass

            # Find first unused image from results
            photo = None
            for result in data['results']:
                image_id = result['id']
                if image_id not in used_images:
                    photo = result
                    used_images.add(image_id)
                    break

            # If all images are used, try with generic category query
            if photo is None:
                safe_print(f"  ‚ö†Ô∏è  All images for '{query}' already used, trying generic category search...")
                generic_query = category_context.get(category, 'technology')
                params['query'] = generic_query

                response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
                response.raise_for_status()
                data = response.json()

                if data.get('results'):
                    for result in data['results']:
                        image_id = result['id']
                        if image_id not in used_images:
                            photo = result
                            used_images.add(image_id)
                            safe_print(f"  ‚úì Found unused image with generic search: {generic_query}")
                            break

                # If still no unused image found, return None (use placeholder)
                if photo is None:
                    safe_print(f"  ‚ùå No unused images available for category '{category}'")
                    return None

            # Save used images
            used_images_file.parent.mkdir(parents=True, exist_ok=True)
            with open(used_images_file, 'w') as f:
                json.dump(list(used_images), f)

            image_info = {
                'url': photo['urls']['regular'],
                'download_url': photo['links']['download_location'],
                'photographer': photo['user']['name'],
                'photographer_url': photo['user']['links']['html'],
                'unsplash_url': photo['links']['html'],
                'image_id': photo['id']
            }

            safe_print(f"  ‚úì Found image by {image_info['photographer']}")
            return image_info

        except requests.exceptions.Timeout as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API timeout: Request took too long")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.RequestException as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API network error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except json.JSONDecodeError as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API response parsing failed")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: Invalid JSON response")
            return None
        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Image fetch failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def download_image(self, image_info: Dict, keyword: str) -> Optional[str]:
        """Download optimized image to static/images/ directory"""
        if not image_info:
            return None

        try:
            # Create images directory
            images_dir = Path("static/images")
            images_dir.mkdir(parents=True, exist_ok=True)

            # Generate filename
            slug = keyword.lower()
            slug = ''.join(c if c.isalnum() or c.isspace() else '' for c in slug)
            slug = slug.replace(' ', '-')[:30]
            # Use KST for image filename
            from datetime import timezone, timedelta
            kst = timezone(timedelta(hours=9))
            date_str = datetime.now(kst).strftime("%Y%m%d")
            filename = f"{date_str}-{slug}.jpg"
            filepath = images_dir / filename

            # Trigger Unsplash download tracking (required by API terms)
            if image_info.get('download_url'):
                verify_ssl = certifi.where() if certifi else True
                requests.get(
                    image_info['download_url'],
                    headers={"Authorization": f"Client-ID {self.unsplash_key}"},
                    timeout=5,
                    verify=verify_ssl
                )

            # Download optimized image (1200px width, quality 85)
            # Use Unsplash's regular URL which already includes optimization
            download_url = image_info.get('url', '')
            # Add additional optimization parameters
            if '?' in download_url:
                optimized_url = f"{download_url}&w=1200&q=85&fm=jpg"
            else:
                optimized_url = f"{download_url}?w=1200&q=85&fm=jpg"

            safe_print(f"  üì• Downloading optimized image (1200px, q85)...")
            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(optimized_url, timeout=15, verify=verify_ssl)
            response.raise_for_status()

            # Save image
            with open(filepath, 'wb') as f:
                f.write(response.content)

            size_kb = len(response.content) / 1024
            safe_print(f"  ‚úì Image saved: {filepath} ({size_kb:.1f} KB)")

            # Return relative path for Hugo
            return f"/images/{filename}"

        except requests.exceptions.Timeout as e:
            safe_print(f"  ‚ö†Ô∏è  Image download timeout")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     URL: {optimized_url[:80]}...")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  ‚ö†Ô∏è  Image download HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            return None
        except IOError as e:
            safe_print(f"  ‚ö†Ô∏è  File system error during image save")
            safe_print(f"     Path: {filepath}")
            safe_print(f"     Error: {str(e)}")
            return None
        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Image download failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def save_post(self, topic: Dict, title: str, description: str, content: str, image_path: Optional[str] = None, image_credit: Optional[Dict] = None) -> Path:
        """Save post to Hugo content directory"""
        lang = topic['lang']
        category = topic['category']
        keyword = topic['keyword']

        # Generate filename from keyword
        slug = keyword.lower()
        # Remove special characters, keep alphanumeric and spaces
        slug = ''.join(c if c.isalnum() or c.isspace() else '' for c in slug)
        slug = slug.replace(' ', '-')[:50]

        # Create directory
        content_dir = Path(f"content/{lang}/{category}")
        content_dir.mkdir(parents=True, exist_ok=True)

        # Generate filename with date in KST
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        date_str = datetime.now(kst).strftime("%Y-%m-%d")
        filename = f"{date_str}-{slug}.md"
        filepath = content_dir / filename

        # Hugo frontmatter with required image field
        # Use placeholder if no Unsplash image available
        if not image_path:
            # Use category-based placeholder
            image_path = f"/images/placeholder-{category}.jpg"

        # Use KST timezone for date
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        now_kst = datetime.now(kst)

        frontmatter = f"""---
title: "{title}"
date: {now_kst.strftime("%Y-%m-%dT%H:%M:%S%z")}
draft: false
categories: ["{category}"]
tags: {json.dumps(keyword.split()[:3])}
description: "{description}"
image: "{image_path}"
---

"""

        # Add hero image at the top of content if available
        hero_image = ""
        if image_path and image_credit:
            hero_image = f"![{keyword}]({image_path})\n\n"

        # Add image credit at the end of content if available
        credit_line = ""
        if image_credit:
            credit_line = f"\n\n---\n\n*Photo by [{image_credit['photographer']}]({image_credit['photographer_url']}) on [Unsplash]({image_credit['unsplash_url']})*\n"

        # Validate References section and remove if it contains fake URLs
        def has_fake_reference_url(url: str) -> bool:
            """Check if URL is a fake reference"""
            fake_patterns = [
                r'example\.com',
                r'example\.org',
                r'\.gov/[a-z-]+-202[0-9]',
                r'\.org/[a-z-]+-survey',
                r'\.gov/[a-z-]+-compliance',
                r'\.gov/[a-z-]+-report',
            ]
            for pattern in fake_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return True
            return False

        # Check if References section exists - if not, just skip it (don't add fake references)
        ref_headers = {
            'en': '## References',
            'ko': '## Ï∞∏Í≥†ÏûêÎ£å',
            'ja': '## ÂèÇËÄÉÊñáÁåÆ'
        }
        ref_header = ref_headers.get(lang, '## References')

        # First, normalize any non-standard reference formats to standard format
        # Remove bold "**References:**" format if exists (common Claude output)
        bold_ref_patterns = [
            (r'\*\*References?:\*\*\n', ''),  # **References:**
            (r'\*\*ÂèÇËÄÉ(?:ÊñáÁåÆ|Ë≥áÊñô):\*\*\n', ''),  # **ÂèÇËÄÉÊñáÁåÆ:** or **ÂèÇËÄÉË≥áÊñô:**
            (r'\*\*Ï∞∏Í≥†ÏûêÎ£å:\*\*\n', ''),  # **Ï∞∏Í≥†ÏûêÎ£å:**
        ]
        for pattern, replacement in bold_ref_patterns:
            content = re.sub(pattern, replacement, content)

        # Extract References section if exists
        has_references = ref_header in content or '## Reference' in content or '## Ï∞∏Í≥†' in content or '## ÂèÇËÄÉ' in content

        if has_references:
            # Extract URLs from References section using regex
            # Pattern: [text](url) or bare URLs
            url_pattern = r'https?://[^\s\)\]<>"]+'  
            urls_in_content = re.findall(url_pattern, content)

            # Check if any URLs are fake
            fake_urls = [url for url in urls_in_content if has_fake_reference_url(url)]

            if fake_urls:
                safe_print(f"  ‚ö†Ô∏è  Fake reference URLs detected: {len(fake_urls)} found")
                safe_print(f"      Examples: {fake_urls[:3]}")

                # Remove References section entirely
                # Match from any References header to the next ## header or end of content
                ref_pattern = r'\n## (?:References?|ÂèÇËÄÉ(?:ÊñáÁåÆ|Ë≥áÊñô)|Ï∞∏Í≥†ÏûêÎ£å)\n.*?(?=\n## |\Z)'
                content = re.sub(ref_pattern, '', content, flags=re.DOTALL)
                safe_print(f"  üóëÔ∏è  Removed References section with fake URLs")
                has_references = False  # Mark as no valid references
            else:
                safe_print(f"  ‚úÖ References section validated ({len(urls_in_content)} URLs)")

        # If no valid References section exists, add from queue
        if not has_references and topic.get('references'):
            references = topic['references']
            safe_print(f"  ‚ÑπÔ∏è  No References section in content, adding from queue ({len(references)} refs)")

            # Build References section
            ref_section = f"\n\n{ref_header}\n\n"
            for i, ref in enumerate(references, 1):
                ref_section += f"{i}. [{ref['title']}]({ref['url']})\n"

            # Append to content
            content = content.rstrip() + ref_section
            safe_print(f"  ‚úÖ Added {len(references)} references from queue")
        elif not has_references:
            safe_print(f"  ‚ÑπÔ∏è  No references available (neither in content nor queue)")

        # Add affiliate links if applicable
        affiliate_programs_used = []
        if should_add_affiliate_links(category):
            safe_print(f"  üîó Checking for product mentions to add affiliate links...")

            # Detect products mentioned in content
            detected_products = detect_product_mentions(content, lang, category)

            if detected_products:
                safe_print(f"  üì¶ Detected {len(detected_products)} products: {', '.join(detected_products[:3])}")

                # Add affiliate link for the first detected product only (to avoid being too commercial)
                primary_product = detected_products[0]
                link_data = generate_affiliate_link(primary_product, lang)

                if link_data:
                    # Find insertion point: after first ## section
                    sections = content.split('\n## ')
                    if len(sections) > 1:
                        # Insert after first section
                        affiliate_box = create_affiliate_box(primary_product, lang, link_data)
                        sections[1] = sections[1] + '\n' + affiliate_box
                        content = '\n## '.join(sections)

                        affiliate_programs_used.append(link_data['program'])
                        safe_print(f"  ‚úÖ Added affiliate link for '{primary_product}' ({link_data['program']})")
                    else:
                        safe_print(f"  ‚ö†Ô∏è  Could not find insertion point for affiliate link")
                else:
                    safe_print(f"  ‚ÑπÔ∏è  No affiliate program configured for {lang}")
            else:
                safe_print(f"  ‚ÑπÔ∏è  No product mentions detected")
        else:
            safe_print(f"  ‚ÑπÔ∏è  Affiliate links disabled for category: {category}")

        # Add affiliate disclosure if links were added
        if affiliate_programs_used:
            disclosure = get_affiliate_disclosure(lang, affiliate_programs_used)
            content = content.rstrip() + disclosure
            safe_print(f"  ‚ö†Ô∏è  Added affiliate disclosure")

        # Write file with hero image at top
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(frontmatter)
            f.write(hero_image)
            f.write(content)
            f.write(credit_line)

        safe_print(f"  üíæ Saved to: {filepath}")
        return filepath


def main():
    parser = argparse.ArgumentParser(description="Generate blog posts")
    parser.add_argument("--count", type=int, default=3, help="Number of posts to generate")
    parser.add_argument("--topic-id", type=str, help="Specific topic ID to generate")
    args = parser.parse_args()

    # Pre-flight checks
    safe_print(f"\n{'='*60}")
    safe_print(f"  üîç Pre-flight Environment Checks")
    safe_print(f"{'='*60}\n")

    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    unsplash_key = os.environ.get("UNSPLASH_ACCESS_KEY")

    if anthropic_key:
        safe_print("  ‚úì ANTHROPIC_API_KEY: Configured")
    else:
        safe_print("  ‚ùå ANTHROPIC_API_KEY: NOT FOUND")

    if unsplash_key:
        safe_print("  ‚úì UNSPLASH_ACCESS_KEY: Configured")
    else:
        safe_print("  ‚ö†Ô∏è  UNSPLASH_ACCESS_KEY: NOT FOUND")
        safe_print("     Posts will use placeholder images!")

    safe_print("")

    # Initialize generator
    try:
        generator = ContentGenerator()
    except ValueError as e:
        safe_print(f"Error: {str(e)}")
        safe_print("\nSet ANTHROPIC_API_KEY environment variable:")
        safe_print("  export ANTHROPIC_API_KEY='your-api-key'")
        sys.exit(1)

    # Get topics
    if args.topic_id:
        # Load specific topic (for testing)
        from topic_queue import get_queue
        queue = get_queue()
        data = queue._load_queue()
        topics = [t for t in data['topics'] if t['id'] == args.topic_id]
        if not topics:
            safe_print(f"Error: Topic {args.topic_id} not found")
            sys.exit(1)
    else:
        # Reserve topics from queue
        topics = reserve_topics(count=args.count)

    if not topics:
        safe_print("No topics available in queue")
        sys.exit(0)

    safe_print(f"\n{'='*60}")
    safe_print(f"  Generating {len(topics)} posts")
    safe_print(f"{'='*60}\n")

    generated_files = []

    for i, topic in enumerate(topics, 1):
        safe_print(f"[{i}/{len(topics)}] {topic['id']}")
        safe_print(f"  Keyword: {topic['keyword']}")
        safe_print(f"  Category: {topic['category']}")
        safe_print(f"  Language: {topic['lang']}")

        try:
            # Generate content
            safe_print(f"  ‚Üí Step 1/5: Generating draft...")
            draft = generator.generate_draft(topic)

            safe_print(f"  ‚Üí Step 2/5: Editing draft...")
            final_content = generator.edit_draft(draft, topic)

            # Generate metadata
            safe_print(f"  ‚Üí Step 3/5: Generating metadata...")
            try:
                title = generator.generate_title(final_content, topic['keyword'], topic['lang'])
                description = generator.generate_description(final_content, topic['keyword'], topic['lang'])
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  WARNING: Metadata generation failed, using defaults")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                title = topic['keyword']
                description = f"Article about {topic['keyword']}"

            # Fetch featured image
            safe_print(f"  ‚Üí Step 4/5: Fetching image...")
            image_path = None
            image_credit = None
            try:
                image_info = generator.fetch_featured_image(topic['keyword'], topic['category'])
                if image_info:
                    image_path = generator.download_image(image_info, topic['keyword'])
                    if image_path:
                        image_credit = image_info
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  WARNING: Image fetch failed, will use placeholder")
                safe_print(f"     Error: {mask_secrets(str(e))}")

            # Save post with image
            safe_print(f"  ‚Üí Step 5/5: Saving post...")
            try:
                filepath = generator.save_post(topic, title, description, final_content, image_path, image_credit)
            except IOError as e:
                safe_print(f"  ‚ùå ERROR: Failed to save post to filesystem")
                safe_print(f"     Error: {str(e)}")
                raise
            except Exception as e:
                safe_print(f"  ‚ùå ERROR: Unexpected error during save")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                raise

            # Mark as completed
            if not args.topic_id:
                try:
                    mark_completed(topic['id'])
                except Exception as e:
                    safe_print(f"  ‚ö†Ô∏è  WARNING: Failed to mark topic as completed in queue")
                    safe_print(f"     Topic ID: {topic['id']}")
                    safe_print(f"     Error: {str(e)}")
                    # Don't fail the whole process if queue update fails

            generated_files.append(str(filepath))
            safe_print(f"  ‚úÖ Completed!\n")

        except KeyError as e:
            safe_print(f"  ‚ùå FAILED: Missing required field in topic data")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Missing field: {str(e)}\n")
            if not args.topic_id:
                mark_failed(topic['id'], f"Missing field: {str(e)}")
        except ValueError as e:
            safe_print(f"  ‚ùå FAILED: Invalid data or API response")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))
        except Exception as e:
            safe_print(f"  ‚ùå FAILED: Unexpected error")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error type: {type(e).__name__}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))

    # Save generated files list for quality gate
    output_file = Path("generated_files.json")
    with open(output_file, 'w') as f:
        json.dump(generated_files, f, indent=2)

    # Post-generation quality check
    safe_print(f"\n{'='*60}")
    safe_print(f"  üìä Post-Generation Quality Check")
    safe_print(f"{'='*60}\n")

    posts_without_references = 0
    posts_with_placeholders = 0

    for filepath in generated_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

                # Check for references section
                has_references = '## References' in content or '## ÂèÇËÄÉ' in content or '## Ï∞∏Í≥†ÏûêÎ£å' in content
                if not has_references:
                    posts_without_references += 1
                    safe_print(f"  ‚ö†Ô∏è  No references: {Path(filepath).name}")

                # Check for placeholder images
                if 'placeholder-' in content:
                    posts_with_placeholders += 1
                    safe_print(f"  ‚ö†Ô∏è  Placeholder image: {Path(filepath).name}")
        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Could not check: {Path(filepath).name}")

    safe_print("")

    if posts_without_references > 0:
        safe_print(f"üö® WARNING: {posts_without_references}/{len(generated_files)} posts have NO references!")
        safe_print(f"   This reduces content credibility and SEO value.")
        safe_print(f"   FIX: Ensure Google Custom Search API is configured in keyword curation\n")

    if posts_with_placeholders > 0:
        safe_print(f"üö® WARNING: {posts_with_placeholders}/{len(generated_files)} posts use PLACEHOLDER images!")
        safe_print(f"   This hurts user experience and engagement.")
        safe_print(f"   FIX: Ensure UNSPLASH_ACCESS_KEY is set in environment variables\n")

    if posts_without_references == 0 and posts_with_placeholders == 0:
        safe_print(f"‚úÖ Quality Check PASSED: All posts have references and real images!\n")

    safe_print(f"{'='*60}")
    safe_print(f"  ‚úì Generated {len(generated_files)} posts")
    safe_print(f"  File list saved to: {output_file}")
    safe_print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
